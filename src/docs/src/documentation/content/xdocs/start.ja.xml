<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN" "http://forrest.apache.org/dtd/document-v20.dtd">
<document>
  <header>
    <title>Getting Started (はじめよう！)</title>
  </header>
  <body>
  
<!-- ========================================================== -->  

<!-- SET UP PIG -->
 <section>
		<title>Pig のセットアップ</title>
	
<!-- ++++++++++++++++++++++++++++++++++ -->
 <section id="req">
 <title>システム要件</title>
 <p><strong>必須</strong></p>
      <p>Unix ユーザ、 Windows ユーザとも次のソフトウェアが必要です:</p>
		<ul>
		  <li> <strong>Hadoop 0.20.2, 020.203, 020.204,  0.20.205, 1.0.0, 1.0.1, or 0.23.0, 0.23.1</strong> - <a href="http://hadoop.apache.org/common/releases.html">http://hadoop.apache.org/common/releases.html</a> (Pig が使う Hadoop のバージョンを変えるには、 HADOOP_HOME 環境変数に Hadop をインストールしたディレクトリを設定してください。 HADOOP_HOME 環境変数を設定しない場合は、 Pig に組み込まれたバージョンが使われます。現在 Pig には Hadoop 1.0.0 が組み込まれています)</li>
		  <li> <strong>Java 1.6</strong> - <a href="http://java.sun.com/javase/downloads/index.jsp">http://java.sun.com/javase/downloads/index.jsp</a> (JAVA_HOME 環境変数に Java インストールディレクトリを指定してください)</li>	
		</ul>
		<p></p>
    <p>Windows ユーザはこれ以外に Cygwin と Perl パッケージをインストールする必要があります: <a href="http://www.cygwin.com/"> http://www.cygwin.com/</a></p>

<p></p>
 <p><strong>任意</strong></p>
 		<ul>
          <li> <strong>Python 2.5</strong> - <a href="http://jython.org/downloads.html">http://jython.org/downloads.html</a> (Python UDF を使う場合、または Pig を Python に組み込む場合)</li>
          <li> <strong>JavaScript 1.7</strong> - <a href="https://developer.mozilla.org/en/Rhino_downloads_archive">https://developer.mozilla.org/en/Rhino_downloads_archive</a> および <a href="http://mirrors.ibiblio.org/pub/mirrors/maven2/rhino/js/">http://mirrors.ibiblio.org/pub/mirrors/maven2/rhino/js/</a>  (JavaScript UDF を使う場合、または Pig を JavaScript に組み込む場合) </li>		  
          <li> <strong>JRuby 1.6.7</strong> - <a href="http://www.jruby.org/download">http://www.jruby.org/download</a> (JRuby UDF を使う場合) </li>
		  <li> <strong>Ant 1.7</strong> - <a href="http://ant.apache.org/">http://ant.apache.org/</a> (ビルド用) </li>
		  <li> <strong>JUnit 4.5</strong> - <a href="http://junit.sourceforge.net/">http://junit.sourceforge.net/</a> (単体テスト用) </li>
		</ul>
 
  </section>         
   
<!-- ++++++++++++++++++++++++++++++++++ -->        
 <section id="download">
 <title>Pig のダウンロード</title>
	<p>Pig を入手するには、次のようにしてください:</p>
	
	<ol>
	<li>Apache ダウンロードミラーから新しめの安定バージョンをダウンロード
	(<a href="http://hadoop.apache.org/pig/releases.html">Pig Releases</a> を見てください) 。</li>
	
    <li>ダウンロードした Pig のアーカイブを解凍して、次のことを確認:
	    <ul>
	    <li>Pig を実行するためのシェルスクリプト pig が bin ディレクトリに配置されていること (/pig-n.n.n/bin/pig) 。
        Pig 実行のための環境変数についてシェルスクリプト内に説明があります。</li>
	    <li>Pig のプロパティファイル pig.properties が conf ディレクトリ (/pig-n.n.n/conf/pig.properties) に配置されていること。他の場所に設定ファイルを置く時は、 PIG_CONF_DIR 環境変数にディレクトリを指定する必要があります。</li>
	</ul>	
	</li>
	<li>/pig-n.n.n/bin をパスに追加。 bash, sh, ksh では export コマンドを、 tcsh, csh では setenv コマンドを使ってください。例: <br></br>
	<code>$ export PATH=/&lt;my-path-to-pig&gt;/pig-n.n.n/bin:$PATH</code>
</li>
<li>
次のコマンドで、 Pig がインストールできていることを確認: <code>$ pig -help</code>
</li>
</ol>

</section>  

<!-- ++++++++++++++++++++++++++++++++++ -->
<section id="build">
<title>Pig のビルド</title>
      <p>Pig のビルドは、次のようにしてください:</p>
     <ol>
	  <li>Subversion リポジトリから Pig をチェックアウト: <code>svn co http://svn.apache.org/repos/asf/pig/trunk</code> </li>
      <li>トップディレクトリでビルドを実行: <code>ant</code> <br></br>
      トップディレクトリに pig.jar が作られていたら成功</li>	
	  <li>pig.jar をテスト: <code>ant test</code></li>
     </ol>
 </section>
</section>

  <!-- ==================================================================== -->
    
   <!-- RUNNING PIG  -->
   <section id="run">
	<title>Pig の実行</title> 
	<p>Pig はいくつかのモードで実行できます。</p>
	<table>
	<tr>
	<td></td>
    <td><strong>ローカルモード</strong></td>
    <td><strong>Mapreduce モード</strong></td>
	</tr>
	<tr>
	<td><strong>対話モード</strong></td>
    <td>yes</td>
    <td>yes</td>
	</tr>
	<tr>
	<td><strong>バッチモード</strong> </td>
    <td>yes</td>
    <td>yes</td>
	</tr>
	</table>
	
	<!-- ++++++++++++++++++++++++++++++++++ -->
	   <section id="execution-modes">
	<title>実行モード</title> 
<p>Pig には二種類の実行モードがあります: </p>
<ul>
<li><strong>ローカルモード</strong> - Pig をローカルモードで実行するのに必要なのはマシンが一台だけです。全ファイルがローカルマシンに配置され、ローカルマシン上で実行されます。ローカルモードで実行するには -x フラグをつけてください (pig -x local) 。
</li>
  <li><strong>MapReduce モード</strong> - Pig を MapReduceモードで実行するには Hadoop クラスタが必要です。MapReduce モードはデフォルトのモードです。 -x フラグで MapReduce モードでの実行を指定できますが、<em>必須ではありません</em> (pig あるいは pig -x mapreduce).
</li>
</ul>
<p></p>

<p>どちらのモードも "pig" コマンドで実行できます (実体は Perl スクリプト bin/pig) 。または、 "java" コマンドで実行することも可能です (java -cp pig.jar ...) 。
</p>


<section>
<title>例</title>

<p>pig コマンドでローカルモードと MapReduce モードの双方で実行する例。</p>
<source>
/* ローカルモード */
$ pig -x local ...
 
 
/* MapReduce モード */
$ pig ...
あるいは
$ pig -x mapreduce ...
</source>

<p>java コマンドでローカルモードと MapReduce モードの双方で実行する例。</p>
<source>
/* ローカルモード */
$ java -cp pig.jar org.apache.pig.Main -x local ...


/* MapReduce モード */
$ java -cp pig.jar org.apache.pig.Main ...
あるいは
$ java -cp pig.jar org.apache.pig.Main -x mapreduce ...
</source>

</section>
</section>

<!-- ++++++++++++++++++++++++++++++++++ -->
<section id="interactive-mode">
<title>対話モード</title>
<p>Grunt シェルは Pig の対話モードです。Grunt シェルを起動するには下記のように "pig" コマンドを実行します。 Grunt シェルでは Pig Latin 文と Pig コマンドを対話的に実行できます。
</p>

<section>
<title>例</title>
<p>下記の Pig Latin は /etc/passwd ファイルからすべてのユーザ ID を列挙します。はじめに /etc/passwd ファイルを作業ディレクトリにコピーしてください。次に、 "pig" コマンドで Grunt シェルを起動してください (ローカルモードあるいは MapReduce モードで) 。次に、Grunt シェルのプロンプトで Pig Latin 文を対話的に入力してください (その際、文ごとにセミコロンを忘れないように) 。 DUMP 演算子を打つと結果が端末に表示されます</p>
<source>
grunt&gt; A = load 'passwd' using PigStorage(':'); 
grunt&gt; B = foreach A generate $0 as id; 
grunt&gt; dump B; 
</source>

<p><strong>ローカルモード</strong></p>
<source>
$ pig -x local
... - Connecting to ...
grunt> 
</source>

<p><strong>MapReduce モード</strong> </p>
<source>
$ pig -x mapreduce
... - Connecting to ...
grunt> 

あるいは

$ pig 
... - Connecting to ...
grunt> 
</source>
</section>
</section>

<!-- ++++++++++++++++++++++++++++++++++ -->
<section id="batch-mode">
<title>バッチモード</title>

<p><a href="#pig-scripts">Pig スクリプト</a> と "pig" コマンド (ローカルモードあるいは MapReduce モード) を使って Pig をバッチモードで実行できます。</p>

<section>
<title>例</title>

<p>下記の Pig スクリプト (id.pig) は /etc/passwd ファイルから全ユーザ ID を列挙します。はじめに /etc/passwd ファイルを作業ディレクトリにコピーしてください。次に、コマンドラインから Pig スクリプトを実行してください (ローカルモードあるいは MapReduce モードで) 。最後の STORE 演算子で結果がファイル (id.out) に出力されます。</p>
<source>
/* id.pig */

A = load 'passwd' using PigStorage(':');  -- パスワードファイルをロード
B = foreach A generate $0 as id;  -- ユーザ ID を抜き出す
store B into 'id.out'; -- 結果を id.out に書き込む
</source>

<p><strong>ローカルモード</strong></p>
<source>
$ pig -x local id.pig
</source>
<p><strong>MapReduce モード</strong> </p>
<source>
$ pig id.pig
あるいは
$ pig -x mapreduce id.pig
</source>
</section>

  <!-- ==================================================================== -->
    
   <!-- PIG SCRIPTS -->
   <section id="pig-scripts">
	<title>Pig スクリプト</title>
	
<p>Pig Latin 文と Pig コマンドは Pig スクリプトとして 1 ファイルにまとめられます。特に必要でない限り、 Pig スクリプトの拡張子は *.pig とするのがよい習慣です。</p>	
	
<p>Pig スクリプトはコマンドラインからも Grunt シェルからも実行できます
(詳しくは <a href="cmds.html#run">run</a> コマンドと <a href="cmds.html#exec">exec</a> コマンドを見てください) 。</p>
	
<p><a href="cont.html#Parameter-Sub">パラメータ置換</a>を使うとコマンドラインパラメータから値が渡せます。</p>

<!-- +++++++++++++++++++++++++++++++++++++++++++ -->	
   <p id="comments"><strong>スクリプト中のコメント</strong></p>
   
   <p>Pig スクリプトの中にはコメントを書くことができます:</p>
   <ul>
      <li>
        <p>複数行コメントは 「/* ... */」</p>
      </li>
      <li>
        <p>単一行コメントは 「-- ...」</p>
      </li>
   </ul>
<source>
/* myscript.pig
単純なスクリプトです
3 つの Pig Latin 文で構成されています
*/

A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); -- データをロード
B = FOREACH A GENERATE name;  -- データ変換
DUMP B;  -- 結果を取得
</source>   
	
<!-- +++++++++++++++++++++++++++++++++++++++++++ -->		

<p id="dfs"><strong>スクリプトと分散ファイルシステム</strong></p>

<p>Pig は HDFS, Amazon S3 といったような分散ファイルシステム上の Pig スクリプトの実行 (および Jar ファイルの登録) をサポートしています。実行の際にはスクリプトのフル URI が必要です (Jar ファイルについては <a href="basic.html#register">REGISTER</a> を見てください) 。たとえば、 HDFS 上の Pig スクリプトを実行するためには、次のようにします:</p>
<source>
$ pig hdfs://nn.mydomain.com:9020/myscripts/script.pig
</source> 
</section>	
</section>
</section>

  <!-- ==================================================================== -->
    
   <!-- PIG LATIN STATEMENTS -->
   <section id="pl-statements">
	<title>Pig Latin 文</title>	
   <p>Pig Latin 文は Pig によるデータ処理の基本構成物です。 Pig Latin 文は<a href="basic.html#relations">関係</a>を入力に取り、別の関係を出力する演算子です (この定義は、ファイルシステムからデータを読み込む LOAD と、ファイルシステムにデータを書き出す STORE を除く、すべての Pig Latin 演算子にあてはまります) 。 Pig Latin の文は<a href="basic.html#expressions">式</a>と<a href="basic.html#Schemas">スキーマ</a>を含むことがあります。 Pig Latin の文は複数行に渡ることがあります。また、 Pig Latin の文はセミコロン (「;」) で終わる必要があります。デフォルトでは、 Pig Latin の文は<a href="perf.html#multi-query-execution">複数クエリ実行</a> で処理されます。</p>
   
   <p>Pig Latin は通常次のように構成されます:</p>
   <ul>
      <li>
         <p>LOAD 文でファイルシステムからデータを読み込む</p>
      </li>
      <li>
         <p>データを処理する一連の変換処理</p>
      </li>
      <li>
         <p>結果を表示する DUMP 文、あるいは結果を保存するための STORE 文</p>
      </li>
   </ul>
<p></p>
   <p>出力を行うためには DUMP 文か STORE 文が必要です。</p>
<ul>
<li>
<p>次の例は、 LOAD 文と FOREACH の妥当性検証を行いますが、実行はしません。</p>
<source>
A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE name;
</source> 
</li>
<li>
<p>次の例は、 LOAD 文、 FOREACH 文、 DUMP 文の妥当性検証を行ったのちに、実行します。</p>
<source>
A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE name;
DUMP B;
(John)
(Mary)
(Bill)
(Joe)
</source>
</li>
</ul>
  
   <!-- ++++++++++++++++++++++++++++++++++ -->   
   <section id="data-load">
   <title>データ読み込み</title>
   <p>データを読み込むには、 <a href="basic.html#LOAD">LOAD 演算子</a>と<a href="udf.html#load-store-functions">ロード関数 (デフォルトは PigStorage 関数) </a>を使ってください。</p>
   </section>
  
   <!-- ++++++++++++++++++++++++++++++++++ -->   
   <section id="data-work-with">
   <title>データ処理</title>
   <p>Pig では様々な方法でデータを変換できます。まずは、次に挙げる演算子に慣れるのがよいでしょう:</p>
   <ul>
      <li>
         <p>データのタプル (すなわち行) を処理するには <a href="basic.html#filter">FILTER</a> 演算子を使います。列を処理するには <a href="basic.html#foreach">FOREACH</a> 演算子を使います。</p>
      </li>
      <li>
         <p>一つの関係の中でデータをグループ化するためには <a href="basic.html#GROUP">GROUP</a> 演算子を使います。複数の関係のデータを結合・グループ化するためには、 <a href="basic.html#COGROUP">COGROUP</a> 演算子、 <a href="basic.html#join-inner">内部 JOIN</a> 演算子、 <a href="basic.html#join-outer">外部 JOIN</a> 演算子を使います。</p>
      </li>
      <li>
         <p>複数の関係をマージするためには <a href="basic.html#UNION">UNION</a> 演算子を使います。一つの関係を複数の関係に分割するためには <a href="basic.html#SPLIT">SPLIT</a> 演算子を使います。</p>
      </li>
   </ul>
   </section>
   
<!-- ++++++++++++++++++++++++++++++++++ --> 
      <section id="data-store">
   <title>中間データの保管</title>

      <p>MapReduce ジョブの中間データは HDFS 上の一時領域に保管されます。この一時領域はあらかじめ HDFS 上に存在している必要があります。一時領域の場所は pig.temp.dir プロパティで設定可能です。プロパティのデフォルト値は "/tmp" です。これは Pig 0.7.0 以前のバージョンでハードコードされていた値と同じです。</p>
      </section>
   
    <section id="data-results">
   <title>最終結果の保管</title>
   <p>ファイルシステムをに結果を書き込むには、 <a href="basic.html#store">STORE</a> 演算子と<a href="udf.html#load-store-functions">ストア関数 (デフォルトは PigStorage 関数) </a>を使います。</p>
   <p><strong>注記:</strong> テスト段階やデバッグ段階では、 DUMP を使って端末に結果を表示することができます。しかし、本番環境では STORE 演算子を使うべきです (<a href="perf.html#store-dump">Store 対 Dump</a> を読んでください) 。</p>
   </section> 

 <!-- ++++++++++++++++++++++++++++++++++ -->     
   <section id="debug">
   <title>Pig Latin のデバッグ</title>
   <p>Pig Latin にはデバッグ支援のための演算子があります:</p>
   <ul>
      <li>
         <p><a href="test.html#DUMP">DUMP 演算子:</a> 結果を端末に表示します。</p>
      </li>
      <li>
         <p><a href="test.html#DESCRIBE">DESCRIBE 演算子:</a> 関係のスキーマを表示します。</p>
      </li>
      <li>
         <p><a href="test.html#EXPLAIN">EXPLAIN 演算子:</a> 論理実行計画、物理実行計画、 MapReduce 実行計画を表示します。</p>
      </li>
      <li>
         <p><a href="test.html#ILLUSTRATE">ILLUSTRATE 演算子:</a> 文が実行される過程をステップ・バイ・ステップで表示します。</p>
      </li>
   </ul>
</section> 
</section>  


<!-- ================================================================== -->
<!-- PIG PROPERTIES -->
<section id="properties">
<title>Pig のプロパティ</title>
   <p>Pig の動作をカスタマイズするための Java プロパティは数多くあります。Pig のプロパティの一覧は <a href="cmds.html#help">help properties</a> コマンドで確認できます。すべてのプロパティは省略可能であり、必須のものはありません。</p>
<p></p>
<p id="pig-properties">Pig のプロパティを指定するためには次の仕組みが使えます:</p>
<ul>
	<li>pig.properties ファイル (pig.properties ファイルを含むディレクトリをクラスパスに追加)</li>
        <li>コマンドラインオプション -D にプロパティを指定 (例: pig -Dpig.tmpfilecompression=true)</li>
	<li>コマンドラインオプション -P にプロパティファイルを指定 (例: pig -P mypig.properties)</li>
	<li><a href="cmds.html#set">set</a> コマンド (例: set pig.exec.nocombiner true)</li>
</ul>
<p><strong>注記:</strong> プロパティファイルの書式は Java 標準の書式です。</p>
<p>優先順位は次のとおりです: pig.properties &lt; -D プロパティ &lt; -P プロパティファイル &lt; set コマンド。これはつまり、 -D オプション、 -P オプション、プロパティファイルで同じプロパティか指定されたら、 -P オプションで指定したプロパティファイルに記載された値が優先されます。</p>

<p id="hadoop-properties">Hadoop のプロパティを指定するためには、次の仕組みが使えます:</p>
<ul>
	<li>hadoop-site.xml ファイル (hadoop-site.xml ファイルを含むディレクトリをクラスパスに追加)</li>
	<li>コマンドラインオプション -D にプロパティを指定 (例: pig –Dmapreduce.task.profile=true)</li>
	<li>コマンドラインオプション -P にプロパティファイルを指定 (例: pig -P property_file)</li>
	<li><a href="cmds.html#set">set</a> コマンド (例: set mapred.map.tasks.speculative.execution false)</li>
</ul>
<p></p>
<p>優先順位は Pig のプロパティと同様です: hadoop-site.xml &lt; -D Hadoop プロパティ &lt; -P プロパティファイル &lt; set コマンド。</p>
<p>Hadoop properties are not interpreted by Pig but are passed directly to Hadoop. Any Hadoop property can be passed this way. </p>
<p>Pig は Hadoop のプロパティを解釈せず、 Hadoop にそのまま引き渡します。すべての Hadoop プロパティが同じやり方で引き渡されます。</p>
<p>Pig が収集するすべてのプロパティ (Hadoop プロパティを含む) は、 UDFContext オブジェクトを介して UDF からアクセス可能です。プロパティを取得するためには、 getJobConf メソッドを使います。</p>
</section>  


  <!-- ==================================================================== -->
  <!-- PIG TUTORIAL -->
  <section id="tutorial">
<title>Pig チュートリアル </title>

<p>ローカルモードや MapReduce モードでどのように Pig スクリプトを走らせるか、 Pig チュートリアルによって学ぶことができます (<a href="#execution-modes">実行モード</a> を参照)。</p>

<p>チュートリアルをはじめるためには、次の準備作業が必要です:</p>

<ol>
<li>JAVA_HOME 環境変数が Java のインストール場所を指していることを確認</li>
<li>PATH 環境変数が pig コマンドのあるディレクトリを含んでいることを確認 (これによって pig コマンドでチュートリアルが実行できるようになります)
<source>
$ export PATH=/&lt;my-path-to-pig&gt;/pig-0.9.0/bin:$PATH 
</source>
</li>
<li>PIG_HOME 環境変数を設定:
<source>
$ export PIG_HOME=/&lt;my-path-to-pig&gt;/pig-0.9.0 
</source></li>
<li>pigtutorial.tar.gz ファイルを作成:
<ul>
    <li>Pig チュートリアルのディレクトリに移動 (.../pig-0.9.0/tutorial) 。</li>
	<li>チュートリアルディレクトリ内の build.xml ファイルを編集。
<source>
変更前:   &lt;property name="pigjar" value="../pig.jar" /&gt;
変更後:   &lt;property name="pigjar" value="../pig-0.9.0-core.jar" /&gt;
</source>
	</li>
	<li>チュートリアルディレクトリ内で "ant" コマンドを実行。これで pigtutorial.tar.gz ファイルが作成される。
	</li>
</ul>

</li>
<li>作成された pigtutorial.tar.gz ファイルを作業用のディレクトリに移動。</li>
<li>pigtutorial.tar.gz ファイルを展開。
<source>
$ tar -xzf pigtutorial.tar.gz
</source>
</li>
<li>pigtmp という名前の新しいディレクトリが作成されます。このディレクトリは <a href="#Pig+Tutorial+Files">Pig チュートリアルファイル</a> を含んでいます。これらのファイルは Hadoop 0.20.2 で動作します。また、 <a href="#pig-script-1">Pig スクリプト 1</a> と <a href="#pig-script-2">Pig スクリプト 2</a> を実行するのに必要なものすべてが含まれています。</li>
</ol>





 <!-- ++++++++++++++++++++++++++++++++++ --> 
<section>
<title>ローカルモードで Pig スクリプトを実行</title>

<p>ローカルモードで Pig スクリプトを実行するためには次のようにします:</p>
<ol>
<li>pigtmp ディレクトリに移動</li>
<li>次のコマンドを実行 (script1-local.pig か script2-local.pig のどちらかを利用)
<source>
$ pig -x local script1-local.pig
</source>
</li>
<li>part-r-00000 ディレクトリに格納された結果ファイルの内容を確認。
<p>次のような Hadoop の警告メッセージが出力されていますが、無視して構いません:</p>
<source>
2010-04-08 12:55:33,642 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics 
- Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
</source>
</li>
</ol>
</section>

 <!-- ++++++++++++++++++++++++++++++++++ --> 
<section>
<title>MapReduce モードで Pig スクリプトを実行</title>

<p>Pig スクリプトを MapReduce モードで実行するためには、次のようにします:</p>
<ol>
<li>pigtmp ディレクトリに移動。</li>
<li>excite.log.bz2 ファイルを HDFS 上のディレクトリにコピー。
<source>
$ hadoop fs –copyFromLocal excite.log.bz2 .
</source>
</li>

<li>PIG_CLASSPATH 環境変数にクラスタ設定ファイルが格納されたディレクトリを指定 (core-site.xml, hdfs-site.xml, mapred-site.xml を含むディレクトリ):
<source>
export PIG_CLASSPATH=/mycluster/conf
</source></li>
<li>Set the HADOOP_CONF_DIR environment variable to the location of the cluster configuration directory:
<source>
export HADOOP_CONF_DIR=/mycluster/conf
</source></li>

<li>次のコマンドを実行 (script1-hadoop.pig か script2-hadoop.pig のどちらかを利用):
<source>
$ pig script1-hadoop.pig
</source>
</li>

<li>HDFS 上の script1-hadoop-results あるいは script2-hadoop-results ディレクトリに格納された結果ファイルの内容を確認:
<source>
$ hadoop fs -ls script1-hadoop-results
$ hadoop fs -cat 'script1-hadoop-results/*' | less
</source>
</li>
</ol>
</section>

 <!-- ++++++++++++++++++++++++++++++++++ -->   
<section>
<title>Pig チュートリアルファイル</title>

<p>Pig チュートリアルファイル (pigtutorial.tar.gz) の内容は次のとおりです。</p>

<table>
<tr>
<td>
<p> <strong>ファイル</strong> </p>
</td>
<td>
<p> <strong>説明</strong></p>
</td>
</tr>
<tr>
<td>
<p> pig.jar </p>
</td>
<td>
<p>Pig の JAR ファイル</p>
</td>
</tr>
<tr>
<td>
<p> tutorial.jar </p>
</td>
<td>
<p>ユーザ定義関数 (UDF) と Java クラス</p>
</td>
</tr>
<tr>
<td>
<p> script1-local.pig </p>
</td>
<td>
<p>Pig スクリプト 1, 検索文字列の人気度 (ローカルモード)</p>
</td>
</tr>
<tr>
<td>
<p> script1-hadoop.pig </p>
</td>
<td>
<p>Pig スクリプト 1, 検索文字列の人気度 (MapReduce モード)</p>
</td>
</tr>
<tr>
<td>
<p> script2-local.pig </p>
</td>
<td>
<p>Pig スクリプト 2, 検索文字列の人気度の時系列変化 (ローカルモード)</p>
</td>
</tr>
<tr>
<td>
<p> script2-hadoop.pig </p>
</td>
<td>
<p>Pig スクリプト 2, 検索文字列の人気度の時系列変化 (MapReduce モード) </p>
</td>
</tr>
<tr>
<td>
<p> excite-small.log </p>
</td>
<td>
<p>Excite 検索エンジンのログファイル (ローカルモード)</p>
</td>
</tr>
<tr>
<td>
<p> excite.log.bz2 </p>
</td>
<td>
<p>Excite 検索エンジンのログファイル (MapReduce モード)</p>
</td>
</tr>
</table>


<p>ユーザ定義関数 (UDF) は次のとおりです。</p>

<table>
<tr>
<td>
<p> <strong>UDF</strong> </p>
</td>
<td>
<p> <strong>説明</strong></p>
</td>
</tr>
<tr>
<td>
<p> ExtractHour </p>
</td>
<td>
<p>レコードから時刻を抽出。</p>
</td>
</tr>
<tr>
<td>
<p> NGramGenerator </p>
</td>
<td>
<p>単語群から n-gram を作成。</p>
</td>
</tr>
<tr>
<td>
<p> NonURLDetector </p>
</td>
<td>
<p>検索文字列が空であるか URL であるレコードを削除。</p>
</td>
</tr>
<tr>
<td>
<p> ScoreGenerator </p>
</td>
<td>
<p>N-gram の人気度を計算。</p>
</td>
</tr>
<tr>
<td>
<p> ToLower </p>
</td>
<td>
<p>検索文字列を小文字に変換。</p>
</td>
</tr>
<tr>
<td>
<p> TutorialUtil </p>
</td>
<td>
<p>検索文字列を単語に分解。</p>
</td>
</tr>
</table>

</section>

 <!-- ++++++++++++++++++++++++++++++++++ -->   
<section id="pig-script-1">
<title> Pig Script 1: Query Phrase Popularity</title>

<p>The Query Phrase Popularity script (script1-local.pig or script1-hadoop.pig) processes a search query log file from the Excite search engine and finds search phrases that occur with particular high frequency during certain times of the day. </p>
<p>The script is shown here: </p>
<ul>
<li><p> Register the tutorial JAR file so that the included UDFs can be called in the script. </p>
</li>
</ul>

<source>
REGISTER ./tutorial.jar; 
</source>
<ul>
<li><p> Use the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields <strong>user</strong>, <strong>time</strong>, and <strong>query</strong>.  </p>
</li>
</ul>

<source>
raw = LOAD 'excite.log' USING PigStorage('\t') AS (user, time, query);
</source>
<ul>
<li><p> Call the NonURLDetector UDF to remove records if the query field is empty or a URL.  </p>
</li>
</ul>

<source>
clean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);
</source>
<ul>
<li><p> Call the ToLower UDF to change the query field to lowercase.  </p>
</li>
</ul>

<source>
clean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;
</source>
<ul>
<li><p> Because the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour (HH) from the time field. </p>
</li>
</ul>

<source>
houred = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;
</source>
<ul>
<li><p> Call the NGramGenerator UDF to compose the n-grams of the query. </p>
</li>
</ul>

<source>
ngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;
</source>
<ul>
<li><p> Use the DISTINCT operator to get the unique n-grams for all records.  </p>
</li>
</ul>

<source>
ngramed2 = DISTINCT ngramed1;
</source>
<ul>
<li><p> Use the GROUP operator to group records by n-gram and hour. </p>
</li>
</ul>

<source>
hour_frequency1 = GROUP ngramed2 BY (ngram, hour);
</source>
<ul>
<li><p> Use the COUNTfunction to get the count (occurrences) of each n-gram.  </p>
</li>
</ul>

<source>
hour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;
</source>
<ul>
<li><p> Use the GROUP operator to group records by n-gram only. Each group now corresponds to a distinct n-gram and has the count for each hour. </p>
</li>
</ul>

<source>
uniq_frequency1 = GROUP hour_frequency2 BY group::ngram;
</source>
<ul>
<li><p> For each group, identify the hour in which this n-gram is used with a particularly high frequency. Call the ScoreGenerator UDF to calculate a "popularity" score for the n-gram. </p>
</li>
</ul>

<source>
uniq_frequency2 = FOREACH uniq_frequency1 GENERATE flatten($0), flatten(org.apache.pig.tutorial.ScoreGenerator($1));
</source>
<ul>
<li><p> Use the FOREACH-GENERATE operator to assign names to the fields.  </p>
</li>
</ul>

<source>
uniq_frequency3 = FOREACH uniq_frequency2 GENERATE $1 as hour, $0 as ngram, $2 as score, $3 as count, $4 as mean;
</source>
<ul>
<li><p> Use the FILTER operator to move all records with a score less than or equal to 2.0. </p>
</li>
</ul>

<source>
filtered_uniq_frequency = FILTER uniq_frequency3 BY score &gt; 2.0;
</source>
<ul>
<li><p> Use the ORDER operator to sort the remaining records by hour and score. </p>
</li>
</ul>

<source>
ordered_uniq_frequency = ORDER filtered_uniq_frequency BY hour, score;
</source>
<ul>
<li><p> Use the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: <strong>hour</strong>, <strong>ngram</strong>, <strong>score</strong>, <strong>count</strong>, <strong>mean</strong>. </p>
</li>
</ul>
<source>
STORE ordered_uniq_frequency INTO '/tmp/tutorial-results' USING PigStorage(); 
</source>
</section>

 <!-- ++++++++++++++++++++++++++++++++++ -->   
<section id="pig-script-2">
<title>Pig Script 2: Temporal Query Phrase Popularity</title>

<p>The Temporal Query Phrase Popularity script (script2-local.pig or script2-hadoop.pig) processes a search query log file from the Excite search engine and compares the occurrence of frequency of search phrases across two time periods separated by twelve hours. </p>
<p>The script is shown here: </p>
<ul>
<li><p> Register the tutorial JAR file so that the user defined functions (UDFs) can be called in the script. </p>
</li>
</ul>

<source>
REGISTER ./tutorial.jar;
</source>
<ul>
<li><p> Use the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields <strong>user</strong>, <strong>time</strong>, and <strong>query</strong>. </p>
</li>
</ul>

<source>
raw = LOAD 'excite.log' USING PigStorage('\t') AS (user, time, query);
</source>
<ul>
<li><p> Call the NonURLDetector UDF to remove records if the query field is empty or a URL. </p>
</li>
</ul>

<source>
clean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);
</source>
<ul>
<li><p> Call the ToLower UDF to change the query field to lowercase. </p>
</li>
</ul>

<source>
clean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;
</source>
<ul>
<li><p> Because the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour from the time field. </p>
</li>
</ul>

<source>
houred = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;
</source>
<ul>
<li><p> Call the NGramGenerator UDF to compose the n-grams of the query. </p>
</li>
</ul>

<source>
ngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;
</source>
<ul>
<li><p> Use the DISTINCT operator to get the unique n-grams for all records.  </p>
</li>
</ul>

<source>
ngramed2 = DISTINCT ngramed1;
</source>
<ul>
<li><p> Use the GROUP operator to group the records by n-gram and hour.  </p>
</li>
</ul>

<source>
hour_frequency1 = GROUP ngramed2 BY (ngram, hour);
</source>
<ul>
<li><p> Use the COUNT function to get the count (occurrences) of each n-gram.  </p>
</li>
</ul>

<source>
hour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;
</source>
<ul>
<li><p> Use the FOREACH-GENERATE operator to assign names to the fields. </p>
</li>
</ul>

<source>
hour_frequency3 = FOREACH hour_frequency2 GENERATE $0 as ngram, $1 as hour, $2 as count;
</source>
<ul>
<li><p> Use the  FILTERoperator to get the n-grams for hour ‘00’  </p>
</li>
</ul>

<source>
hour00 = FILTER hour_frequency2 BY hour eq '00';
</source>
<ul>
<li><p> Uses the FILTER operators to get the n-grams for hour ‘12’ </p>
</li>
</ul>

<source>
hour12 = FILTER hour_frequency3 BY hour eq '12';
</source>
<ul>
<li><p> Use the JOIN operator to get the n-grams that appear in both hours. </p>
</li>
</ul>

<source>
same = JOIN hour00 BY $0, hour12 BY $0;
</source>
<ul>
<li><p> Use the FOREACH-GENERATE operator to record their frequency. </p>
</li>
</ul>

<source>
same1 = FOREACH same GENERATE hour_frequency2::hour00::group::ngram as ngram, $2 as count00, $5 as count12;
</source>
<ul>
<li><p> Use the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: <strong>hour</strong>, <strong>count00</strong>, <strong>count12</strong>. </p>
</li>
</ul>

<source>
STORE same1 INTO '/tmp/tutorial-join-results' USING PigStorage();
</source>
</section>
</section>


</body>
</document>
