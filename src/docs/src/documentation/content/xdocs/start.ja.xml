<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  See the NOTICE file distributed with
  this work for additional information regarding copyright ownership.
  The ASF licenses this file to You under the Apache License, Version 2.0
  (the "License"); you may not use this file except in compliance with
  the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN" "http://forrest.apache.org/dtd/document-v20.dtd">
<document>
  <header>
    <title>Getting Started (はじめよう！)</title>
  </header>
  <body>
  
<!-- ========================================================== -->  

<!-- SET UP PIG -->
 <section>
		<title>Pig のセットアップ</title>
	
<!-- ++++++++++++++++++++++++++++++++++ -->
 <section id="req">
 <title>システム要件</title>
 <p><strong>必須</strong></p>
      <p>Unix ユーザ、 Windows ユーザとも次のソフトウェアが必要です:</p>
		<ul>
		  <li> <strong>Hadoop 0.20.2, 020.203, 020.204,  0.20.205, 1.0.0, 1.0.1, or 0.23.0, 0.23.1</strong> - <a href="http://hadoop.apache.org/common/releases.html">http://hadoop.apache.org/common/releases.html</a> (Pig が使う Hadoop のバージョンを変えるには、 HADOOP_HOME 環境変数に Hadop をインストールしたディレクトリを設定してください。 HADOOP_HOME 環境変数を設定しない場合は、 Pig に組み込まれたバージョンが使われます。現在 Pig には Hadoop 1.0.0 が組み込まれています)</li>
		  <li> <strong>Java 1.6</strong> - <a href="http://java.sun.com/javase/downloads/index.jsp">http://java.sun.com/javase/downloads/index.jsp</a> (JAVA_HOME 環境変数に Java インストールディレクトリを指定してください)</li>	
		</ul>
		<p></p>
    <p>Windows ユーザはこれ以外に Cygwin と Perl パッケージをインストールする必要があります: <a href="http://www.cygwin.com/"> http://www.cygwin.com/</a></p>

<p></p>
 <p><strong>任意</strong></p>
 		<ul>
          <li> <strong>Python 2.5</strong> - <a href="http://jython.org/downloads.html">http://jython.org/downloads.html</a> (Python UDF を使う場合、または Pig を Python に組み込む場合)</li>
          <li> <strong>JavaScript 1.7</strong> - <a href="https://developer.mozilla.org/en/Rhino_downloads_archive">https://developer.mozilla.org/en/Rhino_downloads_archive</a> および <a href="http://mirrors.ibiblio.org/pub/mirrors/maven2/rhino/js/">http://mirrors.ibiblio.org/pub/mirrors/maven2/rhino/js/</a>  (JavaScript UDF を使う場合、または Pig を JavaScript に組み込む場合) </li>		  
          <li> <strong>JRuby 1.6.7</strong> - <a href="http://www.jruby.org/download">http://www.jruby.org/download</a> (JRuby UDF を使う場合) </li>
		  <li> <strong>Ant 1.7</strong> - <a href="http://ant.apache.org/">http://ant.apache.org/</a> (ビルド用) </li>
		  <li> <strong>JUnit 4.5</strong> - <a href="http://junit.sourceforge.net/">http://junit.sourceforge.net/</a> (単体テスト用) </li>
		</ul>
 
  </section>         
   
<!-- ++++++++++++++++++++++++++++++++++ -->        
 <section id="download">
 <title>Pig のダウンロード</title>
	<p>Pig を入手するには、次のようにしてください:</p>
	
	<ol>
	<li>Apache ダウンロードミラーから新しめの安定バージョンをダウンロード
	(<a href="http://hadoop.apache.org/pig/releases.html">Pig Releases</a> を見てください) 。</li>
	
    <li>ダウンロードした Pig のアーカイブを解凍して、次のことを確認:
	    <ul>
	    <li>Pig を実行するためのシェルスクリプト pig が bin ディレクトリに配置されていること (/pig-n.n.n/bin/pig) 。
        Pig 実行のための環境変数についてシェルスクリプト内に説明があります。</li>
	    <li>Pig のプロパティファイル pig.properties が conf ディレクトリ (/pig-n.n.n/conf/pig.properties) に配置されていること。他の場所に設定ファイルを置く時は、 PIG_CONF_DIR 環境変数にディレクトリを指定する必要があります。</li>
	</ul>	
	</li>
	<li>/pig-n.n.n/bin をパスに追加。 bash, sh, ksh では export コマンドを、 tcsh, csh では setenv コマンドを使ってください。例: <br></br>
	<code>$ export PATH=/&lt;my-path-to-pig&gt;/pig-n.n.n/bin:$PATH</code>
</li>
<li>
次のコマンドで、 Pig がインストールできていることを確認: <code>$ pig -help</code>
</li>
</ol>

</section>  

<!-- ++++++++++++++++++++++++++++++++++ -->
<section id="build">
<title>Pig のビルド</title>
      <p>Pig のビルドは、次のようにしてください:</p>
     <ol>
	  <li>Subversion リポジトリから Pig をチェックアウト: <code>svn co http://svn.apache.org/repos/asf/pig/trunk</code> </li>
      <li>トップディレクトリでビルドを実行: <code>ant</code> <br></br>
      トップディレクトリに pig.jar が作られていたら成功</li>	
	  <li>pig.jar をテスト: <code>ant test</code></li>
     </ol>
 </section>
</section>

  <!-- ==================================================================== -->
    
   <!-- RUNNING PIG  -->
   <section id="run">
	<title>Pig の実行</title> 
	<p>Pig はいくつかのモードで実行できます。</p>
	<table>
	<tr>
	<td></td>
    <td><strong>ローカルモード</strong></td>
    <td><strong>Mapreduce モード</strong></td>
	</tr>
	<tr>
	<td><strong>対話モード</strong></td>
    <td>yes</td>
    <td>yes</td>
	</tr>
	<tr>
	<td><strong>バッチモード</strong> </td>
    <td>yes</td>
    <td>yes</td>
	</tr>
	</table>
	
	<!-- ++++++++++++++++++++++++++++++++++ -->
	   <section id="execution-modes">
	<title>実行モード</title> 
<p>Pig には二種類の実行モードがあります: </p>
<ul>
<li><strong>ローカルモード</strong> - Pig をローカルモードで実行するのに必要なのはマシンが一台だけです。全ファイルがローカルマシンに配置され、ローカルマシン上で実行されます。ローカルモードで実行するには -x フラグをつけてください (pig -x local) 。
</li>
  <li><strong>MapReduce モード</strong> - Pig を MapReduceモードで実行するには Hadoop クラスタが必要です。MapReduce モードはデフォルトのモードです。 -x フラグで MapReduce モードでの実行を指定できますが、<em>必須ではありません</em> (pig あるいは pig -x mapreduce).
</li>
</ul>
<p></p>

<p>どちらのモードも "pig" コマンドで実行できます (実体は Perl スクリプト bin/pig) 。または、 "java" コマンドで実行することも可能です (java -cp pig.jar ...) 。
</p>


<section>
<title>例</title>

<p>pig コマンドでローカルモードと MapReduce モードの双方で実行する例。</p>
<source>
/* ローカルモード */
$ pig -x local ...
 
 
/* MapReduce モード */
$ pig ...
あるいは
$ pig -x mapreduce ...
</source>

<p>java コマンドでローカルモードと MapReduce モードの双方で実行する例。</p>
<source>
/* ローカルモード */
$ java -cp pig.jar org.apache.pig.Main -x local ...


/* MapReduce モード */
$ java -cp pig.jar org.apache.pig.Main ...
あるいは
$ java -cp pig.jar org.apache.pig.Main -x mapreduce ...
</source>

</section>
</section>

<!-- ++++++++++++++++++++++++++++++++++ -->
<section id="interactive-mode">
<title>対話モード</title>
<p>Grunt シェルは Pig の対話モードです。Grunt シェルを起動するには下記のように "pig" コマンドを実行します。 Grunt シェルでは Pig Latin 文と Pig コマンドを対話的に実行できます。
</p>

<section>
<title>例</title>
<p>下記の Pig Latin は /etc/passwd ファイルからすべてのユーザ ID を列挙します。はじめに /etc/passwd ファイルを作業ディレクトリにコピーしてください。次に、 "pig" コマンドで Grunt シェルを起動してください (ローカルモードあるいは MapReduce モードで) 。次に、Grunt シェルのプロンプトで Pig Latin 文を対話的に入力してください (その際、文ごとにセミコロンを忘れないように) 。 DUMP 演算子を打つと結果が端末に表示されます</p>
<source>
grunt&gt; A = load 'passwd' using PigStorage(':'); 
grunt&gt; B = foreach A generate $0 as id; 
grunt&gt; dump B; 
</source>

<p><strong>ローカルモード</strong></p>
<source>
$ pig -x local
... - Connecting to ...
grunt> 
</source>

<p><strong>MapReduce モード</strong> </p>
<source>
$ pig -x mapreduce
... - Connecting to ...
grunt> 

あるいは

$ pig 
... - Connecting to ...
grunt> 
</source>
</section>
</section>

<!-- ++++++++++++++++++++++++++++++++++ -->
<section id="batch-mode">
<title>バッチモード</title>

<p><a href="#pig-scripts">Pig スクリプト</a> と "pig" コマンド (ローカルモードあるいは MapReduce モード) を使って Pig をバッチモードで実行できます。</p>

<section>
<title>例</title>

<p>下記の Pig スクリプト (id.pig) は /etc/passwd ファイルから全ユーザ ID を列挙します。はじめに /etc/passwd ファイルを作業ディレクトリにコピーしてください。次に、コマンドラインから Pig スクリプトを実行してください (ローカルモードあるいは MapReduce モードで) 。最後の STORE 演算子で結果がファイル (id.out) に出力されます。</p>
<source>
/* id.pig */

A = load 'passwd' using PigStorage(':');  -- パスワードファイルをロード
B = foreach A generate $0 as id;  -- ユーザ ID を抜き出す
store B into 'id.out'; -- 結果を id.out に書き込む
</source>

<p><strong>ローカルモード</strong></p>
<source>
$ pig -x local id.pig
</source>
<p><strong>MapReduce モード</strong> </p>
<source>
$ pig id.pig
あるいは
$ pig -x mapreduce id.pig
</source>
</section>

  <!-- ==================================================================== -->
    
   <!-- PIG SCRIPTS -->
   <section id="pig-scripts">
	<title>Pig スクリプト</title>
	
<p>Pig Latin 文と Pig コマンドは Pig スクリプトとして 1 ファイルにまとめられます。特に必要でない限り、 Pig スクリプトの拡張子は *.pig とするのがよい習慣です。</p>	
	
<p>Pig スクリプトはコマンドラインからも Grunt シェルからも実行できます
(詳しくは <a href="cmds.html#run">run</a> コマンドと <a href="cmds.html#exec">exec</a> コマンドを見てください) 。</p>
	
<p><a href="cont.html#Parameter-Sub">パラメータ置換</a>を使うとコマンドラインパラメータから値が渡せます。</p>

<!-- +++++++++++++++++++++++++++++++++++++++++++ -->	
   <p id="comments"><strong>スクリプト中のコメント</strong></p>
   
   <p>Pig スクリプトの中にはコメントを書くことができます:</p>
   <ul>
      <li>
        <p>複数行コメントは 「/* ... */」</p>
      </li>
      <li>
        <p>単一行コメントは 「-- ...」</p>
      </li>
   </ul>
<source>
/* myscript.pig
単純なスクリプトです
3 つの Pig Latin 文で構成されています
*/

A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float); -- データをロード
B = FOREACH A GENERATE name;  -- データ変換
DUMP B;  -- 結果を取得
</source>   
	
<!-- +++++++++++++++++++++++++++++++++++++++++++ -->		

<p id="dfs"><strong>スクリプトと分散ファイルシステム</strong></p>

<p>Pig は HDFS, Amazon S3 といったような分散ファイルシステム上の Pig スクリプトの実行 (および Jar ファイルの登録) をサポートしています。実行の際にはスクリプトのフル URI が必要です (Jar ファイルについては <a href="basic.html#register">REGISTER</a> を見てください) 。たとえば、 HDFS 上の Pig スクリプトを実行するためには、次のようにします:</p>
<source>
$ pig hdfs://nn.mydomain.com:9020/myscripts/script.pig
</source> 
</section>	
</section>
</section>

  <!-- ==================================================================== -->
    
   <!-- PIG LATIN STATEMENTS -->
   <section id="pl-statements">
	<title>Pig Latin Statements</title>	
   <p>Pig Latin statements are the basic constructs you use to process data using Pig. 
   A Pig Latin statement is an operator that takes a <a href="basic.html#relations">relation</a> as input and produces another relation as output. 
   (This definition applies to all Pig Latin operators except LOAD and STORE which read data from and write data to the file system.) 
   Pig Latin statements may include <a href="basic.html#Expressions">expressions</a> and <a href="basic.html#Schemas">schemas</a>. 
   Pig Latin statements can span multiple lines and must end with a semi-colon ( ; ).  
   By default, Pig Latin statements are processed using <a href="perf.html#multi-query-execution">multi-query execution</a>.  
 </p>
   
   <p>Pig Latin statements are generally organized as follows:</p>
   <ul>
      <li>
         <p>A LOAD statement to read data from the file system. </p>
      </li>
      <li>
         <p>A series of "transformation" statements to process the data. </p>
      </li>
      <li>
         <p>A DUMP statement to view results or a STORE statement to save the results.</p>
      </li>
   </ul>
<p></p>
   <p>Note that a DUMP or STORE statement is required to generate output.</p>
<ul>
<li>
<p>In this example Pig will validate, but not execute, the LOAD and FOREACH statements.</p>
<source>
A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE name;
</source> 
</li>
<li>
<p>In this example, Pig will validate and then execute the LOAD, FOREACH, and DUMP statements.</p>
<source>
A = LOAD 'student' USING PigStorage() AS (name:chararray, age:int, gpa:float);
B = FOREACH A GENERATE name;
DUMP B;
(John)
(Mary)
(Bill)
(Joe)
</source>
</li>
</ul>
  
   <!-- ++++++++++++++++++++++++++++++++++ -->   
   <section id="data-load">
   <title>Loading Data</title>
   <p>Use the  <a href="basic.html#LOAD">LOAD</a> operator and the <a href="udf.html#load-store-functions">load/store functions</a> to read data into Pig (PigStorage is the default load function).</p>
   </section>
  
   <!-- ++++++++++++++++++++++++++++++++++ -->   
   <section id="data-work-with">
   <title>Working with Data</title>
   <p>Pig allows you to transform data in many ways. As a starting point, become familiar with these operators:</p>
   <ul>
      <li>
         <p>Use the <a href="basic.html#FILTER">FILTER</a> operator to work with tuples or rows of data. 
         Use the <a href="basic.html#FOREACH">FOREACH</a> operator to work with columns of data.</p>
      </li>
      <li>
         <p>Use the <a href="basic.html#GROUP ">GROUP</a> operator to group data in a single relation. 
         Use the <a href="basic.html#COGROUP ">COGROUP</a>,
         <a href="basic.html#join-inner">inner JOIN</a>, and
         <a href="basic.html#join-outer">outer JOIN</a>
         operators  to group or join data in two or more relations.</p>
      </li>
      <li>
         <p>Use the <a href="basic.html#UNION">UNION</a> operator to merge the contents of two or more relations. 
         Use the <a href="basic.html#SPLIT">SPLIT</a> operator to partition the contents of a relation into multiple relations.</p>
      </li>
   </ul>
   </section>
   
<!-- ++++++++++++++++++++++++++++++++++ --> 
      <section id="data-store">
   <title>Storing Intermediate Results</title>

      <p>Pig stores the intermediate data generated between MapReduce jobs in a temporary location on HDFS. 
   This location must already exist on HDFS prior to use. 
   This location can be configured using the pig.temp.dir property. The property's default value is "/tmp" which is the same 
   as the hardcoded location in Pig 0.7.0 and earlier versions. </p>
      </section>
   
    <section id="data-results">
   <title>Storing Final Results</title>
   <p>Use the  <a href="basic.html#STORE">STORE</a> operator and the <a href="udf.html#load-store-functions">load/store functions</a> 
   to write results to the file system (PigStorage is the default store function). </p>
<p><strong>Note:</strong> During the testing/debugging phase of your implementation, you can use DUMP to display results to your terminal screen. 
However, in a production environment you always want to use the STORE operator to save your results (see <a href="perf.html#Store-Dump">Store vs. Dump</a>).</p>   
   </section> 

 <!-- ++++++++++++++++++++++++++++++++++ -->     
   <section id="debug">
   <title>Debugging Pig Latin</title>
   <p>Pig Latin provides operators that can help you debug your Pig Latin statements:</p>
   <ul>
      <li>
         <p>Use the  <a href="test.html#DUMP">DUMP</a> operator to display results to your terminal screen. </p>
      </li>
      <li>
         <p>Use the  <a href="test.html#DESCRIBE">DESCRIBE</a> operator to review the schema of a relation.</p>
      </li>
      <li>
         <p>Use the  <a href="test.html#EXPLAIN">EXPLAIN</a> operator to view the logical, physical, or map reduce execution plans to compute a relation.</p>
      </li>
      <li>
         <p>Use the  <a href="test.html#ILLUSTRATE">ILLUSTRATE</a> operator to view the step-by-step execution of a series of statements.</p>
      </li>
   </ul>
</section> 
</section>  


<!-- ================================================================== -->
<!-- PIG PROPERTIES -->
<section id="properties">
<title>Pig Properties</title>
   <p>Pig supports a number of Java properties that you can use to customize Pig behavior. You can retrieve a list of the properties using the <a href="cmds.html#help">help properties</a> command. All of these properties are optional; none are required. </p>
<p></p>
<p id="pig-properties">To specify Pig properties use one of these mechanisms:</p>
<ul>
	<li>The pig.properties file (add the directory that contains the pig.properties file to the classpath)</li>
	<li>The -D command line option and a Pig property (pig -Dpig.tmpfilecompression=true)</li>
	<li>The -P command line option and a properties file (pig -P mypig.properties)</li>
	<li>The <a href="cmds.html#set">set</a> command (set pig.exec.nocombiner true)</li>
</ul>
<p><strong>Note:</strong> The properties file uses standard Java property file format.</p>
<p>The following precedence order is supported: pig.properties &lt; -D Pig property &lt; -P properties file &lt; set command. This means that if the same property is provided using the –D command line option as well as the –P command line option and a properties file, the value of the property in the properties file will take precedence.</p>

<p id="hadoop-properties">To specify Hadoop properties you can use the same mechanisms:</p>
<ul>
	<li>The hadoop-site.xml file (add the directory that contains the hadoop-site.xml file to the classpath)</li>
	<li>The -D command line option and a Hadoop property (pig –Dmapreduce.task.profile=true) </li>
	<li>The -P command line option and a property file (pig -P property_file)</li>
	<li>The <a href="cmds.html#set">set</a> command (set mapred.map.tasks.speculative.execution false)</li>
</ul>
<p></p>
<p>The same precedence holds: hadoop-site.xml &lt; -D Hadoop property &lt; -P properties_file &lt; set command.</p>
<p>Hadoop properties are not interpreted by Pig but are passed directly to Hadoop. Any Hadoop property can be passed this way. </p>
<p>All properties that Pig collects, including Hadoop properties, are available to any UDF via the UDFContext object. To get access to the properties, you can call the getJobConf method.</p>
</section>  


  <!-- ==================================================================== -->
  <!-- PIG TUTORIAL -->
  <section id="tutorial">
<title>Pig Tutorial </title>

<p>The Pig tutorial shows you how to run Pig scripts using Pig's local mode and mapreduce mode (see <a href="#execution-modes">Execution Modes</a>).</p>

<p>To get started, do the following preliminary tasks:</p>

<ol>
<li>Make sure the JAVA_HOME environment variable is set the root of your Java installation.</li>
<li>Make sure your PATH includes bin/pig (this enables you to run the tutorials using the "pig" command). 
<source>
$ export PATH=/&lt;my-path-to-pig&gt;/pig-0.9.0/bin:$PATH 
</source>
</li>
<li>Set the PIG_HOME environment variable:
<source>
$ export PIG_HOME=/&lt;my-path-to-pig&gt;/pig-0.9.0 
</source></li>
<li>Create the pigtutorial.tar.gz file:
<ul>
    <li>Move to the Pig tutorial directory (.../pig-0.9.0/tutorial).</li>
	<li>Edit the build.xml file in the tutorial directory. 
<source>
Change this:   &lt;property name="pigjar" value="../pig.jar" /&gt;
To this:       &lt;property name="pigjar" value="../pig-0.9.0-core.jar" /&gt;
</source>
	</li>
	<li>Run the "ant" command from the tutorial directory. This will create the pigtutorial.tar.gz file.
	</li>
</ul>

</li>
<li>Copy the pigtutorial.tar.gz file from the Pig tutorial directory to your local directory. </li>
<li>Unzip the pigtutorial.tar.gz file.
<source>
$ tar -xzf pigtutorial.tar.gz
</source>
</li>
<li>A new directory named pigtmp is created. This directory contains the <a href="#Pig+Tutorial+Files">Pig Tutorial Files</a>. These files work with Hadoop 0.20.2 and include everything you need to run <a href="#pig-script-1">Pig Script 1</a> and <a href="#pig-script-2">Pig Script 2</a>.</li>
</ol>





 <!-- ++++++++++++++++++++++++++++++++++ --> 
<section>
<title> Running the Pig Scripts in Local Mode</title>

<p>To run the Pig scripts in local mode, do the following: </p>
<ol>
<li>Move to the pigtmp directory.</li>
<li>Execute the following command (using either script1-local.pig or script2-local.pig). 
<source>
$ pig -x local script1-local.pig
</source>
</li>
<li>Review the result files, located in the part-r-00000 directory.
<p>The output may contain a few Hadoop warnings which can be ignored:</p>
<source>
2010-04-08 12:55:33,642 [main] INFO  org.apache.hadoop.metrics.jvm.JvmMetrics 
- Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
</source>
</li>
</ol>
</section>

 <!-- ++++++++++++++++++++++++++++++++++ --> 
<section>
<title> Running the Pig Scripts in Mapreduce Mode</title>

<p>To run the Pig scripts in mapreduce mode, do the following: </p>
<ol>
<li>Move to the pigtmp directory.</li>
<li>Copy the excite.log.bz2 file from the pigtmp directory to the HDFS directory.
<source>
$ hadoop fs –copyFromLocal excite.log.bz2 .
</source>
</li>

<li>Set the PIG_CLASSPATH environment variable to the location of the cluster configuration directory (the directory that contains the core-site.xml, hdfs-site.xml and mapred-site.xml files):
<source>
export PIG_CLASSPATH=/mycluster/conf
</source></li>
<li>Set the HADOOP_CONF_DIR environment variable to the location of the cluster configuration directory:
<source>
export HADOOP_CONF_DIR=/mycluster/conf
</source></li>

<li>Execute the following command (using either script1-hadoop.pig or script2-hadoop.pig):
<source>
$ pig script1-hadoop.pig
</source>
</li>

<li>Review the result files, located in the script1-hadoop-results or script2-hadoop-results HDFS directory:
<source>
$ hadoop fs -ls script1-hadoop-results
$ hadoop fs -cat 'script1-hadoop-results/*' | less
</source>
</li>
</ol>
</section>

 <!-- ++++++++++++++++++++++++++++++++++ -->   
<section>
<title> Pig Tutorial Files</title>

<p>The contents of the Pig tutorial file (pigtutorial.tar.gz) are described here. </p>

<table>
<tr>
<td>
<p> <strong>File</strong> </p>
</td>
<td>
<p> <strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p> pig.jar </p>
</td>
<td>
<p> Pig JAR file </p>
</td>
</tr>
<tr>
<td>
<p> tutorial.jar </p>
</td>
<td>
<p> User defined functions (UDFs) and Java classes </p>
</td>
</tr>
<tr>
<td>
<p> script1-local.pig </p>
</td>
<td>
<p> Pig Script 1, Query Phrase Popularity (local mode) </p>
</td>
</tr>
<tr>
<td>
<p> script1-hadoop.pig </p>
</td>
<td>
<p> Pig Script 1, Query Phrase Popularity (mapreduce mode) </p>
</td>
</tr>
<tr>
<td>
<p> script2-local.pig </p>
</td>
<td>
<p> Pig Script 2, Temporal Query Phrase Popularity (local mode)</p>
</td>
</tr>
<tr>
<td>
<p> script2-hadoop.pig </p>
</td>
<td>
<p> Pig Script 2, Temporal Query Phrase Popularity (mapreduce mode) </p>
</td>
</tr>
<tr>
<td>
<p> excite-small.log </p>
</td>
<td>
<p> Log file, Excite search engine (local mode) </p>
</td>
</tr>
<tr>
<td>
<p> excite.log.bz2 </p>
</td>
<td>
<p> Log file, Excite search engine (mapreduce) </p>
</td>
</tr>
</table>


<p>The user defined functions (UDFs) are described here. </p>

<table>
<tr>
<td>
<p> <strong>UDF</strong> </p>
</td>
<td>
<p> <strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p> ExtractHour </p>
</td>
<td>
<p> Extracts the hour from the record.</p>
</td>
</tr>
<tr>
<td>
<p> NGramGenerator </p>
</td>
<td>
<p> Composes n-grams from the set of words. </p>
</td>
</tr>
<tr>
<td>
<p> NonURLDetector </p>
</td>
<td>
<p> Removes the record if the query field is empty or a URL. </p>
</td>
</tr>
<tr>
<td>
<p> ScoreGenerator </p>
</td>
<td>
<p> Calculates a "popularity" score for the n-gram.</p>
</td>
</tr>
<tr>
<td>
<p> ToLower </p>
</td>
<td>
<p> Changes the query field to lowercase. </p>
</td>
</tr>
<tr>
<td>
<p> TutorialUtil </p>
</td>
<td>
<p> Divides the query string into a set of words.</p>
</td>
</tr>
</table>

</section>

 <!-- ++++++++++++++++++++++++++++++++++ -->   
<section id="pig-script-1">
<title> Pig Script 1: Query Phrase Popularity</title>

<p>The Query Phrase Popularity script (script1-local.pig or script1-hadoop.pig) processes a search query log file from the Excite search engine and finds search phrases that occur with particular high frequency during certain times of the day. </p>
<p>The script is shown here: </p>
<ul>
<li><p> Register the tutorial JAR file so that the included UDFs can be called in the script. </p>
</li>
</ul>

<source>
REGISTER ./tutorial.jar; 
</source>
<ul>
<li><p> Use the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields <strong>user</strong>, <strong>time</strong>, and <strong>query</strong>.  </p>
</li>
</ul>

<source>
raw = LOAD 'excite.log' USING PigStorage('\t') AS (user, time, query);
</source>
<ul>
<li><p> Call the NonURLDetector UDF to remove records if the query field is empty or a URL.  </p>
</li>
</ul>

<source>
clean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);
</source>
<ul>
<li><p> Call the ToLower UDF to change the query field to lowercase.  </p>
</li>
</ul>

<source>
clean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;
</source>
<ul>
<li><p> Because the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour (HH) from the time field. </p>
</li>
</ul>

<source>
houred = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;
</source>
<ul>
<li><p> Call the NGramGenerator UDF to compose the n-grams of the query. </p>
</li>
</ul>

<source>
ngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;
</source>
<ul>
<li><p> Use the DISTINCT operator to get the unique n-grams for all records.  </p>
</li>
</ul>

<source>
ngramed2 = DISTINCT ngramed1;
</source>
<ul>
<li><p> Use the GROUP operator to group records by n-gram and hour. </p>
</li>
</ul>

<source>
hour_frequency1 = GROUP ngramed2 BY (ngram, hour);
</source>
<ul>
<li><p> Use the COUNTfunction to get the count (occurrences) of each n-gram.  </p>
</li>
</ul>

<source>
hour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;
</source>
<ul>
<li><p> Use the GROUP operator to group records by n-gram only. Each group now corresponds to a distinct n-gram and has the count for each hour. </p>
</li>
</ul>

<source>
uniq_frequency1 = GROUP hour_frequency2 BY group::ngram;
</source>
<ul>
<li><p> For each group, identify the hour in which this n-gram is used with a particularly high frequency. Call the ScoreGenerator UDF to calculate a "popularity" score for the n-gram. </p>
</li>
</ul>

<source>
uniq_frequency2 = FOREACH uniq_frequency1 GENERATE flatten($0), flatten(org.apache.pig.tutorial.ScoreGenerator($1));
</source>
<ul>
<li><p> Use the FOREACH-GENERATE operator to assign names to the fields.  </p>
</li>
</ul>

<source>
uniq_frequency3 = FOREACH uniq_frequency2 GENERATE $1 as hour, $0 as ngram, $2 as score, $3 as count, $4 as mean;
</source>
<ul>
<li><p> Use the FILTER operator to move all records with a score less than or equal to 2.0. </p>
</li>
</ul>

<source>
filtered_uniq_frequency = FILTER uniq_frequency3 BY score &gt; 2.0;
</source>
<ul>
<li><p> Use the ORDER operator to sort the remaining records by hour and score. </p>
</li>
</ul>

<source>
ordered_uniq_frequency = ORDER filtered_uniq_frequency BY hour, score;
</source>
<ul>
<li><p> Use the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: <strong>hour</strong>, <strong>ngram</strong>, <strong>score</strong>, <strong>count</strong>, <strong>mean</strong>. </p>
</li>
</ul>
<source>
STORE ordered_uniq_frequency INTO '/tmp/tutorial-results' USING PigStorage(); 
</source>
</section>

 <!-- ++++++++++++++++++++++++++++++++++ -->   
<section id="pig-script-2">
<title>Pig Script 2: Temporal Query Phrase Popularity</title>

<p>The Temporal Query Phrase Popularity script (script2-local.pig or script2-hadoop.pig) processes a search query log file from the Excite search engine and compares the occurrence of frequency of search phrases across two time periods separated by twelve hours. </p>
<p>The script is shown here: </p>
<ul>
<li><p> Register the tutorial JAR file so that the user defined functions (UDFs) can be called in the script. </p>
</li>
</ul>

<source>
REGISTER ./tutorial.jar;
</source>
<ul>
<li><p> Use the PigStorage function to load the excite log file (excite.log or excite-small.log) into the “raw” bag as an array of records with the fields <strong>user</strong>, <strong>time</strong>, and <strong>query</strong>. </p>
</li>
</ul>

<source>
raw = LOAD 'excite.log' USING PigStorage('\t') AS (user, time, query);
</source>
<ul>
<li><p> Call the NonURLDetector UDF to remove records if the query field is empty or a URL. </p>
</li>
</ul>

<source>
clean1 = FILTER raw BY org.apache.pig.tutorial.NonURLDetector(query);
</source>
<ul>
<li><p> Call the ToLower UDF to change the query field to lowercase. </p>
</li>
</ul>

<source>
clean2 = FOREACH clean1 GENERATE user, time, org.apache.pig.tutorial.ToLower(query) as query;
</source>
<ul>
<li><p> Because the log file only contains queries for a single day, we are only interested in the hour. The excite query log timestamp format is YYMMDDHHMMSS. Call the ExtractHour UDF to extract the hour from the time field. </p>
</li>
</ul>

<source>
houred = FOREACH clean2 GENERATE user, org.apache.pig.tutorial.ExtractHour(time) as hour, query;
</source>
<ul>
<li><p> Call the NGramGenerator UDF to compose the n-grams of the query. </p>
</li>
</ul>

<source>
ngramed1 = FOREACH houred GENERATE user, hour, flatten(org.apache.pig.tutorial.NGramGenerator(query)) as ngram;
</source>
<ul>
<li><p> Use the DISTINCT operator to get the unique n-grams for all records.  </p>
</li>
</ul>

<source>
ngramed2 = DISTINCT ngramed1;
</source>
<ul>
<li><p> Use the GROUP operator to group the records by n-gram and hour.  </p>
</li>
</ul>

<source>
hour_frequency1 = GROUP ngramed2 BY (ngram, hour);
</source>
<ul>
<li><p> Use the COUNT function to get the count (occurrences) of each n-gram.  </p>
</li>
</ul>

<source>
hour_frequency2 = FOREACH hour_frequency1 GENERATE flatten($0), COUNT($1) as count;
</source>
<ul>
<li><p> Use the FOREACH-GENERATE operator to assign names to the fields. </p>
</li>
</ul>

<source>
hour_frequency3 = FOREACH hour_frequency2 GENERATE $0 as ngram, $1 as hour, $2 as count;
</source>
<ul>
<li><p> Use the  FILTERoperator to get the n-grams for hour ‘00’  </p>
</li>
</ul>

<source>
hour00 = FILTER hour_frequency2 BY hour eq '00';
</source>
<ul>
<li><p> Uses the FILTER operators to get the n-grams for hour ‘12’ </p>
</li>
</ul>

<source>
hour12 = FILTER hour_frequency3 BY hour eq '12';
</source>
<ul>
<li><p> Use the JOIN operator to get the n-grams that appear in both hours. </p>
</li>
</ul>

<source>
same = JOIN hour00 BY $0, hour12 BY $0;
</source>
<ul>
<li><p> Use the FOREACH-GENERATE operator to record their frequency. </p>
</li>
</ul>

<source>
same1 = FOREACH same GENERATE hour_frequency2::hour00::group::ngram as ngram, $2 as count00, $5 as count12;
</source>
<ul>
<li><p> Use the PigStorage function to store the results. The output file contains a list of n-grams with the following fields: <strong>hour</strong>, <strong>count00</strong>, <strong>count12</strong>. </p>
</li>
</ul>

<source>
STORE same1 INTO '/tmp/tutorial-join-results' USING PigStorage();
</source>
</section>
</section>


</body>
</document>
