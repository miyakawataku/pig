<?xml version="1.0" encoding="UTF-8"?>
<!--
  Copyright 2002-2004 The Apache Software Foundation

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->

<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
          "http://forrest.apache.org/dtd/document-v20.dtd">


<document>
<header>
<title>ユーザ定義関数</title>
</header>
<body>

<section id="udfs">
<title>導入</title>
<p>Pig は独自の処理を実現するためのユーザ定義関数 (UDF) を広くサポートしています。 Pig の UDF は現在 4 つの言語で実装できます: Java, Python, JavaScript, Ruby です。</p>

<p>もっとも広くサポートされているのは Java 定義の関数です。 Java 定義の関数を使えば、ロード・ストア、値の変換、集計を含む処理のすべての処理がカスタマイズできます。 Java 定義の関数は、 Pig 自体と同じ言語による実装であるため、また <a href="#algebraic-interface">Algebraic</a> や <a href="#accumulator-interface">Accumulator</a> のようなインタフェースをサポートしているため、他の言語よりも効率的です。</p>

<p>Python, JavaScript, Ruby 定義の関数のサポートはより狭いものです。これの関数は、目下改善中の新しい拡張です。今のところサポートされているのは、基本的なインタフェースだけです。ロード・ストア関数はサポートされません。また、 JavaScript と Ruby の関数は、 Java や Python ほどの量のテストを経ていないため、実験的な機能として提供されます。実行時に Pig がスクリプト言語による UDF をみつけると、 Jython, Rhino, JRuby などの JAR ファイルを計算ノードへと自動的に送り込みます。</p>

<p>Pig は Piggy Bank という Java UDF のリポジトリもサポートしています。 Piggy Bank を使うことで、他のユーザが作った Java UDF が利用でき、また自分の作った Java UDF を提供することもできます。</p>

</section>


<!-- ================================================================== -->
<!-- WRITING UDFS -->

<section id="udf-java">
<title>Java UDF を書く</title>

<!-- =============================================================== -->
<section id="eval-functions">
<title>評価関数</title>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="eval-functions-use">
<title>単純な評価関数を使う</title>
<p>評価関数はもっとも一般的な関数の種類です。次のように、 FOREACH 文の中で使えます。</p>

<source>
-- myscript.pig
REGISTER myudfs.jar;
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = FOREACH A GENERATE myudfs.UPPER(name);
DUMP B;
</source>

<p>このスクリプトは次のコマンドで実行できます。簡便のために、このドキュメント中のプログラム例はすべてローカルモードで動かしていますが、 Hadoop モードで動かすことも可能です。 Pig の実行方法についての詳細は、 <a href="start.html#tutorial">Pig チュートリアル</a>を見てください。</p>

<source>
java -cp pig.jar org.apache.pig.Main -x local myscript.pig
</source>

<p>スクリプトの最初の行は、 UDF を含む JAR ファイルの場所を指定しています (JAR ファイル名は引用符でくくりません。引用符をつけると文法エラーになります) 。 JAR ファイルは最初にクラスパス上で探索されます。クラスパス上になかった場合、絶対パスか、 Pig を起動した場所からの相対パスとして JAR ファイルを探索します。 JAR ファイルが見つからなかった場合、エラーメッセージが表示されます: <code>java.io.IOException:&nbsp;Can't&nbsp;read&nbsp;jar&nbsp;file:&nbsp;myudfs.jar</code></p>

<p><code>REGISTER</code> コマンドは 1 つのスクリプトで複数回使えます。関数の完全修飾名が複数の JAR ファイル中で見つかる場合、 Java の意味論に整合して、最初に見つかったファイルが使われます。</p>

<p>UDF の名前はパッケージ名で完全修飾する必要があります。正しく完全修飾名を指定しない場合、エラーになります: <code>java.io.IOException:&nbsp;Cannot&nbsp;instantiate:UPPER</code> 。また、関数名は大文字・小文字を区別します (UPPER と upper は別の名前として扱われます) 。 UDF は 1 つ以上の引数を取ることができます。関数のドキュメンテーションは正確なシグネチャを記載するべきです。</p>

<p>この例に示した関数は 1 つの ASCII 文字列を引数に取り、大文字にして戻します。 SQL の関数による列変換に親しんだ方なら、 UPPER 関数が同種のものだと分かることでしょう。ただし、後に見るように、 Pig の評価関数は単なる列変換関数にとどまらず、集計関数やフィルタ関数を包含しています。</p>

<p>UDF を使うだけであれば、以上が UDF について知るべきことのほとんどすべてです。</p>

</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="eval-functions-write">
<title>評価関数の書き方</title>
<p><code>UPPER</code> 関数の実装を見てみましょう。</p>

<source>
1  package myudfs;
2  import java.io.IOException;
3  import org.apache.pig.EvalFunc;
4  import org.apache.pig.data.Tuple;
5
6  public class UPPER extends EvalFunc&lt;String&gt;
7  {
8    public String exec(Tuple input) throws IOException {
9        if (input == null || input.size() == 0)
10            return null;
11        try{
12            String str = (String)input.get(0);
13           return str.toUpperCase();
14        }catch(Exception e){
15            throw new IOException("Caught exception processing input row ", e);
16        }
17    }
18  }
</source>

<p>1 行目は関数が <code>myudfs</code> パッケージに属することを意味しています。この UDF クラスはすべての評価関数の基底クラスである <code>EvalFunc</code> クラスを継承し、 UDF の戻り値の型である Java の <code>String</code> を型パラメータとしています。 <code>EvalFunc</code> については後に詳しく見ますが、今必要なことは <code>exec</code> メソッドを実装することです。このメソッドは入力となるタプルごとに呼び出されます。メソッドの引数は Pig スクリプト中で呼び出した関数の引数を順番通りに収めたタプルです。この例では、学生の名前に対応する文字列フィールドが 1 つ収められています。</p>
<p>最初にするべきことは、不正なデータの扱いを決めることです。これはデータの型式に依存します。たとえばデータが <code>bytearray</code> 型だった場合、まだ適切な型に変換されていないことが分かります。この例では、データの型式が期待したものと異なった場合、 NULL を戻す必要があります。一方で、もしも入力データが別の型だった場合には、型変換はすでに行われて、データは適切な型になっているものと判断できます。 15 行目でエラーを投げているのがその場合です。</p>
<p>9 から 10 行目では入力データが null か空でないかをチェックして、そうだった場合には null を戻しています。</p>
<p>12 から 13 行目では見てのとおり関数本来の仕事をしています。</p>
<p>関数が実装できたので、コンパルして JAR ファイルに固める必要があります。 UDF をコンパイルするためにはあらかじめ <code>pig.jar</code> をビルドする必要があります。 SVN リポジトリから Pig のコードをチェックアウトして pig.jar を作るには、次のコマンドを実行してください:</p>

<source>
svn co http://svn.apache.org/repos/asf/pig/trunk
cd trunk
ant
</source>

<p>上記コマンドの結果として、作業ディレクトリに pig.jar ができているはずです。次のコマンドで関数をコンパイルして、 JAR に固めます。</p>

<source>
cd myudfs
javac -cp pig.jar UPPER.java
cd ..
jar -cf myudfs.jar myudfs
</source>

<p>上記コマンドの結果として、作業ディレクトリ内に <code>myudfs.jar</code> ができているはずです。この JAR ファイルは前章の説明のとおり Pig スクリプトから使用できます。</p>

</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="aggregate-functions">
<title>集約関数</title>
<p>集約関数も一般的な評価関数の種別です。次のスクリプトのように、集約関数は通常グループ化されたデータに対して用います。</p>

<source>
-- myscript2.pig
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = GROUP A BY name;
C = FOREACH B GENERATE group, COUNT(A);
DUMP C;
</source>

<p>上のスクリプトは <code>COUNT</code> 関数を使って同じ名前の学生の数を数えています。このスクリプトについては注意すべき点が二つあります。まずは <code>REGISTER</code> なしで関数を使っていること、二つ目は関数が完全修飾されていないことです。これは <code>COUNT</code> が組み込み関数であって Pig に同梱されているためです。組み込み関数と UDF との違いはこの二点だけです。組み込み関数については、このドキュメント内で詳述します。</p>
</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="algebraic-interface">
<title>Algebraic インタフェース</title>

<p>集約関数はバッグを引数に取りスカラ値を戻す評価関数です。多くの集約関数は、分散環境で差分更新的に計算できるという、興味深く有用な特質を持っています。 Pig ではこのような関数を「代数的 (algebraic)」と呼びます。 <code>COUNT</code> は代数的関数の一例です。なぜなら、データの部分ごとに要素数を数え、それを合計することで、全体の要素数が得られるからです。 Hadoop の世界ではこのような処理は、 Mapper と Combiner で部分ごとの計算を行い、 Reducer で最後にまとめあげます。</p>

<p>代数的な集約関数をこのように実装することは、性能上とても重要です。 COUNT 関数の実装を例にとってどういうことか見てみましょう (エラー処理その他いくらかのコードは読み易さのために省略しています。完全なコードは <a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/builtin/COUNT.java?view=markup">SVN リポジトリ</a> で見てください) 。</p>

<source>
public class COUNT extends EvalFunc&lt;Long&gt; implements Algebraic{
    public Long exec(Tuple input) throws IOException {return count(input);}
    public String getInitial() {return Initial.class.getName();}
    public String getIntermed() {return Intermed.class.getName();}
    public String getFinal() {return Final.class.getName();}
    static public class Initial extends EvalFunc&lt;Tuple&gt; {
        public Tuple exec(Tuple input) throws IOException {return TupleFactory.getInstance().newTuple(count(input));}
    }
    static public class Intermed extends EvalFunc&lt;Tuple&gt; {
        public Tuple exec(Tuple input) throws IOException {return TupleFactory.getInstance().newTuple(sum(input));}
    }
    static public class Final extends EvalFunc&lt;Long&gt; {
        public Tuple exec(Tuple input) throws IOException {return sum(input);}
    }
    static protected Long count(Tuple input) throws ExecException {
        Object values = input.get(0);
        if (values instanceof DataBag) return ((DataBag)values).size();
        else if (values instanceof Map) return new Long(((Map)values).size());
    }
    static protected Long sum(Tuple input) throws ExecException, NumberFormatException {
        DataBag values = (DataBag)input.get(0);
        long sum = 0;
        for (Iterator (Tuple) it = values.iterator(); it.hasNext();) {
            Tuple t = it.next();
            sum += (Long)t.get(0);
        }
        return sum;
    }
}
</source>

<p><code>COUNT</code> 関数は <code>Algebraic</code> インタフェースを実装しています。 <code>Algebraic</code> インタフェースの内容は次のとおりです: </p>

<source>
public interface Algebraic{
    public String getInitial();
    public String getIntermed();
    public String getFinal();
}
</source>

<p>代数的関数を書くするためには、 <code>Algebraic</code> を実装して、またネストしたクラスとして <code>EvalFunc</code> を継承したクラスを 3 つ定義する必要があります。 <code>Initial</code> クラスの <code>exec</code> メソッドは関数の入力を引数として最初に呼ばれ、中間的な結果を戻します。 <code>Intermed</code> クラスの <code>exec</code> メソッドは、 <code>Initial</code> クラスか <code>Intermed</code> クラスの戻り値を引数として、 0 回以上呼ばれ、中間的な結果を戻します。最後に、 <code>Final</code> クラスの <code>exec</code> メソッドが関数の戻り値となるスカラ値を生成します。</p>

<p>Hadoop の世界では次のようになります。 <code>Initial</code> クラスの <code>exec</code> メソッドは Mapper ごとに呼ばれ、中間的な結果を戻します。 <code>Intermed</code> クラスの <code>exec</code> メソッドは、 Combiner (実行されることもされないこともあります) ごとに呼ばれ、中間的な結果を戻します。 <code>Final</code> クラスの <code>exec</code> メソッドは Reducer ごとに呼ばれ、最終的な結果を生成します。</p>
<p><code>COUNT</code> 関数が、どのように実装されているか見てみましょう。 <code>Initial</code> クラスと <code>Intermed</code> クラスの <code>exec</code> メソッドは <code>Tuple</code> 型でパラメータ化されており、 <code>Final</code> クラスの <code>exec</code> メソッドは <code>COUNT</code> 関数の戻り値の型である <code>Long</code> 型でパラメータ化されています。また、それぞれのクラスの完全修飾名は <code>getInitial</code>, <code>getIntermed</code>, <code>getFinal</code> の各メソッドで戻す必要があります。</p>
</section>

<!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="accumulator-interface">
<title>Accumulator インタフェース</title>

<p>Pig では、 GROUP 演算子や COGROUP 演算子の結果のデータをバッグに詰めて UDF に渡す時に、メモリ消費の問題が起こり得ます。</p>

<p>この問題は、 UDF を代数的関数にすることによってある程度は対処できます。代数的関数は Combiner を使うので、データを複数の実行段階 (Mapper, Combiner, Reducer) で少しずつ処理できるからです。しかしながら、代数的でなく、したがって Combiner を使わないながら、全部のデータを一度に処理する必要がない関数も存在します。</p>

<p>Accumulator インタフェースはこのような UDF でメモリ消費を抑えるために設計されています。このインタフェースを実装した関数に対して Pig は、同じキーに紐づくデータを連続して少しずつ渡します。このようにデータを少しずつ処理する UDF は、次のインタフェースを実装します:</p>
<source>
public interface Accumulator &lt;T&gt; {
   /**
    * タプルを処理します。バッグは現在のキーに紐づく 0 以上の個数のタプルを保持しています。
    */
    public void accumulate(Tuple b) throws IOException;
    /**
     * 現在のキーに紐づくタプルがすべて渡された後に呼ばれます。
     * @return 現在のキーに対する UDF の戻り値を戻します。
     */
    public T getValue();
    /**
     * getValue() の後に、次のキーに備えるために呼ばれます。
     */
    public void cleanup();
}
</source>

<p>いくつか注意すべき点があります:</p>

<ol>
	<li>UDF は EvalFunc を継承し、必要なメソッドをすべて実装する必要があります。</li>
	<li>関数が代数的だったとしても、 FOREACH 文の中で Accumulator 関数と一緒に使うのであれば、 Algebraic インタフェースに加えて Accumulator インタフェースを実装する必要があります。</li>
	<li>インタフェースは関数の戻り値の型でパラメータ化されます。</li>
	<li>accumulate メソッドは、 1 個以上のタプルを詰めたバッグを引数として、 1 回以上呼ばれます。 accumulate メソッドの引数の中身は exec メソッドと同じく、 UDF の全引数です。ただし、その内の 1 つはバッグになります。</li>
	<li>getValue メソッドは、キーに紐づくタプルがすべて処理された後に、結果を得るために呼ばれます。</li>
	<li>cleanup メソッドは、 getValue が呼ばれた後に、次のキーの準備をするために呼ばれます。</li>
</ol>


<p>整数版の MAX 関数を Accumulator インタフェースで実装すると次のようになります:</p>
<source>
public class IntMax extends EvalFunc&lt;Integer&gt; implements Algebraic, Accumulator&lt;Integer&gt; {
    …….
    /* Accumulator interface */
    
    private Integer intermediateMax = null;
    
    @Override
    public void accumulate(Tuple b) throws IOException {
        try {
            Integer curMax = max(b);
            if (curMax == null) {
                return;
            }
            /* if bag is not null, initialize intermediateMax to negative infinity */
            if (intermediateMax == null) {
                intermediateMax = Integer.MIN_VALUE;
            }
            intermediateMax = java.lang.Math.max(intermediateMax, curMax);
        } catch (ExecException ee) {
            throw ee;
        } catch (Exception e) {
            int errCode = 2106;
            String msg = "Error while computing max in " + this.getClass().getSimpleName();
            throw new ExecException(msg, errCode, PigException.BUG, e);           
        }
    }

    @Override
    public void cleanup() {
        intermediateMax = null;
    }

    @Override
    public Integer getValue() {
        return intermediateMax;
    }
}
</source>
</section>


<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="filter-functions">
<title>フィルタ関数</title>
<p>フィルタ関数はブーリアン値を戻す評価関数です。フィルタ関数はブーリアン式が置ける場所ならどこでも使えます。たとえば、 <code>FILTER</code> 演算子や条件演算子式で使えます。</p>

<p>次の例では <code>IsEmpy</code> 組み込み関数を結合に使っています。</p>

<source>
-- 内部結合
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = LOAD 'voter_data' AS (name: chararray, age: int, registration: chararay, contributions: float);
C = COGROUP A BY name, B BY name;
D = FILTER C BY not IsEmpty(A);
E = FILTER D BY not IsEmpty(B);
F = FOREACH E GENERATE flatten(A), flatten(B);
DUMP F;
</source>

<p>実は、フィルタを行わなくても同じ結果が得られます。 <code>FOREACH</code> と <code>FLATTEN</code> の組み合わせによってクロス積が生成され、クロス積は空のバッグを消し去るからです。しかしながら、あらかじめ空のバッグを排除することによって、クロス積の入力が減らせるため、より効率的になります。</p>

<source>
-- 全外部結合
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = LOAD 'voter_data' AS (name: chararray, age: int, registration: chararay, contributions: float);
C = COGROUP A BY name, B BY name;
D = FOREACH C GENERATE group, flatten((IsEmpty(A) ? null : A)), flatten((IsEmpty(B) ? null : B));
dump D;
</source>

<p><code>IsEmpty</code> 関数の実装は次のようになります:</p>

<source>
import java.io.IOException;
import java.util.Map;

import org.apache.pig.FilterFunc;
import org.apache.pig.PigException;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.DataType;

/**
 * バッグかマップが空かどうかを確かめます。
 */
public class IsEmpty extends FilterFunc {

    @Override
    public Boolean exec(Tuple input) throws IOException {
        try {
            Object values = input.get(0);
            if (values instanceof DataBag)
                return ((DataBag)values).size() == 0;
            else if (values instanceof Map)
                return ((Map)values).size() == 0;
            else {
                int errCode = 2102;
                String msg = "Cannot test a " +
                DataType.findTypeName(values) + " for emptiness.";
                throw new ExecException(msg, errCode, PigException.BUG);
            }
        } catch (ExecException ee) {
            throw ee;
        }
    }
} 
</source>
</section>

<section id="udf_simulation">
<title>シミュレーションによる UDF 実装</title>
<p>複雑な種類の EvalFunc を実装すると、単純な種類の実装は自動的に提供されます。つまり、 <a href="#algebraic-interface">Algebraic</a> インタフェースを実装すれば、 Accumulator インタフェースの実装と単純な EvalFunc の exec メソッドはただで得られます。同じように、 <a href="#accumulator-interface">Accumulator</a> インタフェースを実装すると、単純な EvalFunc の exec メソッドはただで得られますが、 Algebraic の実装は得られません。これら自動的に生成される実装はシミュレーションに基づくので、最も効率的という訳ではありません。もし Accumulator インタフェースや EvalFunc の exec メソッドを効率的にしたいのであれば、自分でそれらのメソッドを実装してください。実装したメソッドは、自動的に提供される実装に優先して使用されます。</p>
</section>
<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="pig-types">
<title>Pig の型と Java の型</title>
<p>Pig の型システムについて重要なことは、次の表に示すとおり、 Pig が Java のネイティブの型をほぼそのまま使うという事です。</p>


<table>
<tr>
<th>
Pig の型
</th>
<th>
Java のクラス
</th>
</tr>
<tr>
<td>
<p> bytearray </p>
</td>
<td>
<p> DataByteArray </p>
</td>
</tr>
<tr>
<td>
<p> chararray </p>
</td>
<td>
<p> String </p>
</td>
</tr>
<tr>
<td>
<p> int </p>
</td>
<td>
<p> Integer </p>
</td>
</tr>
<tr>
<td>
<p> long </p>
</td>
<td>
<p> Long </p>
</td>
</tr>
<tr>
<td>
<p> float </p>
</td>
<td>
<p> Float </p>
</td>
</tr>
<tr>
<td>
<p> double </p>
</td>
<td>
<p> Double </p>
</td>
</tr>
<tr>
<td>
<p> tuple </p>
</td>
<td>
<p> Tuple </p>
</td>
</tr>
<tr>
<td>
<p> bag </p>
</td>
<td>
<p> DataBag </p>
</td>
</tr>
<tr>
<td>
<p> map </p>
</td>
<td>
<p> Map&lt;Object, Object&gt; </p>
</td>
</tr>
</table>

<p>Pig 独自の型は <a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/data/">SVN リポジトリ</a> で見られます。</p>
<p><code>Tuple</code> と <code>DataBag</code> は実クラスでなくインタフェースである点で特殊です。これにより、ユーザは独自のタプル型やバッグ型を作ることで Pig が拡張できます。このため、 UDF はバッグやタプルを直接インスタンス化できません。インスタンスを作るためには、 <code>TupleFactory</code> や <code>BagFactory</code> といったファクトリクラスを使う必要があります。</p>
<p><code>TOKENIZE</code> 組み込み関数を見るとバッグとタプルがどのように作られるか分かります。この関数は文字列を引数に取って、文字列中の単語のバッグを戻します (今のところ Pig のバッグが保持できるのはタプルだけであることに注意してください) 。</p>

<source>
package org.apache.pig.builtin;

import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;

public class TOKENIZE extends EvalFunc&lt;DataBag&gt; {
    TupleFactory mTupleFactory = TupleFactory.getInstance();
    BagFactory mBagFactory = BagFactory.getInstance();

    public DataBag exec(Tuple input) throws IOException 
        try {
            DataBag output = mBagFactory.newDefaultBag();
            Object o = input.get(0);
            if ((o instanceof String)) {
                throw new IOException("Expected input to be chararray, but  got " + o.getClass().getName());
            }
            StringTokenizer tok = new StringTokenizer((String)o, " \",()*", false);
            while (tok.hasMoreTokens()) output.add(mTupleFactory.newTuple(tok.nextToken()));
            return output;
        } catch (ExecException ee) {
            // エラー処理
        }
    }
}
</source>
</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="schemas">
<title>スキーマと Java UDF</title>

<p>Pig はバリデーションと効率化のために型情報を使います。そのため、 UDF を型伝播に参加させることは重要です。普通 UDF を実装する際に出力スキーマを Pig に伝えるために何かをすることはありません。これは、 Pig が Javaの<a href="http://www.oracle.com/technetwork/articles/java/javareflection-1536171.html">リフレクション</a>を使って型情報を見つけ出すためです。 UDF の戻り値がスカラ値かマップの時には、特に何をする必要もありません。しかしながら、 UDF がタプルか、タプルのバッグを戻す時には、 Pig がタプルの構造を把握できるように助けてあげる必要があります。</p>
<p>UDF がタプルかバッグを戻し、スキーマ情報が与えられていない場合、 Pig はタプルが bytearray 型のフィールドを 1 つ持っているものと仮定します。そうでない場合、スキーマを指定しないことが失敗の原因になります。これについては次で見てみましょう。</p>
<p>ここで、 <code>Swap</code> UDF が 2 つの引数を取って順番をひっくり返すものとします。 UDF がスキーマを指定しない場合に、次のスクリプトがどうなるか見てみましょう。</p>

<source>
register myudfs.jar;
A = load 'student_data' as (name: chararray, age: int, gpa: float);
B = foreach A generate flatten(myudfs.Swap(name, age)), gpa;
C = foreach B generate $2;
D = limit B 20;
dump D;
</source>

<p>このスクリプトは 4 行目の <code>C = foreach B generate $2;</code> で次のエラーを発生します。</p>

<source>
java.io.IOException: Out of bound access. Trying to access non-existent column: 2. Schema {bytearray,gpa: float} has 2 column(s).
</source>

<p>これは、 Pig が B には 2 つの列しかないと認識しているのに、 4 行目で 3 つ目の列を指定しているからです (Pig では列のインデックスは 0 から始まります) 。</p>
<p>スキーマを含んだ関数は次のように書きます。</p>

<source>
package myudfs;
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

public class Swap extends EvalFunc&lt;Tuple&gt; {
    public Tuple exec(Tuple input) throws IOException {
        if (input == null || input.size()   2
            return null;
        try{
            Tuple output = TupleFactory.getInstance().newTuple(2);
            output.set(0, input.get(1));
            output.set(1, input.get(0));
            return output;
        } catch(Exception e){
            System.err.println("Failed to process input; error - " + e.getMessage());
            return null;
        }
    }
    public Schema outputSchema(Schema input) {
        try{
            Schema tupleSchema = new Schema();
            tupleSchema.add(input.getField(1));
            tupleSchema.add(input.getField(0));
            return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input),tupleSchema, DataType.TUPLE));
        }catch (Exception e){
                return null;
        }
    }
}
</source>

<p>この関数はタプル型のフィールドのスキーマ (<code>FieldSchema</code> 型) を生成しています。フィールド名は <code>EvalFunc</code> の <code>getSchemaName</code> メソッドを使って構築しています。結果のフィールド名は、最初の引数に渡した UDF 名を含んでおり、一連の番号によって一意性が保証されます。さきほどのスクリプトで <code>dump D;</code> の代わりに <code>describe B;</code> を実行すると、次の出力が得られます:</p>

<source>
B: {myudfs.swap_age_3::age: int,myudfs.swap_age_3::name: chararray,gpa: float}
</source>

<p><code>FieldSchema</code> のコンストラクタの 2 番目の引数はフィールドのスキーマで、今回の場合は 2 つのフィールドを含むタプルになります。 3 番目の引数はスキーマの型を表し、今回の場合は <code>TUPLE</code> になります。サポートされているスキーマの型は <code>org.apache.pig.data.DataType</code> クラスで定義されています。</p>

<source>
public class DataType {
    public static final byte UNKNOWN   =   0;
    public static final byte NULL      =   1;
    public static final byte BOOLEAN   =   5; // Pig の内部使用のみ
    public static final byte BYTE      =   6; // Pig の内部使用のみ
    public static final byte INTEGER   =  10;
    public static final byte LONG      =  15;
    public static final byte FLOAT     =  20;
    public static final byte DOUBLE    =  25;
    public static final byte BYTEARRAY =  50;
    public static final byte CHARARRAY =  55;
    public static final byte MAP       = 100;
    public static final byte TUPLE     = 110;
    public static final byte BAG       = 120;
    public static final byte ERROR     =  -1;
    // more code here
}
</source>

<p>スキーマを定義するためには <code>org.apache.pig.data.DataType</code> クラスをインポートする必要があります。また、スキーマのクラスである <code>org.apache.pig.impl.logicalLayer.schema.Schema</code> クラスもインポートする必要があります。</p>
<p>上の例では、タプル型の出力スキーマをどのように作るかを見ました。バッグでもほとんど同じです。 <code>TOKENIZE</code> 関数の例を見てみましょう。</p>

<p>見てお分かりのように、 <code>Swap</code> 関数のスキーマ定義と極めて似通っています。相違点の一つは、バッグ中の単語のスキーマを表すために、入力スキーマの情報を再利用するのではなく、フィールドのスキーマを新しく作っていることです。もうひとつの違いは、スキーマの型が TUPLE でなく BAG であることです。</p>

<source>
package org.apache.pig.builtin;

import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

public class TOKENIZE extends EvalFunc&lt;DataBag&gt; {
    TupleFactory mTupleFactory = TupleFactory.getInstance();
    BagFactory mBagFactory = BagFactory.getInstance();
    public DataBag exec(Tuple input) throws IOException {
        try {
            DataBag output = mBagFactory.newDefaultBag();
            Object o = input.get(0);
            if ((o instanceof String)) {
                throw new IOException("Expected input to be chararray, but  got " + o.getClass().getName());
            }
            StringTokenizer tok = new StringTokenizer((String)o, " \",()*", false);
            while (tok.hasMoreTokens()) output.add(mTupleFactory.newTuple(tok.nextToken()));
            return output;
        } catch (ExecException ee) {
            // エラー処理
        }
    }
    public Schema outputSchema(Schema input) {
         try{
             Schema bagSchema = new Schema();
             bagSchema.add(new Schema.FieldSchema("token", DataType.CHARARRAY));

             return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input),
                                                    bagSchema, DataType.BAG));
         }catch (Exception e){
            return null;
         }
    }
}
</source>

<p>スキーマと UDF についてもうひとつ特筆すべきこととして、 UDF で実際にデータを処理する前に、入力データのスキーマが分かるようにしてほしいという要望が上がっています。たとえば、入力タプルを、フィールド名がキーとなるマップに変換したいとしても、現在のところ方法がありません。これはいずれサポートしたい機能です。</p>

</section>


<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="error-handling">
<title>エラー処理</title>

<p>UDF で起きるエラーにはいくつかの種類があります:</p>
<ol>
<li><p>特定の行に影響を与えるが、他の行には影響がないエラー。このようなエラーの例は、不正な入力値やゼロ除算です。このような状況での適切な処理方法は、警告メッセージを出力して、 null を戻すことです。次節で見る <code>ABS</code> 関数は、この手法を採用しています。今のところ、警告メッセージは標準エラーに出力することになっていますが、いずれ UDF にロガーを渡すようにするつもりです。 null を戻すことは、不正な値が bytearray 型である場合にしか意味がないということに注意してください。それ以外の型の場合は、特定の型が既に割り当てられており、引数は適切な値になっているはずです。そうでない場合は内部エラーであり、全体の処理を失敗させる必要があります。両方の場合について、次節の <code>ABS</code> 関数の実装の中で見ていきます。</p></li>
<li><p>処理全体に影響するが、再試行で解決する可能性があるエラー。このようなエラーの例は、参照ファイルが存在せず、開けない場合です。これは環境に依存する一時的な問題である可能性があるため、再試行すればうまく行くかも知れません。この場合、次節の <code>ABS</code> 関数のように、 <code>IOException</code> を投げて Pig にエラーを通知します。</p></li>
<li><p>処理全体に影響し、再試行でも解決できないエラー。このようなエラーの例は、パーミッションが適切でないため、参照ファイルが開けない場合です。今のところ Pig には、このようなエラーに適切に対処する方法がありません。 Hadoop についても同様です。したがってこのようなエラーは上記の 2 と同じやり方で処理します。</p></li>
</ol>

</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="function-overloading">
<title>関数オーバーロード</title>

<p>Pig に型システムが導入される以前、数値計算に使われる値はすべて double 型とみなされていました。しかしながら、実際の型が int や long である場合、これは効率的なやり方ではありません (int が使えるところで double を使うと、約 2 倍の処理時間が掛かった例があります) 。今や Pig は型システムを備えているので、型情報を用い、引数に対して最も効率的な関数を選ぶことができます。</p>
<p>UDF の作者は、性能の向上が見込まれる場合、型を指定したバージョンの実装を提供することが推奨されます。一方で、関数を使う側は、型ごとの実装を気にしなくても正しく動作するようになっています。次の例で見るように、これは関数テーブルの仕組みによって実現しています。</p>
<p>次の例で見る <code>ABS</code> 関数は、引数の数値の絶対値を戻します。</p>

<source>
import java.io.IOException;
import java.util.List;
import java.util.ArrayList;
import org.apache.pig.EvalFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

public class ABS extends EvalFunc&lt;Double&gt; {
    public Double exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        Double d;
        try{
            d = DataType.toDouble(input.get(0));
        } catch (NumberFormatException nfe){
            System.err.println("Failed to process input; error - " + nfe.getMessage());
            return null;
        } catch (Exception e){
            throw new IOException("Caught exception processing input row ", e);
        }
        return Math.abs(d);
    }
    public List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {
        List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt;();
        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.BYTEARRAY))));
        funcList.add(new FuncSpec(DoubleAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.DOUBLE))));
        funcList.add(new FuncSpec(FloatAbs.class.getName(),   new Schema(new Schema.FieldSchema(null, DataType.FLOAT))));
        funcList.add(new FuncSpec(IntAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.INTEGER))));
        funcList.add(new FuncSpec(LongAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.LONG))));
        return funcList;
    }
}
</source>

<p>この例で重要なのは <code>getArgToFuncMapping()</code> メソッドです。このメソッドは、入力スキーマから関数実装クラスへのマッピングのリストを戻します。この例では、メインの <code>ABS</code> クラスは <code>bytearray</code> 型の入力値のみを処理し、その他の型の処理は同パッケージ内の別ファイル内で実装された他のクラスに委譲しています。他のクラスの一例として、 <code>int</code> 型の入力値を処理するクラスを見てみましょう。</p>

<source>
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;

public class IntAbs extends EvalFunc&lt;Integer&gt; {
    public Integer exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        Integer d;
        try{
            d = (Integer)input.get(0);
        } catch (Exception e){
            throw new IOException("Caught exception processing input row ", e);
        }
        return Math.abs(d);
    }
}
</source>


<p>エラー処理について: <code>ABS</code> クラスは、まだ実際の型に変換されていない <code>bytearray</code> 値を処理します。 <code>NumberFormatException</code> が起きた時に null を戻しているのはこのためです。しかしながら <code>IntAbs</code> は実際の型である <code>Integer</code> に変換された上で呼ばれるので、不正なデータは既に弾かれているはずです。入力データを <code>Integer</code> にキャストできない時に例外を投げているのはこのためです。</p>
<p>以上の例は、 UDF がただ一つの引数を取り、引数の型ごとに別々の実装があるという、そこそこ単純な例ですが、もっと複雑な場合もあります。 Pig は「正確合致」の実装を見つけられない場合は、「最良合致」の実装を見つけようとします。「最良合致」の規則は、安全に使える中で最も効率的な実装を見つける、というものです。つまり Pig は、それぞれの引数について、入力の型と同じかより大きい型の中で、最も小さい型の実装を見つけます。型の優先順位は次のとおりです: <code>int&gt;long&gt;float&gt;double</code> 。</p>
<p>例として、後述する <code>piggybank</code> 中の <code>MAX</code> 関数について見てみましょう。この関数は 2 つの値を引数に取って、大きい方の値を戻します。 <code>MAX</code> の関数テーブルは次のとおりです:</p>

<source>
public List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {
    List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt;();
    Util.addToFunctionList(funcList, IntMax.class.getName(), DataType.INTEGER);
    Util.addToFunctionList(funcList, DoubleMax.class.getName(), DataType.DOUBLE);
    Util.addToFunctionList(funcList, FloatMax.class.getName(), DataType.FLOAT);
    Util.addToFunctionList(funcList, LongMax.class.getName(), DataType.LONG);

    return funcList;
}
</source>

<p><code>Util.addToFunctionList</code> はヘルパメソッドです。これは、 2 番目の引数に指定したクラス名と、 3 番目の引数に指定した型のフィールドを 2 つ含むスキーマからなるマッピングを、最初の引数である関数テーブルに追加します。</p>

<p>この関数が Pig スクリプトの中でどのように使われるか見てみましょう:</p>

<source>
REGISTER piggybank.jar
A = LOAD 'student_data' AS (name: chararray, gpa1: float, gpa2: double);
B = FOREACH A GENERATE name, org.apache.pig.piggybank.evaluation.math.MAX(gpa1, gpa2);
DUMP B;
</source>

<p>この例では、関数は <code>float</code> 型の値と <code>double</code> 型の値を引数に取っています。この場合、 double 値を 2 つ取る実装が最良です。 Pig は暗黙キャストを挿入することで、ユーザに代わってこの選択を行います。上記のスクリプトは下記のスクリプトと同じように実行されます:</p>

<source>
A = LOAD 'student_data' AS (name: chararray, gpa1: float, gpa2: double);
B = FOREACH A GENERATE name, org.apache.pig.piggybank.evaluation.math.MAX((double)gpa1, gpa2);
DUMP B;
</source>

<p>「最良合致」の特殊な場合は、スキーマが指定されていないデータを処理する時です。この時、データの型は <code>bytearray</code> と解釈されます。データの型が分からないので、最良合致を見つける方法がありません。唯一、関数テーブルが 1 つの要素だけからなっている場合のみ、キャストが実行されます。この動作は、後方互換性を維持するために役に立ちます。</p>

<p>この章の最初で示した <code>UPPER</code> 関数の例をもう一度見てみましょう。この節で書いたとおり、 <code>UPPER</code> 関数がうまく働くのは <code>chararray</code> 型の値が渡された場合のみです。型が明示的に設定されていないデータが処理できるようにするためには、 1 要素からなる関数テーブルを追加します:</p>

<source>
package myudfs;
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;

public class UPPER extends EvalFunc&lt;String&gt;
{
    public String exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        try{
            String str = (String)input.get(0);
            return str.toUpperCase();
        }catch(Exception e){
            System.err.println("WARN: UPPER: failed to process input; error - " + e.getMessage());
            return null;
        }
    }
    public List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {
        List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt; ();
        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY))));
        return funcList;
    }
}
</source>

<p>こうすれば、次のスクリプトが動くようになります:</p>

<source>
-- myscript.pig
REGISTER myudfs.jar;
A = LOAD 'student_data' AS (name, age, gpa);
B = FOREACH A GENERATE myudfs.UPPER(name);
DUMP B;
</source>

</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="reporting-progress">
<title>進行状況の報告</title>

<p>大きな共有システムの勘所は、システムのリソースが効率的に使われるようにすることです。その一つの側面として、暴走して処理が進んでいないプロセスを検出することがあります。 Pig はこの目的のために、ハートビートの仕組みを採用しています。いずれかのタスクがハートビートを送出しなくなったら、システムはタスクが死んだものと判断してこれを殺します。</p>
<p>ほとんどの場合、 UDF による単一のタプルの処理の実行時間はとても短いため、 UDF がハートビートを送出する必要はありません。大きなバッグに対する集約関数でも同様です。なぜなら、バッグのイテレーション処理がハートビートを送出してくれるからです。しかしながら、分単位の時間が掛かる複雑な計算を実行する関数については、コードの中で進行状況を報告する必要があります。これを実施するのはとても簡単です。 <code>EvalFunc</code> クラスが提供している <code>progress</code> メソッドを、 <code>exec</code> メソッドから呼び出すだけです。</p>
<p>例えば、 <code>UPPER</code> 関数では次のようになります:</p>

<source>
public class UPPER extends EvalFunc&lt;String&gt;
{
        public String exec(Tuple input) throws IOException {
                if (input == null || input.size() == 0)
                return null;
                try{
                        progress();
                        String str = (String)input.get(0);
                        return str.toUpperCase();
                }catch(Exception e){
                    throw new IOException("Caught exception processing input row ", e);
                }
        }
}
</source>
</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="distributed-cache">
<title>分散キャッシュを使う</title>
<p>分散キャッシュに送り込む HDFS 上のファイルは、 EvalFunc クラスの getCacheFiles メソッドで指定します。 exec メソッドの中では、ファイルが既に分散キャッシュに入っているものとして処理できます。例を挙げます:</p>
<source>
public class Udfcachetest extends EvalFunc&lt;String&gt; { 

    public String exec(Tuple input) throws IOException { 
        FileReader fr = new FileReader("./smallfile"); 
        BufferedReader d = new BufferedReader(fr); 
        return d.readLine(); 
    } 

    public List&lt;String&gt; getCacheFiles() { 
        List&lt;String&gt; list = new ArrayList&lt;String&gt;(1); 
        list.add("/user/pig/tests/data/small#smallfile"); 
        return list; 
    } 
} 

a = load '1.txt'; 
b = foreach a generate Udfcachetest(*); 
dump b;
</source>
</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="import-lists">
<title>インポートリスト</title>
<p>インポートリストに UDF が属するパッケージを指定することで、 UDF 呼び出しごとに完全修飾名を書く必要がなくなります。インポートリストは、 Pig 起動時にコマンドラインから Java プロパティ udf.import.list で指定します:</p>
<source>
pig -Dudf.import.list=com.yahoo.yst.sds.ULT
</source>
<p>複数の場所を指定することもできます:</p>
<source>
pig -Dudf.import.list=com.yahoo.yst.sds.ULT:org.apache.pig.piggybank.evaluation
</source>
</section>
<p>スクリプト中でインポートされた UDF を使うには、次のようにします:</p>
<source>
myscript.pig:
A = load '/data/SDS/data/searcg_US/20090820' using ULTLoader as (s, m, l);
....

command:
pig -cp sds.jar -Dudf.import.list=com.yahoo.yst.sds.ULT myscript.pig 
</source>

</section>

<!-- =============================================================== -->
<!-- BEGIN LOAD/STORE FUNCTIONS -->
<section id="load-store-functions">
<title>ロード・ストア関数</title>

<p>ロード・ストア関数は Pig にデータを入力する方法と、 Pig からデータを出力する方法を制御します。一つの関数で入力と出力の双方を行う場合もありますが、そうしなくても構いません。</p>
<p>Pig のロード・ストア関数の API は Hadoop の InputFormat と OutputFormat と適合するようにできています。このため、既存の Hadoop の InputFormat や OutputFormat を元にして、少ないコードで LoadFunc や StoreFunc が作れます。データをレコードに読み込む際に最も面倒なところは InputFormat の中で、データを書き込む際に最も面倒なところは OutputFormat の中で行います。このため、 Hadoop の InputFormat と OutputFormat さえ用意されていれば、 Pig が新しい形式のデータを読み書きできるようにすることは容易です。</p>
<p><strong>注意:</strong> LoadFunc と StoreFunc はいずれも Hadoop 0.20 の API を使うようにできています (InputFormat, OutputFormat その他のクラスについて) 。これらのクラスは org.apache.hadoop.mapred パッケージの中ではなく、<strong>新しい</strong> org.apache.hadoop.mapreduce パッケージの中にあります。</p>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="load-functions">
<title> Load Functions</title>
<p id="loadfunc"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadFunc.java?view=markup">LoadFunc</a> 
abstract class has three main methods for loading data and for most use cases it would suffice to extend it. There are three other optional interfaces which can be implemented to achieve extended functionality: </p>

<ul>
<li id="LoadMetadata"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadMetadata.java?view=markup">LoadMetadata</a> 
has methods to deal with metadata - most implementation of loaders don't need to implement this unless they interact with some metadata system. The getSchema() method in this interface provides a way for loader implementations to communicate the schema of the data back to pig. If a loader implementation returns data comprised of fields of real types (rather than DataByteArray fields), it should provide the schema describing the data returned through the getSchema() method. The other methods are concerned with other types of metadata like partition keys and statistics. Implementations can return null return values for these methods if they are not applicable for that implementation.</li>

<li id="LoadPushDown"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadPushDown.java?view=markup">LoadPushDown</a> 
has methods to push operations from Pig runtime into loader implementations. Currently only the pushProjection() method is called by Pig to communicate to the loader the exact fields that are required in the Pig script. The loader implementation can choose to honor the request (return only those fields required by Pig script) or not honor the request (return all fields in the data). If the loader implementation can efficiently honor the request, it should implement LoadPushDown to improve query performance. (Irrespective of whether the implementation can or cannot honor the request, if the implementation also implements getSchema(), the schema returned in getSchema() should describe the entire tuple of data.)
<ul>
	<li id="pushprojection">pushProjection(): This method tells LoadFunc which fields are required in the Pig script, thus enabling LoadFunc to optimize performance by loading only those fields that are needed. pushProjection() takes a requiredFieldList. requiredFieldList is read only and cannot be changed by LoadFunc. requiredFieldList includes a list of requiredField: each requiredField indicates a field required by the Pig script; each requiredField includes index, alias, type (which is reserved for future use), and subFields. Pig will use the column index requiredField.index to communicate with the LoadFunc about the fields required by the Pig script. If the required field is a map, Pig will optionally pass requiredField.subFields which contains a list of keys that the Pig script needs for the map. For example, if the Pig script needs two keys for the map, "key1" and "key2", the subFields for that map will contain two requiredField; the alias field for the first RequiredField will be "key1" and the alias for the second requiredField will be "key2". LoadFunc will use requiredFieldResponse.requiredFieldRequestHonored to indicate whether the pushProjection() request is honored.
</li>
</ul>
</li>

<li id="loadcaster"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadCaster.java?view=markup">LoadCaster</a> 
has methods to convert byte arrays to specific types. A loader implementation should implement this if casts (implicit or explicit) from DataByteArray fields to other types need to be supported. </li>
</ul>

 <p>The LoadFunc abstract class is the main class to extend for implementing a loader. The methods which need to be overridden are explained below:</p>
 <ul>
 <li id="getInputFormat">getInputFormat(): This method is called by Pig to get the InputFormat used by the loader. The methods in the InputFormat (and underlying RecordReader) are called by Pig in the same manner (and in the same context) as by Hadoop in a MapReduce java program. If the InputFormat is a Hadoop packaged one, the implementation should use the new API based one under org.apache.hadoop.mapreduce. If it is a custom InputFormat, it should be implemented using the new API in org.apache.hadoop.mapreduce.<br></br> <br></br> 
 
 If a custom loader using a text-based InputFormat or a file-based InputFormat would like to read files in all subdirectories under a given input directory recursively, then it should use the PigTextInputFormat and PigFileInputFormat classes provided in org.apache.pig.backend.hadoop.executionengine.mapReduceLayer. The Pig InputFormat classes work around a current limitation in the Hadoop TextInputFormat and FileInputFormat classes which only read one level down from the provided input directory. For example, if the input in the load statement is 'dir1' and there are subdirs 'dir2' and 'dir2/dir3' beneath dir1, the Hadoop TextInputFormat and FileInputFormat classes read the files under 'dir1' only. Using PigTextInputFormat or PigFileInputFormat (or by extending them), the files in all the directories can be read. </li>
 
 <li id="setLocation">setLocation(): This method is called by Pig to communicate the load location to the loader. The loader should use this method to communicate the same information to the underlying InputFormat. This method is called multiple times by pig - implementations should bear this in mind and should ensure there are no inconsistent side effects due to the multiple calls. </li>
 
 <li id="prepareToRead">prepareToRead(): Through this method the RecordReader associated with the InputFormat provided by the LoadFunc is passed to the LoadFunc. The RecordReader can then be used by the implementation in getNext() to return a tuple representing a record of data back to pig. </li>
 
 <li id="getnext">getNext(): The meaning of getNext() has not changed and is called by Pig runtime to get the next tuple in the data - in this method the implementation should use the underlying RecordReader and construct the tuple to return. </li>
 </ul>

 <p id="loadfunc-default">The following methods have default implementations in LoadFunc and should be overridden only if needed: </p>
 <ul>
 <li id="setUdfContextSignature">setUdfContextSignature(): This method will be called by Pig both in the front end and back end to pass a unique signature to the Loader. The signature can be used to store into the UDFContext any information which the Loader needs to store between various method invocations in the front end and back end. A use case is to store RequiredFieldList passed to it in LoadPushDown.pushProjection(RequiredFieldList) for use in the back end before returning tuples in getNext(). The default implementation in LoadFunc has an empty body. This method will be called before other methods. </li>
 
 <li id="relativeToAbsolutePath">relativeToAbsolutePath(): Pig runtime will call this method to allow the Loader to convert a relative load location to an absolute location. The default implementation provided in LoadFunc handles this for FileSystem locations. If the load source is something else, loader implementation may choose to override this.</li>
 </ul>

<p><strong>Example Implementation</strong></p>
<p>
The loader implementation in the example is a loader for text data with line delimiter as '\n' and '\t' as default field delimiter (which can be overridden by passing a different field delimiter in the constructor) - this is similar to current PigStorage loader in Pig. The implementation uses an existing Hadoop supported Inputformat - TextInputFormat - as the underlying InputFormat.
</p>
<source>
public class SimpleTextLoader extends LoadFunc {
    protected RecordReader in = null;
    private byte fieldDel = '\t';
    private ArrayList&lt;Object&gt; mProtoTuple = null;
    private TupleFactory mTupleFactory = TupleFactory.getInstance();
    private static final int BUFFER_SIZE = 1024;

    public SimpleTextLoader() {
    }

    /**
     * Constructs a Pig loader that uses specified character as a field delimiter.
     *
     * @param delimiter
     *            the single byte character that is used to separate fields.
     *            ("\t" is the default.)
     */
    public SimpleTextLoader(String delimiter) {
        this();
        if (delimiter.length() == 1) {
            this.fieldDel = (byte)delimiter.charAt(0);
        } else if (delimiter.length() &gt;  1 &amp; &amp; delimiter.charAt(0) == '\\') {
            switch (delimiter.charAt(1)) {
            case 't':
                this.fieldDel = (byte)'\t';
                break;

            case 'x':
               fieldDel =
                    Integer.valueOf(delimiter.substring(2), 16).byteValue();
               break;

            case 'u':
                this.fieldDel =
                    Integer.valueOf(delimiter.substring(2)).byteValue();
                break;

            default:
                throw new RuntimeException("Unknown delimiter " + delimiter);
            }
        } else {
            throw new RuntimeException("PigStorage delimeter must be a single character");
        }
    }

    @Override
    public Tuple getNext() throws IOException {
        try {
            boolean notDone = in.nextKeyValue();
            if (notDone) {
                return null;
            }
            Text value = (Text) in.getCurrentValue();
            byte[] buf = value.getBytes();
            int len = value.getLength();
            int start = 0;

            for (int i = 0; i &lt; len; i++) {
                if (buf[i] == fieldDel) {
                    readField(buf, start, i);
                    start = i + 1;
                }
            }
            // pick up the last field
            readField(buf, start, len);

            Tuple t =  mTupleFactory.newTupleNoCopy(mProtoTuple);
            mProtoTuple = null;
            return t;
        } catch (InterruptedException e) {
            int errCode = 6018;
            String errMsg = "Error while reading input";
            throw new ExecException(errMsg, errCode,
                    PigException.REMOTE_ENVIRONMENT, e);
        }

    }

    private void readField(byte[] buf, int start, int end) {
        if (mProtoTuple == null) {
            mProtoTuple = new ArrayList&lt;Object&gt;();
        }

        if (start == end) {
            // NULL value
            mProtoTuple.add(null);
        } else {
            mProtoTuple.add(new DataByteArray(buf, start, end));
        }
    }

    @Override
    public InputFormat getInputFormat() {
        return new TextInputFormat();
    }

    @Override
    public void prepareToRead(RecordReader reader, PigSplit split) {
        in = reader;
    }

    @Override
    public void setLocation(String location, Job job)
            throws IOException {
        FileInputFormat.setInputPaths(job, location);
    }
}
</source>

</section>
<!-- END LOAD FUNCTION -->

<!-- =============================================================== -->
<section id="store-functions">
<title> Store Functions</title>

<p id="storefunc"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/StoreFunc.java?view=markup">StoreFunc</a> 
abstract class has the main methods for storing data and for most use cases it should suffice to extend it. There is an optional interface which can be implemented to achieve extended functionality: </p>
<ul>
<li id="storemetadata"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/StoreMetadata.java?view=markup">StoreMetadata:</a> 
This interface has methods to interact with metadata systems to store schema and store statistics. This interface is optional and should only be implemented if metadata needs to stored. </li>
</ul>

<p id="storefunc-override">The methods which need to be overridden in StoreFunc are explained below: </p>
<ul>
<li id="getOutputFormat">getOutputFormat(): This method will be called by Pig to get the OutputFormat used by the storer. The methods in the OutputFormat (and underlying RecordWriter and OutputCommitter) will be called by pig in the same manner (and in the same context) as by Hadoop in a map-reduce java program. If the OutputFormat is a hadoop packaged one, the implementation should use the new API based one under org.apache.hadoop.mapreduce. If it is a custom OutputFormat, it should be implemented using the new API under org.apache.hadoop.mapreduce. The checkOutputSpecs() method of the OutputFormat will be called by pig to check the output location up-front. This method will also be called as part of the Hadoop call sequence when the job is launched. So implementations should ensure that this method can be called multiple times without inconsistent side effects. </li>
<li id="setStoreLocation">setStoreLocation(): This method is called by Pig to communicate the store location to the storer. The storer should use this method to communicate the same information to the underlying OutputFormat. This method is called multiple times by pig - implementations should bear in mind that this method is called multiple times and should ensure there are no inconsistent side effects due to the multiple calls. </li>
<li id="prepareToWrite">prepareToWrite(): In the new API, writing of the data is through the OutputFormat provided by the StoreFunc. In prepareToWrite() the RecordWriter associated with the OutputFormat provided by the StoreFunc is passed to the StoreFunc. The RecordWriter can then be used by the implementation in putNext() to write a tuple representing a record of data in a manner expected by the RecordWriter. </li>
<li id="putNext">putNext(): The meaning of putNext() has not changed and is called by Pig runtime to write the next tuple of data - in the new API, this is the method wherein the implementation will use the underlying RecordWriter to write the Tuple out.</li>
</ul>

<p id="storefunc-default">The following methods have default implementations in StoreFunc and should be overridden only if necessary: </p>
<ul>
<li id="setStoreFuncUDFContextSignature">setStoreFuncUDFContextSignature(): This method will be called by Pig both in the front end and back end to pass a unique signature to the Storer. The signature can be used to store into the UDFContext any information which the Storer needs to store between various method invocations in the front end and back end. The default implementation in StoreFunc has an empty body. This method will be called before other methods. 
</li>
<li id="relToAbsPathForStoreLocation">relToAbsPathForStoreLocation(): Pig runtime will call this method to allow the Storer to convert a relative store location to an absolute location. An implementation is provided in StoreFunc which handles this for FileSystem based locations. </li>
<li id="checkschema">checkSchema(): A Store function should implement this function to check that a given schema describing the data to be written is acceptable to it. The default implementation in StoreFunc has an empty body. This method will be called before any calls to setStoreLocation(). </li>
</ul>

<p><strong>Example Implementation</strong></p>
<p>
The storer implementation in the example is a storer for text data with line delimiter as '\n' and '\t' as default field delimiter (which can be overridden by passing a different field delimiter in the constructor) - this is similar to current PigStorage storer in Pig. The implementation uses an existing Hadoop supported OutputFormat - TextOutputFormat as the underlying OutputFormat. 
</p>

<source>
public class SimpleTextStorer extends StoreFunc {
    protected RecordWriter writer = null;

    private byte fieldDel = '\t';
    private static final int BUFFER_SIZE = 1024;
    private static final String UTF8 = "UTF-8";
    public PigStorage() {
    }

    public PigStorage(String delimiter) {
        this();
        if (delimiter.length() == 1) {
            this.fieldDel = (byte)delimiter.charAt(0);
        } else if (delimiter.length() > 1delimiter.charAt(0) == '\\') {
            switch (delimiter.charAt(1)) {
            case 't':
                this.fieldDel = (byte)'\t';
                break;

            case 'x':
               fieldDel =
                    Integer.valueOf(delimiter.substring(2), 16).byteValue();
               break;
            case 'u':
                this.fieldDel =
                    Integer.valueOf(delimiter.substring(2)).byteValue();
                break;

            default:
                throw new RuntimeException("Unknown delimiter " + delimiter);
            }
        } else {
            throw new RuntimeException("PigStorage delimeter must be a single character");
        }
    }

    ByteArrayOutputStream mOut = new ByteArrayOutputStream(BUFFER_SIZE);

    @Override
    public void putNext(Tuple f) throws IOException {
        int sz = f.size();
        for (int i = 0; i &lt; sz; i++) {
            Object field;
            try {
                field = f.get(i);
            } catch (ExecException ee) {
                throw ee;
            }

            putField(field);

            if (i != sz - 1) {
                mOut.write(fieldDel);
            }
        }
        Text text = new Text(mOut.toByteArray());
        try {
            writer.write(null, text);
            mOut.reset();
        } catch (InterruptedException e) {
            throw new IOException(e);
        }
    }

    @SuppressWarnings("unchecked")
    private void putField(Object field) throws IOException {
        //string constants for each delimiter
        String tupleBeginDelim = "(";
        String tupleEndDelim = ")";
        String bagBeginDelim = "{";
        String bagEndDelim = "}";
        String mapBeginDelim = "[";
        String mapEndDelim = "]";
        String fieldDelim = ",";
        String mapKeyValueDelim = "#";

        switch (DataType.findType(field)) {
        case DataType.NULL:
            break; // just leave it empty

        case DataType.BOOLEAN:
            mOut.write(((Boolean)field).toString().getBytes());
            break;

        case DataType.INTEGER:
            mOut.write(((Integer)field).toString().getBytes());
            break;

        case DataType.LONG:
            mOut.write(((Long)field).toString().getBytes());
            break;

        case DataType.FLOAT:
            mOut.write(((Float)field).toString().getBytes());
            break;

        case DataType.DOUBLE:
            mOut.write(((Double)field).toString().getBytes());
            break;

        case DataType.BYTEARRAY: {
            byte[] b = ((DataByteArray)field).get();
            mOut.write(b, 0, b.length);
            break;
                                 }

        case DataType.CHARARRAY:
            // oddly enough, writeBytes writes a string
            mOut.write(((String)field).getBytes(UTF8));
            break;

        case DataType.MAP:
            boolean mapHasNext = false;
            Map&lt;String, Object&gt; m = (Map&lt;String, Object&gt;)field;
            mOut.write(mapBeginDelim.getBytes(UTF8));
            for(Map.Entry&lt;String, Object&gt; e: m.entrySet()) {
                if(mapHasNext) {
                    mOut.write(fieldDelim.getBytes(UTF8));
                } else {
                    mapHasNext = true;
                }
                putField(e.getKey());
                mOut.write(mapKeyValueDelim.getBytes(UTF8));
                putField(e.getValue());
            }
            mOut.write(mapEndDelim.getBytes(UTF8));
            break;

        case DataType.TUPLE:
            boolean tupleHasNext = false;
            Tuple t = (Tuple)field;
            mOut.write(tupleBeginDelim.getBytes(UTF8));
            for(int i = 0; i &lt; t.size(); ++i) {
                if(tupleHasNext) {
                    mOut.write(fieldDelim.getBytes(UTF8));
                } else {
                    tupleHasNext = true;
                }
                try {
                    putField(t.get(i));
                } catch (ExecException ee) {
                    throw ee;
                }
            }
            mOut.write(tupleEndDelim.getBytes(UTF8));
            break;

        case DataType.BAG:
            boolean bagHasNext = false;
            mOut.write(bagBeginDelim.getBytes(UTF8));
            Iterator&lt;Tuple&gt; tupleIter = ((DataBag)field).iterator();
            while(tupleIter.hasNext()) {
                if(bagHasNext) {
                    mOut.write(fieldDelim.getBytes(UTF8));
                } else {
                    bagHasNext = true;
                }
                putField((Object)tupleIter.next());
            }
            mOut.write(bagEndDelim.getBytes(UTF8));
            break;

        default: {
            int errCode = 2108;
            String msg = "Could not determine data type of field: " + field;
            throw new ExecException(msg, errCode, PigException.BUG);
        }

        }
    }

    @Override
    public OutputFormat getOutputFormat() {
        return new TextOutputFormat&lt;WritableComparable, Text&gt;();
    }

    @Override
    public void prepareToWrite(RecordWriter writer) {
        this.writer = writer;
    }

    @Override
    public void setStoreLocation(String location, Job job) throws IOException {
        job.getConfiguration().set("mapred.textoutputformat.separator", "");
        FileOutputFormat.setOutputPath(job, new Path(location));
        if (location.endsWith(".bz2")) {
            FileOutputFormat.setCompressOutput(job, true);
            FileOutputFormat.setOutputCompressorClass(job,  BZip2Codec.class);
        }  else if (location.endsWith(".gz")) {
            FileOutputFormat.setCompressOutput(job, true);
            FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
        }
    }
}
</source>

</section>
<!-- END STORE FUNCTION -->
</section>
<!-- END LOAD/STORE FUNCTIONS -->




<!-- =============================================================== -->
<section>
<title>Advanced Topics</title>

<section id="udf-interfaces">
<title>UDF Interfaces</title>
<p>Java UDFs can be invoked multiple ways. The simplest UDF can just extend EvalFunc, which requires only the exec function to be implemented (see <a href="#eval-functions-write"> How to Write a Simple Eval Function</a>). Every eval UDF must implement this. Additionally, if a function is algebraic, it can implement <code>Algebraic</code> interface to significantly improve query performance in the cases when combiner can be used (see <a href="#Algebraic-Interface">Algebraic Interface</a>). Finally, a function that can process tuples in an incremental fashion can also implement the Accumulator interface to improve query memory consumption (see <a href="#Accumulator-Interface">Accumulator Interface</a>).
</p>

<p>The optimizer selects the exact method by which a UDF is invoked based on the UDF type and the query. Note that only a single interface is used at any given time. The optimizer tries to find the most efficient way to execute the function. If a combiner is used and the function implements the Algebraic interface then this interface will be used to invoke the function. If the combiner is not invoked but the accumulator can be used and the function implements Accumulator interface then that interface is used. If neither of the conditions is satisfied then the exec function is used to invoke the UDF. 
</p>
 </section>
 
 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="function-instantiation">
<title>Function Instantiation</title>
<p>One problem that users run into is when they make assumption about how many times a constructor for their UDF is called. For instance, they might be creating side files in the store function and doing it in the constructor seems like a good idea. The problem with this approach is that in most cases Pig instantiates functions on the client side to, for instance, examine the schema of the data.  </p>
<p>Users should not make assumptions about how many times a function is instantiated; instead, they should make their code resilient to multiple instantiations. For instance, they could check if the files exist before creating them. </p>
</section>

 
  <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="udf-configurations">
<title>Passing Configurations to UDFs</title>
<p>The singleton UDFContext class provides two features to UDF writers. First, on the backend, it allows UDFs to get access to the JobConf object, by calling getJobConf. This is only available on the backend (at run time) as the JobConf has not yet been constructed on the front end (during planning time).</p>

<p>Second, it allows UDFs to pass configuration information between instantiations of the UDF on the front and backends. UDFs can store information in a configuration object when they are constructed on the front end, or during other front end calls such as describeSchema. They can then read that information on the backend when exec (for EvalFunc) or getNext (for LoadFunc) is called. Note that information will not be passed between instantiations of the function on the backend. The communication channel only works from front end to back end.</p>

<p>To store information, the UDF calls getUDFProperties. This returns a Properties object which the UDF can record the information in or read the information from. To avoid name space conflicts UDFs are required to provide a signature when obtaining a Properties object. This can be done in two ways. The UDF can provide its Class object (via this.getClass()). In this case, every instantiation of the UDF will be given the same Properties object. The UDF can also provide its Class plus an array of Strings. The UDF can pass its constructor arguments, or some other identifying strings. This allows each instantiation of the UDF to have a different properties object thus avoiding name space collisions between instantiations of the UDF.</p>
</section>

 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="udf-monitoring">
<title>Monitoring Long-Running UDFs</title>
<p>Sometimes one may discover that a UDF that executes very quickly in the vast majority of cases turns out to run exceedingly slowly on occasion. This can happen, for example, if a UDF uses complex regular expressions to parse free-form strings, or if a UDF uses some external service to communicate with. As of version 0.8, Pig provides a facility for monitoring the length of time a UDF is executing for every invocation, and terminating its execution if it runs too long. This facility can be turned on using a simple Java annotation:</p>
	
<source>
	import org.apache.pig.builtin.MonitoredUDF;
	
	@MonitoredUDF
	public class MyUDF extends EvalFunc&lt;Integer&gt; {
	  /* implementation goes here */
	}
</source>

<p>Simply annotating your UDF in this way will cause Pig to terminate the UDF's exec() method if it runs for more than 10 seconds, and return the default value of null. The duration of the timeout and the default value can be specified in the annotation, if desired:</p>

<source>
	import org.apache.pig.builtin.MonitoredUDF;
	
	@MonitoredUDF(timeUnit = TimeUnit.MILLISECONDS, duration = 100, intDefault = 10)
	public class MyUDF extends EvalFunc&lt;Integer&gt; {
	  /* implementation goes here */
	}
</source>

<p>intDefault, longDefault, doubleDefault, floatDefault, and stringDefault can be specified in the annotation; the correct default will be chosen based on the return type of the UDF. Custom defaults for tuples and bags are not supported at this time.</p>

<p>If desired, custom logic can also be implemented for error handling by creating a subclass of MonitoredUDFExecutor.ErrorCallback, and overriding its handleError and/or handleTimeout methods. Both of those methods are static, and are passed in the instance of the EvalFunc that produced an exception, as well as an exception, so you may use any state you have in the UDF to process the errors as desired. The default behavior is to increment Hadoop counters every time an error is encountered. Once you have an implementation of the ErrorCallback that performs your custom logic, you can provide it in the annotation:</p>

<source>
	import org.apache.pig.builtin.MonitoredUDF;

	@MonitoredUDF(errorCallback=MySpecialErrorCallback.class)
	public class MyUDF extends EvalFunc&lt;Integer&gt; {
	  /* implementation goes here */
	}
</source>

<p>Currently the MonitoredUDF annotation works with regular and Algebraic UDFs, but has no effect on UDFs that run in the Accumulator mode.</p>

</section>
</section>
</section>


<!-- =============================================================== -->
<section id="python-udfs">
<title>Writing Python UDFs</title>

<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="register-python">
<title>Registering the UDF</title>
<p>You can register a Python script as shown here. This example uses org.apache.pig.scripting.jython.JythonScriptEngine to interpret the Python script. You can develop and use custom script engines to support multiple programming languages and ways to interpret them. Currently, Pig identifies jython as a keyword and ships the required scriptengine (jython) to interpret it. </p>
<source>
Register 'test.py' using jython as myfuncs;
</source>

<p>The following syntax is also supported, where myfuncs is the namespace created for all the functions inside test.py.</p>
<source>
register 'test.py' using org.apache.pig.scripting.jython.JythonScriptEngine as myfuncs;
</source>


<p>A typical test.py looks like this:</p>
<source>
@outputSchema("word:chararray")
def helloworld():  
  return 'Hello, World'

@outputSchema("word:chararray,num:long")
def complex(word):
  return str(word),len(word)

@outputSchemaFunction("squareSchema")
def square(num):
  return ((num)*(num))

@schemaFunction("squareSchema")
def squareSchema(input):
  return input

# No decorator - bytearray
def concat(str):
  return str+str
</source>

<p>The register statement above registers the Python functions defined in test.py in Pig’s runtime within the defined namespace (myfuncs here). They can then be referred later on in the pig script as myfuncs.helloworld(), myfuncs.complex(), and myfuncs.square(). An example usage is:</p>

<source>
b = foreach a generate myfuncs.helloworld(), myfuncs.square(3);
</source>


<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
</section>
<section id="decorators">
<title>Decorators and Schemas</title>
<p>To annotate a Python script so that Pig can identify return types, use Python decorators to define output schema for the script UDF. </p>
<ul>
<li id="outputschema">outputSchema - Defines schema for a script UDF in a format that Pig understands and is able to parse. </li>
<li id="outputfunctionschema">outputFunctionSchema - Defines a script delegate function that defines schema for this function depending upon the input type. This is needed for functions that can accept generic types and perform generic operations on these types. A simple example is square which can accept multiple types. SchemaFunction for this type is a simple identity function (same schema as input). </li>
<li id="schemafunction">schemaFunction - Defines delegate function and is not registered to Pig. </li>
</ul>

<p>When no decorator is specified, Pig assumes the output datatype as bytearray and converts the output generated by script function to bytearray. This is consistent with Pig's behavior in case of Java UDFs. </p>

<p>Sample Schema String - y:{t:(word:chararray,num:long)}, variable names inside a schema string are not used anywhere, they just make the syntax identifiable to the parser. </p>
</section>


<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section>
 <title>Example Scripts</title>
 <p>Simple tasks like string manipulation, mathematical computations, and reorganizing data types can be easily done using Python scripts without having to develop long and complex UDFs in Java. The overall overhead of using scripting language is much less and development cost is almost negligible. The following UDFs, developed in Python, can be used with Pig. </p>
 
 <source>
 mySampleLib.py
 ---------------------
 #/usr/bin/python
 
 ##################
 # Math functions #
 ##################
 #Square - Square of a number of any data type
 @outputSchemaFunction("squareSchema")
 def square(num):
   return ((num)*(num))
 @schemaFunction("squareSchema")
 def squareSchema(input):
   return input
 
 #Percent- Percentage
 @outputSchema("percent:double")
 def percent(num, total):
   return num * 100 / total
 
 ####################
 # String Functions #
 ####################
 #commaFormat- format a number with commas, 12345-> 12,345
 @outputSchema("numformat:chararray")
 def commaFormat(num):
   return '{:,}'.format(num)
 
 #concatMultiple- concat multiple words
 @outputSchema("onestring:chararray")
 def concatMult4(word1, word2, word3, word4):
   return word1 word2 word3 word4
 
 #######################
 # Data Type Functions #
 #######################
 #collectBag- collect elements of a bag into other bag
 #This is useful UDF after group operation
 @outputSchema("y:bag{t:tuple(len:int,word:chararray)}") 
 def collectBag(bag):
   outBag = []
   for word in bag:
     tup=(len(bag), word[1])
     outBag.append(tup)
   return outBag
 
 # Few comments- 
 # Pig mandates that a bag should be a bag of tuples, Python UDFs should follow this pattern.
 # Tuples in Python are immutable, appending to a tuple is not possible.
 </source>
 </section>
 
 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="python-advanced">
 <title>Advanced Topics</title> 
  <section id="import-modules">
  
<!-- ++++++++++++++++++++ -->
<title>Importing Modules</title>
<p>You can import Python modules in your Python script. Pig resolves Python dependencies recursively, which means Pig will automatically ship all dependent Python modules to the backend. Python modules should be found in the jython search path: JYTHON_HOME, JYTHON_PATH, or current directory.</p>
</section>

<!-- ++++++++++++++++++++ -->
<section id="combined-scripts">
<title>Combined Scripts</title>
  <p>UDFs and Pig scripts are generally stored in separate files. For testing purposes you can combine the code in a single file - a "combined" script. Note, however, if you then decide to embed this "combined" script in a host language, the language of the UDF must match the host language.</p>
 
 <p>This example combines Python and Pig. This "combined" script can only be embedded in Python.</p>
 <p>With Python you MUST use the <code>if __name__ == '__main__': </code> construct to separate UDFs and control flow. Otherwise the script will result in an error.</p>
 <source>
 #!/usr/bin/jython
from org.apache.pig.scripting import *

@outputSchema("word:chararray")
def helloworld():  
   return 'Hello, World'
  
if __name__ == '__main__':
       P = Pig.compile("""a = load '1.txt' as (a0, a1);
                          b = foreach a generate helloworld();
                          store b into 'myoutput'; """)

result = P.bind().runSingle();
 </source>
   </section>

  </section>
</section>


 <!-- =============================================================== -->
 <section id="js-udfs">
 <title>Writing JavaScript UDFs</title>
 
 <p><strong>Note:</strong> <em>JavaScript UDFs are an experimental feature.</em></p>

<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="register-js">
 <title>Registering the UDF</title>
 <p>You can register JavaScript as shown here. This example uses org.apache.pig.scripting.js.JsScriptEngine to interpret JavaScript. You can develop and use custom script engines to support multiple programming languages and ways to interpret them. Currently, Pig identifies js as a keyword and ships the required scriptengine (js) to interpret it. </p>
 <source>
 register 'test.js' using javascript as myfuncs;
 </source>
 
 <p>The following syntax is also supported, where myfuncs is the namespace created for all the functions inside test.js.</p>
 <source>
 register 'test.js' using org.apache.pig.scripting.js.JsScriptEngine as myfuncs;
 </source>

 <p>The register statement above registers the js functions defined in test.js in Pig’s runtime within the defined namespace (myfuncs here). They can then be referred later on in the pig script as myfuncs.helloworld(), myfuncs.complex(), and myfuncs.square(). An example usage is:</p>
 
 <source>
 b = foreach a generate myfuncs.helloworld(), myfuncs.complex($0);
 </source>
 </section>
 
 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="schema-return-types">
 <title>Return Types and Schemas</title>
 <p>Since JavaScript functions are first class objects, you can annotate them by adding attributes. Add an outputSchema attribute to your function so that Pig can identify return types for the script UDF. </p>
 <ul>
 <li>outputSchema - Defines schema for a script udf in a format that Pig understands and is able to parse. </li>
 <li>Sample Schema String - y:{t:(word:chararray,num:long)} <br></br>Variable names inside a schema string are used for type conversion between Pig and JavaScript. Tuples are converted to Objects using the names and vice versa</li>
 </ul>
 </section>
 
 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="js-example">
 <title>Example Scripts</title> 
  <p>A simple JavaScript UDF (udf.js) is shown here.</p>
 <source>
 helloworld.outputSchema = "word:chararray";
function helloworld() {
    return 'Hello, World';
}
    
complex.outputSchema = "word:chararray,num:long";
function complex(word){
    return {word:word, num:word.length};
}
</source>
 
<p>This Pig script registers the JavaScript UDF (udf.js).</p>
<source>
 register ‘udf.js’ using javascript as myfuncs; 
A = load ‘data’ as (a0:chararray, a1:int);
B = foreach A generate myfuncs.helloworld(), myfuncs.complex(a0);
... ... 
</source>
  </section>
  
   <!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="js-advanced">
 <title>Advanced Topics</title> 

 <p>UDFs and Pig scripts are generally stored in separate files. For testing purposes you can combine the code in a single file - a "combined" script. 
 Note, however, if you then decide to embed this "combined" script in a host language, the language of the UDF and the host language must match.</p>
 
 <p>This example combines JavaScript and Pig. This "combined" script can only be embedded in JavaScript.</p>
<p>With JavaScript, the control flow MUST be defined in the main function. Otherwise the script will result in an error.</p>
 <source>
importPackage(Packages.org.apache.pig.scripting.js)
pig = org.apache.pig.scripting.js.JSPig;

helloworld.outputSchema = "word:chararray" 
function helloworld() { 
    return 'Hello, World'; 
}

function main() {
  var P = pig.compile(" a = load '1.txt' as (a0, a1);”+
       “b = foreach a generate helloworld();”+
           “store b into 'myoutput';");

  var result = P.bind().runSingle();
}
 </source>
 
 </section>
</section>

 <!-- =============================================================== -->
 <section id="jruby-udfs">
 <title>Writing Ruby UDFs</title>

 <p><strong>Note:</strong> <em>Ruby UDFs are an experimental feature.</em></p>

<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="write-jruby">
 <title>Writing a Ruby UDF</title>
 <p>You must extend PigUdf and define your Ruby UDFs in the class. </p>
 <source>
 require 'pigudf'
 class Myudfs &lt; PigUdf
     def square num
         return nil if num.nil?
         num**2
     end
 end
 </source>
 </section>
 <section id="jruby-schema-return-types">
 <title>Return Types and Schemas</title>
 <p>You have two ways to define the return schema:</p>
 <p>outputSchema - Defines the schema for a UDF in a format that Pig understands. </p>
 <source>
 outputSchema "word:chararray"
 </source>
 <source>
 outputSchema "t:(m:[], t:(name:chararray, age:int, gpa:double), b:{t:(name:chararray, age:int, gpa:double)})"
 </source>
 <p>Schema function</p>
 <source>
 outputSchemaFunction :squareSchema
 def squareSchema input
     input
 end
 </source>
 <p>You need to put outputSchema/outputSchemaFunction statement right before your UDF. The schema function itself can be defined anywhere inside the class.</p>
 </section>
 <section id="register-jruby">
 <title>Registering the UDF</title>
 <p>You can register a Ruby UDF as shown here. </p>
 <source>
 register 'test.rb' using jruby as myfuncs;
 </source>
 <p> This is a shortcut to the complete syntax:</p>
 <source>
 register 'test.rb' using org.apache.pig.scripting.jruby.JrubyScriptEngine as myfuncs;
 </source>
 <p>The <code>register</code> statement above registers the Ruby functions defined in test.rb in Pig’s runtime within the defined namespace (myfuncs in this example). They can then be referred later on in the Pig Latin script as <code>myfuncs.square()</code>. An example usage is:</p>
 <source>
 b = foreach a generate myfuncs.concat($0, $1);
 </source>
 </section>
 <section id="jruby-example">
 <title>Example Scripts</title>
 <p>Here are two complete Ruby UDF samples.</p>
 <source>
 require 'pigudf'
 class Myudfs &lt; PigUdf
 outputSchema "word:chararray"
     def concat *input
         input.inject(:+)
     end
 end
 </source>
 <source>
 require 'pigudf'
 class Myudfs &lt; PigUdf
 outputSchemaFunction :squareSchema
     def square num
         return nil if num.nil?
         num**2
     end
     def squareSchema input
         input
     end
 end
 </source>
 </section>

 <section id="jruby-advanced">
 <title>Advanced Topics</title>
 <p> You can also write Algebraic and Accumulator UDFs using Ruby. You need to extend your class from <code>AlgebraicPigUdf</code> and <code>AccumulatorPigUdf</code> respectively. For an Algebraic UDF, define <code>initial</code>, <code>intermed</code>, and <code>final</code> methods in the class. For an Accumulator UDF, define <code>exec</code> and <code>get</code> methods in the class. Below are example for each type of UDF: </p>
 <source>
 class Count &lt; AlgebraicPigUdf
     output_schema Schema.long
     def initial t
          t.nil? ? 0 : 1
     end
     def intermed t
          return 0 if t.nil?
          t.flatten.inject(:+)
     end
     def final t
         intermed(t)
     end
 end
 </source>
 <source>
 class Sum &lt; AccumulatorPigUdf
     output_schema { |i| i.in.in[0] }
     def exec items
         @sum ||= 0
         @sum += items.flatten.inject(:+)
     end
     def get
         @sum
     end
 end
 </source>
 </section>

</section> 

<!-- ================================================================== -->
<!-- PIGGYBANK -->
<section id="piggybank">
<title>Piggy Bank</title>
<p>Piggy Bank is a place for Pig users to share the Java UDFs they have written for use with Pig. 
The functions are contributed "as-is." 
If you find a bug in a function, take the time to fix it and contribute the fix to Piggy Bank. 
If you don't find the UDF you need, take the time to write and contribute the function to Piggy Bank.
</p>

<p><strong>Note:</strong> Piggy Bank currently supports Java UDFs. Support for Python and JavaScript UDFs will be added at a later date.</p>

 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="piggbank-access">
<title>Accessing Functions</title>

<p>The Piggy Bank functions are currently distributed in source form. Users are required to checkout the code and build the package themselves. No binary distributions or nightly builds are available at this time. </p>

<p>To build a jar file that contains all available UDFs, follow these steps: </p>
<ul>
<li>Checkout UDF code: <code>svn co http://svn.apache.org/repos/asf/pig/trunk/contrib/piggybank</code> </li>
<li>Add pig.jar to your ClassPath: <code>export CLASSPATH=$CLASSPATH:/path/to/pig.jar</code> </li>
<li>Build the jar file: from directory<code>trunk/contrib/piggybank/java</code> run <code>ant</code>. 
This will generate <code>piggybank.jar</code> in the same directory. </li>
</ul>
<p></p>

<p>To obtain <code>javadoc</code> description of the functions run <code>ant javadoc</code> from directory <code>trunk/contrib/piggybank/java</code>. The documentation is generate in directory <code>trunk/contrib/piggybank/java/build/javadoc</code>.</p>

<p>To use a function, you need to determine which package it belongs to. The top level packages correspond to the function type and currently are: </p>
<ul>
<li>org.apache.pig.piggybank.comparison - for custom comparator used by ORDER operator </li>
<li>org.apache.pig.piggybank.evaluation - for eval functions like aggregates and column transformations </li>
<li>org.apache.pig.piggybank.filtering - for functions used in FILTER operator </li>
<li>org.apache.pig.piggybank.grouping - for grouping functions</li>
<li>org.apache.pig.piggybank.storage - for load/store functions </li>
</ul>
<p></p>

<p>(The exact package of the function can be seen in the javadocs or by navigating the source tree.) </p>

<p>For example, to use the UPPER command: </p>

<source>
REGISTER /public/share/pig/contrib/piggybank/java/piggybank.jar ;
TweetsInaug = FILTER Tweets BY org.apache.pig.piggybank.evaluation.string.UPPER(text) 
    MATCHES '.*(INAUG|OBAMA|BIDEN|CHENEY|BUSH).*' ;
STORE TweetsInaug INTO 'meta/inaug/tweets_inaug' ;
</source>
</section>

 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="piggybank-contribute">
<title>Contributing Functions</title>

<p>To contribute a Java function that you have written, do the following:</p>
<ol>
<li>Check the existing javadoc to make sure that the function does not already exist as described in <a href="#piggbank-access">Accessing Functions</a>. </li>
<li>Checkout the UDF code as described in <a href="#piggbank-access">Accessing Functions</a>. </li>
<li>Place your java code in the directory that makes sense for your function. The directory structure currently has two levels: (1) function type, as described in <a href="#piggbank-access">Accessing Functions</a>, and (2) function subtype, for some of the types (like math or string for eval functions). If you think your function requires a new subtype, feel free to add one. </li>
<li>Make sure that your function is well documented and uses the 
<a href="http://download.oracle.com/javase/1.4.2/docs/tooldocs/solaris/javadoc.html">javadoc</a> style of documentation. </li>
<li>Make sure that your code follows Pig coding conventions described in <a href="http://wiki.apache.org/pig/HowToContribute">How to Contribute to Pig</a>.</li>
<li>Make sure that for each function, you add a corresponding test class in the test part of the tree. </li>
<li>Submit your patch following the process described in <a href="http://wiki.apache.org/pig/HowToContribute">How to Contribute to Pig</a>. </li>
</ol>
</section>
</section> 


</body>
</document>

