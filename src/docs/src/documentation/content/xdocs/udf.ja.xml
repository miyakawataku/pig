<?xml version="1.0" encoding="UTF-8"?>
<!--
  Copyright 2002-2004 The Apache Software Foundation

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->

<!DOCTYPE document PUBLIC "-//APACHE//DTD Documentation V2.0//EN"
          "http://forrest.apache.org/dtd/document-v20.dtd">


<document>
<header>
<title>ユーザ定義関数</title>
</header>
<body>

<section id="udfs">
<title>導入</title>
<p>Pig は独自の処理を実現するためのユーザ定義関数 (UDF) を広くサポートしています。 Pig の UDF は現在 4 つの言語で実装できます: Java, Python, JavaScript, Ruby です。</p>

<p>もっとも広くサポートされているのは Java 定義の関数です。 Java 定義の関数を使えば、ロード・ストア、値の変換、集計を含む処理のすべての処理がカスタマイズできます。 Java 定義の関数は、 Pig 自体と同じ言語による実装であるため、また <a href="#algebraic-interface">Algebraic</a> や <a href="#accumulator-interface">Accumulator</a> のようなインタフェースをサポートしているため、他の言語よりも効率的です。</p>

<p>Python, JavaScript, Ruby 定義の関数のサポートはより狭いものです。これの関数は、目下改善中の新しい拡張です。今のところサポートされているのは、基本的なインタフェースだけです。ロード・ストア関数はサポートされません。また、 JavaScript と Ruby の関数は、 Java や Python ほどの量のテストを経ていないため、実験的な機能として提供されます。実行時に Pig がスクリプト言語による UDF をみつけると、 Jython, Rhino, JRuby などの JAR ファイルを計算ノードへと自動的に送り込みます。</p>

<p>Pig は Piggy Bank という Java UDF のリポジトリもサポートしています。 Piggy Bank を使うことで、他のユーザが作った Java UDF が利用でき、また自分の作った Java UDF を提供することもできます。</p>

</section>


<!-- ================================================================== -->
<!-- WRITING UDFS -->

<section id="udf-java">
<title>Java UDF を書く</title>

<!-- =============================================================== -->
<section id="eval-functions">
<title>評価関数</title>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="eval-functions-use">
<title>単純な評価関数を使う</title>
<p>評価関数はもっとも一般的な関数の種類です。次のように、 FOREACH 文の中で使えます。</p>

<source>
-- myscript.pig
REGISTER myudfs.jar;
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = FOREACH A GENERATE myudfs.UPPER(name);
DUMP B;
</source>

<p>このスクリプトは次のコマンドで実行できます。簡便のために、このドキュメント中のプログラム例はすべてローカルモードで動かしていますが、 Hadoop モードで動かすことも可能です。 Pig の実行方法についての詳細は、 <a href="start.html#tutorial">Pig チュートリアル</a>を見てください。</p>

<source>
java -cp pig.jar org.apache.pig.Main -x local myscript.pig
</source>

<p>スクリプトの最初の行は、 UDF を含む JAR ファイルの場所を指定しています (JAR ファイル名は引用符でくくりません。引用符をつけると文法エラーになります) 。 JAR ファイルは最初にクラスパス上で探索されます。クラスパス上になかった場合、絶対パスか、 Pig を起動した場所からの相対パスとして JAR ファイルを探索します。 JAR ファイルが見つからなかった場合、エラーメッセージが表示されます: <code>java.io.IOException:&nbsp;Can't&nbsp;read&nbsp;jar&nbsp;file:&nbsp;myudfs.jar</code></p>

<p><code>REGISTER</code> コマンドは 1 つのスクリプトで複数回使えます。関数の完全修飾名が複数の JAR ファイル中で見つかる場合、 Java の意味論に整合して、最初に見つかったファイルが使われます。</p>

<p>UDF の名前はパッケージ名で完全修飾する必要があります。正しく完全修飾名を指定しない場合、エラーになります: <code>java.io.IOException:&nbsp;Cannot&nbsp;instantiate:UPPER</code> 。また、関数名は大文字・小文字を区別します (UPPER と upper は別の名前として扱われます) 。 UDF は 1 つ以上の引数を取ることができます。関数のドキュメンテーションは正確なシグネチャを記載するべきです。</p>

<p>この例に示した関数は 1 つの ASCII 文字列を引数に取り、大文字にして戻します。 SQL の関数による列変換に親しんだ方なら、 UPPER 関数が同種のものだと分かることでしょう。ただし、後に見るように、 Pig の評価関数は単なる列変換関数にとどまらず、集計関数やフィルタ関数を包含しています。</p>

<p>UDF を使うだけであれば、以上が UDF について知るべきことのほとんどすべてです。</p>

</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="eval-functions-write">
<title>評価関数の書き方</title>
<p><code>UPPER</code> 関数の実装を見てみましょう。</p>

<source>
1  package myudfs;
2  import java.io.IOException;
3  import org.apache.pig.EvalFunc;
4  import org.apache.pig.data.Tuple;
5
6  public class UPPER extends EvalFunc&lt;String&gt;
7  {
8    public String exec(Tuple input) throws IOException {
9        if (input == null || input.size() == 0)
10            return null;
11        try{
12            String str = (String)input.get(0);
13           return str.toUpperCase();
14        }catch(Exception e){
15            throw new IOException("Caught exception processing input row ", e);
16        }
17    }
18  }
</source>

<p>1 行目は関数が <code>myudfs</code> パッケージに属することを意味しています。この UDF クラスはすべての評価関数の基底クラスである <code>EvalFunc</code> クラスを継承し、 UDF の戻り値の型である Java の <code>String</code> を型パラメータとしています。 <code>EvalFunc</code> については後に詳しく見ますが、今必要なことは <code>exec</code> メソッドを実装することです。このメソッドは入力となるタプルごとに呼び出されます。メソッドの引数は Pig スクリプト中で呼び出した関数の引数を順番通りに収めたタプルです。この例では、学生の名前に対応する文字列フィールドが 1 つ収められています。</p>
<p>最初にするべきことは、不正なデータの扱いを決めることです。これはデータの型式に依存します。たとえばデータが <code>bytearray</code> 型だった場合、まだ適切な型に変換されていないことが分かります。この例では、データの型式が期待したものと異なった場合、 NULL を戻す必要があります。一方で、もしも入力データが別の型だった場合には、型変換はすでに行われて、データは適切な型になっているものと判断できます。 15 行目でエラーを投げているのがその場合です。</p>
<p>9 から 10 行目では入力データが null か空でないかをチェックして、そうだった場合には null を戻しています。</p>
<p>12 から 13 行目では見てのとおり関数本来の仕事をしています。</p>
<p>関数が実装できたので、コンパルして JAR ファイルに固める必要があります。 UDF をコンパイルするためにはあらかじめ <code>pig.jar</code> をビルドする必要があります。 SVN リポジトリから Pig のコードをチェックアウトして pig.jar を作るには、次のコマンドを実行してください:</p>

<source>
svn co http://svn.apache.org/repos/asf/pig/trunk
cd trunk
ant
</source>

<p>上記コマンドの結果として、作業ディレクトリに pig.jar ができているはずです。次のコマンドで関数をコンパイルして、 JAR に固めます。</p>

<source>
cd myudfs
javac -cp pig.jar UPPER.java
cd ..
jar -cf myudfs.jar myudfs
</source>

<p>上記コマンドの結果として、作業ディレクトリ内に <code>myudfs.jar</code> ができているはずです。この JAR ファイルは前章の説明のとおり Pig スクリプトから使用できます。</p>

</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="aggregate-functions">
<title>集約関数</title>
<p>集約関数も一般的な評価関数の種別です。次のスクリプトのように、集約関数は通常グループ化されたデータに対して用います。</p>

<source>
-- myscript2.pig
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = GROUP A BY name;
C = FOREACH B GENERATE group, COUNT(A);
DUMP C;
</source>

<p>上のスクリプトは <code>COUNT</code> 関数を使って同じ名前の学生の数を数えています。このスクリプトについては注意すべき点が二つあります。まずは <code>REGISTER</code> なしで関数を使っていること、二つ目は関数が完全修飾されていないことです。これは <code>COUNT</code> が組み込み関数であって Pig に同梱されているためです。組み込み関数と UDF との違いはこの二点だけです。組み込み関数については、このドキュメント内で詳述します。</p>
</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="algebraic-interface">
<title>Algebraic インタフェース</title>

<p>集約関数はバッグを引数に取りスカラ値を戻す評価関数です。多くの集約関数は、分散環境で差分更新的に計算できるという、興味深く有用な特質を持っています。 Pig ではこのような関数を「代数的 (algebraic)」と呼びます。 <code>COUNT</code> は代数的関数の一例です。なぜなら、データの部分ごとに要素数を数え、それを合計することで、全体の要素数が得られるからです。 Hadoop の世界ではこのような処理は、 Mapper と Combiner で部分ごとの計算を行い、 Reducer で最後にまとめあげます。</p>

<p>代数的な集約関数をこのように実装することは、性能上とても重要です。 COUNT 関数の実装を例にとってどういうことか見てみましょう (エラー処理その他いくらかのコードは読み易さのために省略しています。完全なコードは <a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/builtin/COUNT.java?view=markup">SVN リポジトリ</a> で見てください) 。</p>

<source>
public class COUNT extends EvalFunc&lt;Long&gt; implements Algebraic{
    public Long exec(Tuple input) throws IOException {return count(input);}
    public String getInitial() {return Initial.class.getName();}
    public String getIntermed() {return Intermed.class.getName();}
    public String getFinal() {return Final.class.getName();}
    static public class Initial extends EvalFunc&lt;Tuple&gt; {
        public Tuple exec(Tuple input) throws IOException {return TupleFactory.getInstance().newTuple(count(input));}
    }
    static public class Intermed extends EvalFunc&lt;Tuple&gt; {
        public Tuple exec(Tuple input) throws IOException {return TupleFactory.getInstance().newTuple(sum(input));}
    }
    static public class Final extends EvalFunc&lt;Long&gt; {
        public Tuple exec(Tuple input) throws IOException {return sum(input);}
    }
    static protected Long count(Tuple input) throws ExecException {
        Object values = input.get(0);
        if (values instanceof DataBag) return ((DataBag)values).size();
        else if (values instanceof Map) return new Long(((Map)values).size());
    }
    static protected Long sum(Tuple input) throws ExecException, NumberFormatException {
        DataBag values = (DataBag)input.get(0);
        long sum = 0;
        for (Iterator (Tuple) it = values.iterator(); it.hasNext();) {
            Tuple t = it.next();
            sum += (Long)t.get(0);
        }
        return sum;
    }
}
</source>

<p><code>COUNT</code> 関数は <code>Algebraic</code> インタフェースを実装しています。 <code>Algebraic</code> インタフェースの内容は次のとおりです: </p>

<source>
public interface Algebraic{
    public String getInitial();
    public String getIntermed();
    public String getFinal();
}
</source>

<p>代数的関数を書くするためには、 <code>Algebraic</code> を実装して、またネストしたクラスとして <code>EvalFunc</code> を継承したクラスを 3 つ定義する必要があります。 <code>Initial</code> クラスの <code>exec</code> メソッドは関数の入力を引数として最初に呼ばれ、中間的な結果を戻します。 <code>Intermed</code> クラスの <code>exec</code> メソッドは、 <code>Initial</code> クラスか <code>Intermed</code> クラスの戻り値を引数として、 0 回以上呼ばれ、中間的な結果を戻します。最後に、 <code>Final</code> クラスの <code>exec</code> メソッドが関数の戻り値となるスカラ値を生成します。</p>

<p>Hadoop の世界では次のようになります。 <code>Initial</code> クラスの <code>exec</code> メソッドは Mapper ごとに呼ばれ、中間的な結果を戻します。 <code>Intermed</code> クラスの <code>exec</code> メソッドは、 Combiner (実行されることもされないこともあります) ごとに呼ばれ、中間的な結果を戻します。 <code>Final</code> クラスの <code>exec</code> メソッドは Reducer ごとに呼ばれ、最終的な結果を生成します。</p>
<p><code>COUNT</code> 関数が、どのように実装されているか見てみましょう。 <code>Initial</code> クラスと <code>Intermed</code> クラスの <code>exec</code> メソッドは <code>Tuple</code> 型でパラメータ化されており、 <code>Final</code> クラスの <code>exec</code> メソッドは <code>COUNT</code> 関数の戻り値の型である <code>Long</code> 型でパラメータ化されています。また、それぞれのクラスの完全修飾名は <code>getInitial</code>, <code>getIntermed</code>, <code>getFinal</code> の各メソッドで戻す必要があります。</p>
</section>

<!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="accumulator-interface">
<title>Accumulator インタフェース</title>

<p>Pig では、 GROUP 演算子や COGROUP 演算子の結果のデータをバッグに詰めて UDF に渡す時に、メモリ消費の問題が起こり得ます。</p>

<p>この問題は、 UDF を代数的関数にすることによってある程度は対処できます。代数的関数は Combiner を使うので、データを複数の実行段階 (Mapper, Combiner, Reducer) で少しずつ処理できるからです。しかしながら、代数的でなく、したがって Combiner を使わないながら、全部のデータを一度に処理する必要がない関数も存在します。</p>

<p>Accumulator インタフェースはこのような UDF でメモリ消費を抑えるために設計されています。このインタフェースを実装した関数に対して Pig は、同じキーに紐づくデータを連続して少しずつ渡します。このようにデータを少しずつ処理する UDF は、次のインタフェースを実装します:</p>
<source>
public interface Accumulator &lt;T&gt; {
   /**
    * タプルを処理します。バッグは現在のキーに紐づく 0 以上の個数のタプルを保持しています。
    */
    public void accumulate(Tuple b) throws IOException;
    /**
     * 現在のキーに紐づくタプルがすべて渡された後に呼ばれます。
     * @return 現在のキーに対する UDF の戻り値を戻します。
     */
    public T getValue();
    /**
     * getValue() の後に、次のキーに備えるために呼ばれます。
     */
    public void cleanup();
}
</source>

<p>いくつか注意すべき点があります:</p>

<ol>
	<li>UDF は EvalFunc を継承し、必要なメソッドをすべて実装する必要があります。</li>
	<li>関数が代数的だったとしても、 FOREACH 文の中で Accumulator 関数と一緒に使うのであれば、 Algebraic インタフェースに加えて Accumulator インタフェースを実装する必要があります。</li>
	<li>インタフェースは関数の戻り値の型でパラメータ化されます。</li>
	<li>accumulate メソッドは、 1 個以上のタプルを詰めたバッグを引数として、 1 回以上呼ばれます。 accumulate メソッドの引数の中身は exec メソッドと同じく、 UDF の全引数です。ただし、その内の 1 つはバッグになります。</li>
	<li>getValue メソッドは、キーに紐づくタプルがすべて処理された後に、結果を得るために呼ばれます。</li>
	<li>cleanup メソッドは、 getValue が呼ばれた後に、次のキーの準備をするために呼ばれます。</li>
</ol>


<p>整数版の MAX 関数を Accumulator インタフェースで実装すると次のようになります:</p>
<source>
public class IntMax extends EvalFunc&lt;Integer&gt; implements Algebraic, Accumulator&lt;Integer&gt; {
    …….
    /* Accumulator interface */
    
    private Integer intermediateMax = null;
    
    @Override
    public void accumulate(Tuple b) throws IOException {
        try {
            Integer curMax = max(b);
            if (curMax == null) {
                return;
            }
            /* if bag is not null, initialize intermediateMax to negative infinity */
            if (intermediateMax == null) {
                intermediateMax = Integer.MIN_VALUE;
            }
            intermediateMax = java.lang.Math.max(intermediateMax, curMax);
        } catch (ExecException ee) {
            throw ee;
        } catch (Exception e) {
            int errCode = 2106;
            String msg = "Error while computing max in " + this.getClass().getSimpleName();
            throw new ExecException(msg, errCode, PigException.BUG, e);           
        }
    }

    @Override
    public void cleanup() {
        intermediateMax = null;
    }

    @Override
    public Integer getValue() {
        return intermediateMax;
    }
}
</source>
</section>


<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="filter-functions">
<title>フィルタ関数</title>
<p>フィルタ関数はブーリアン値を戻す評価関数です。フィルタ関数はブーリアン式が置ける場所ならどこでも使えます。たとえば、 <code>FILTER</code> 演算子や条件演算子式で使えます。</p>

<p>次の例では <code>IsEmpy</code> 組み込み関数を結合に使っています。</p>

<source>
-- 内部結合
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = LOAD 'voter_data' AS (name: chararray, age: int, registration: chararay, contributions: float);
C = COGROUP A BY name, B BY name;
D = FILTER C BY not IsEmpty(A);
E = FILTER D BY not IsEmpty(B);
F = FOREACH E GENERATE flatten(A), flatten(B);
DUMP F;
</source>

<p>実は、フィルタを行わなくても同じ結果が得られます。 <code>FOREACH</code> と <code>FLATTEN</code> の組み合わせによってクロス積が生成され、クロス積は空のバッグを消し去るからです。しかしながら、あらかじめ空のバッグを排除することによって、クロス積の入力が減らせるため、より効率的になります。</p>

<source>
-- 全外部結合
A = LOAD 'student_data' AS (name: chararray, age: int, gpa: float);
B = LOAD 'voter_data' AS (name: chararray, age: int, registration: chararay, contributions: float);
C = COGROUP A BY name, B BY name;
D = FOREACH C GENERATE group, flatten((IsEmpty(A) ? null : A)), flatten((IsEmpty(B) ? null : B));
dump D;
</source>

<p><code>IsEmpty</code> 関数の実装は次のようになります:</p>

<source>
import java.io.IOException;
import java.util.Map;

import org.apache.pig.FilterFunc;
import org.apache.pig.PigException;
import org.apache.pig.backend.executionengine.ExecException;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.DataType;

/**
 * バッグかマップが空かどうかを確かめます。
 */
public class IsEmpty extends FilterFunc {

    @Override
    public Boolean exec(Tuple input) throws IOException {
        try {
            Object values = input.get(0);
            if (values instanceof DataBag)
                return ((DataBag)values).size() == 0;
            else if (values instanceof Map)
                return ((Map)values).size() == 0;
            else {
                int errCode = 2102;
                String msg = "Cannot test a " +
                DataType.findTypeName(values) + " for emptiness.";
                throw new ExecException(msg, errCode, PigException.BUG);
            }
        } catch (ExecException ee) {
            throw ee;
        }
    }
} 
</source>
</section>

<section id="udf_simulation">
<title>シミュレーションによる UDF 実装</title>
<p>複雑な種類の EvalFunc を実装すると、単純な種類の実装は自動的に提供されます。つまり、 <a href="#algebraic-interface">Algebraic</a> インタフェースを実装すれば、 Accumulator インタフェースの実装と単純な EvalFunc の exec メソッドはただで得られます。同じように、 <a href="#accumulator-interface">Accumulator</a> インタフェースを実装すると、単純な EvalFunc の exec メソッドはただで得られますが、 Algebraic の実装は得られません。これら自動的に生成される実装はシミュレーションに基づくので、最も効率的という訳ではありません。もし Accumulator インタフェースや EvalFunc の exec メソッドを効率的にしたいのであれば、自分でそれらのメソッドを実装してください。実装したメソッドは、自動的に提供される実装に優先して使用されます。</p>
</section>
<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="pig-types">
<title>Pig の型と Java の型</title>
<p>Pig の型システムについて重要なことは、次の表に示すとおり、 Pig が Java のネイティブの型をほぼそのまま使うという事です。</p>


<table>
<tr>
<th>
Pig の型
</th>
<th>
Java のクラス
</th>
</tr>
<tr>
<td>
<p> bytearray </p>
</td>
<td>
<p> DataByteArray </p>
</td>
</tr>
<tr>
<td>
<p> chararray </p>
</td>
<td>
<p> String </p>
</td>
</tr>
<tr>
<td>
<p> int </p>
</td>
<td>
<p> Integer </p>
</td>
</tr>
<tr>
<td>
<p> long </p>
</td>
<td>
<p> Long </p>
</td>
</tr>
<tr>
<td>
<p> float </p>
</td>
<td>
<p> Float </p>
</td>
</tr>
<tr>
<td>
<p> double </p>
</td>
<td>
<p> Double </p>
</td>
</tr>
<tr>
<td>
<p> tuple </p>
</td>
<td>
<p> Tuple </p>
</td>
</tr>
<tr>
<td>
<p> bag </p>
</td>
<td>
<p> DataBag </p>
</td>
</tr>
<tr>
<td>
<p> map </p>
</td>
<td>
<p> Map&lt;Object, Object&gt; </p>
</td>
</tr>
</table>

<p>Pig 独自の型は <a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/data/">SVN リポジトリ</a> で見られます。</p>
<p><code>Tuple</code> と <code>DataBag</code> は実クラスでなくインタフェースである点で特殊です。これにより、ユーザは独自のタプル型やバッグ型を作ることで Pig が拡張できます。このため、 UDF はバッグやタプルを直接インスタンス化できません。インスタンスを作るためには、 <code>TupleFactory</code> や <code>BagFactory</code> といったファクトリクラスを使う必要があります。</p>
<p><code>TOKENIZE</code> 組み込み関数を見るとバッグとタプルがどのように作られるか分かります。この関数は文字列を引数に取って、文字列中の単語のバッグを戻します (今のところ Pig のバッグが保持できるのはタプルだけであることに注意してください) 。</p>

<source>
package org.apache.pig.builtin;

import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;

public class TOKENIZE extends EvalFunc&lt;DataBag&gt; {
    TupleFactory mTupleFactory = TupleFactory.getInstance();
    BagFactory mBagFactory = BagFactory.getInstance();

    public DataBag exec(Tuple input) throws IOException 
        try {
            DataBag output = mBagFactory.newDefaultBag();
            Object o = input.get(0);
            if ((o instanceof String)) {
                throw new IOException("Expected input to be chararray, but  got " + o.getClass().getName());
            }
            StringTokenizer tok = new StringTokenizer((String)o, " \",()*", false);
            while (tok.hasMoreTokens()) output.add(mTupleFactory.newTuple(tok.nextToken()));
            return output;
        } catch (ExecException ee) {
            // エラー処理
        }
    }
}
</source>
</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="schemas">
<title>スキーマと Java UDF</title>

<p>Pig はバリデーションと効率化のために型情報を使います。そのため、 UDF を型伝播に参加させることは重要です。普通 UDF を実装する際に出力スキーマを Pig に伝えるために何かをすることはありません。これは、 Pig が Javaの<a href="http://www.oracle.com/technetwork/articles/java/javareflection-1536171.html">リフレクション</a>を使って型情報を見つけ出すためです。 UDF の戻り値がスカラ値かマップの時には、特に何をする必要もありません。しかしながら、 UDF がタプルか、タプルのバッグを戻す時には、 Pig がタプルの構造を把握できるように助けてあげる必要があります。</p>
<p>UDF がタプルかバッグを戻し、スキーマ情報が与えられていない場合、 Pig はタプルが bytearray 型のフィールドを 1 つ持っているものと仮定します。そうでない場合、スキーマを指定しないことが失敗の原因になります。これについては次で見てみましょう。</p>
<p>ここで、 <code>Swap</code> UDF が 2 つの引数を取って順番をひっくり返すものとします。 UDF がスキーマを指定しない場合に、次のスクリプトがどうなるか見てみましょう。</p>

<source>
register myudfs.jar;
A = load 'student_data' as (name: chararray, age: int, gpa: float);
B = foreach A generate flatten(myudfs.Swap(name, age)), gpa;
C = foreach B generate $2;
D = limit B 20;
dump D;
</source>

<p>このスクリプトは 4 行目の <code>C = foreach B generate $2;</code> で次のエラーを発生します。</p>

<source>
java.io.IOException: Out of bound access. Trying to access non-existent column: 2. Schema {bytearray,gpa: float} has 2 column(s).
</source>

<p>これは、 Pig が B には 2 つの列しかないと認識しているのに、 4 行目で 3 つ目の列を指定しているからです (Pig では列のインデックスは 0 から始まります) 。</p>
<p>スキーマを含んだ関数は次のように書きます。</p>

<source>
package myudfs;
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

public class Swap extends EvalFunc&lt;Tuple&gt; {
    public Tuple exec(Tuple input) throws IOException {
        if (input == null || input.size()   2
            return null;
        try{
            Tuple output = TupleFactory.getInstance().newTuple(2);
            output.set(0, input.get(1));
            output.set(1, input.get(0));
            return output;
        } catch(Exception e){
            System.err.println("Failed to process input; error - " + e.getMessage());
            return null;
        }
    }
    public Schema outputSchema(Schema input) {
        try{
            Schema tupleSchema = new Schema();
            tupleSchema.add(input.getField(1));
            tupleSchema.add(input.getField(0));
            return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input),tupleSchema, DataType.TUPLE));
        }catch (Exception e){
                return null;
        }
    }
}
</source>

<p>この関数はタプル型のフィールドのスキーマ (<code>FieldSchema</code> 型) を生成しています。フィールド名は <code>EvalFunc</code> の <code>getSchemaName</code> メソッドを使って構築しています。結果のフィールド名は、最初の引数に渡した UDF 名を含んでおり、一連の番号によって一意性が保証されます。さきほどのスクリプトで <code>dump D;</code> の代わりに <code>describe B;</code> を実行すると、次の出力が得られます:</p>

<source>
B: {myudfs.swap_age_3::age: int,myudfs.swap_age_3::name: chararray,gpa: float}
</source>

<p><code>FieldSchema</code> のコンストラクタの 2 番目の引数はフィールドのスキーマで、今回の場合は 2 つのフィールドを含むタプルになります。 3 番目の引数はスキーマの型を表し、今回の場合は <code>TUPLE</code> になります。サポートされているスキーマの型は <code>org.apache.pig.data.DataType</code> クラスで定義されています。</p>

<source>
public class DataType {
    public static final byte UNKNOWN   =   0;
    public static final byte NULL      =   1;
    public static final byte BOOLEAN   =   5; // Pig の内部使用のみ
    public static final byte BYTE      =   6; // Pig の内部使用のみ
    public static final byte INTEGER   =  10;
    public static final byte LONG      =  15;
    public static final byte FLOAT     =  20;
    public static final byte DOUBLE    =  25;
    public static final byte BYTEARRAY =  50;
    public static final byte CHARARRAY =  55;
    public static final byte MAP       = 100;
    public static final byte TUPLE     = 110;
    public static final byte BAG       = 120;
    public static final byte ERROR     =  -1;
    // more code here
}
</source>

<p>スキーマを定義するためには <code>org.apache.pig.data.DataType</code> クラスをインポートする必要があります。また、スキーマのクラスである <code>org.apache.pig.impl.logicalLayer.schema.Schema</code> クラスもインポートする必要があります。</p>
<p>上の例では、タプル型の出力スキーマをどのように作るかを見ました。バッグでもほとんど同じです。 <code>TOKENIZE</code> 関数の例を見てみましょう。</p>

<p>見てお分かりのように、 <code>Swap</code> 関数のスキーマ定義と極めて似通っています。相違点の一つは、バッグ中の単語のスキーマを表すために、入力スキーマの情報を再利用するのではなく、フィールドのスキーマを新しく作っていることです。もうひとつの違いは、スキーマの型が TUPLE でなく BAG であることです。</p>

<source>
package org.apache.pig.builtin;

import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.BagFactory;
import org.apache.pig.data.DataBag;
import org.apache.pig.data.Tuple;
import org.apache.pig.data.TupleFactory;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

public class TOKENIZE extends EvalFunc&lt;DataBag&gt; {
    TupleFactory mTupleFactory = TupleFactory.getInstance();
    BagFactory mBagFactory = BagFactory.getInstance();
    public DataBag exec(Tuple input) throws IOException {
        try {
            DataBag output = mBagFactory.newDefaultBag();
            Object o = input.get(0);
            if ((o instanceof String)) {
                throw new IOException("Expected input to be chararray, but  got " + o.getClass().getName());
            }
            StringTokenizer tok = new StringTokenizer((String)o, " \",()*", false);
            while (tok.hasMoreTokens()) output.add(mTupleFactory.newTuple(tok.nextToken()));
            return output;
        } catch (ExecException ee) {
            // エラー処理
        }
    }
    public Schema outputSchema(Schema input) {
         try{
             Schema bagSchema = new Schema();
             bagSchema.add(new Schema.FieldSchema("token", DataType.CHARARRAY));

             return new Schema(new Schema.FieldSchema(getSchemaName(this.getClass().getName().toLowerCase(), input),
                                                    bagSchema, DataType.BAG));
         }catch (Exception e){
            return null;
         }
    }
}
</source>

<p>スキーマと UDF についてもうひとつ特筆すべきこととして、 UDF で実際にデータを処理する前に、入力データのスキーマが分かるようにしてほしいという要望が上がっています。たとえば、入力タプルを、フィールド名がキーとなるマップに変換したいとしても、現在のところ方法がありません。これはいずれサポートしたい機能です。</p>

</section>


<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="error-handling">
<title>エラー処理</title>

<p>UDF で起きるエラーにはいくつかの種類があります:</p>
<ol>
<li><p>特定の行に影響を与えるが、他の行には影響がないエラー。このようなエラーの例は、不正な入力値やゼロ除算です。このような状況での適切な処理方法は、警告メッセージを出力して、 null を戻すことです。次節で見る <code>ABS</code> 関数は、この手法を採用しています。今のところ、警告メッセージは標準エラーに出力することになっていますが、いずれ UDF にロガーを渡すようにするつもりです。 null を戻すことは、不正な値が bytearray 型である場合にしか意味がないということに注意してください。それ以外の型の場合は、特定の型が既に割り当てられており、引数は適切な値になっているはずです。そうでない場合は内部エラーであり、全体の処理を失敗させる必要があります。両方の場合について、次節の <code>ABS</code> 関数の実装の中で見ていきます。</p></li>
<li><p>処理全体に影響するが、再試行で解決する可能性があるエラー。このようなエラーの例は、参照ファイルが存在せず、開けない場合です。これは環境に依存する一時的な問題である可能性があるため、再試行すればうまく行くかも知れません。この場合、次節の <code>ABS</code> 関数のように、 <code>IOException</code> を投げて Pig にエラーを通知します。</p></li>
<li><p>処理全体に影響し、再試行でも解決できないエラー。このようなエラーの例は、パーミッションが適切でないため、参照ファイルが開けない場合です。今のところ Pig には、このようなエラーに適切に対処する方法がありません。 Hadoop についても同様です。したがってこのようなエラーは上記の 2 と同じやり方で処理します。</p></li>
</ol>

</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="function-overloading">
<title>関数オーバーロード</title>

<p>Pig に型システムが導入される以前、数値計算に使われる値はすべて double 型とみなされていました。しかしながら、実際の型が int や long である場合、これは効率的なやり方ではありません (int が使えるところで double を使うと、約 2 倍の処理時間が掛かった例があります) 。今や Pig は型システムを備えているので、型情報を用い、引数に対して最も効率的な関数を選ぶことができます。</p>
<p>UDF の作者は、性能の向上が見込まれる場合、型を指定したバージョンの実装を提供することが推奨されます。一方で、関数を使う側は、型ごとの実装を気にしなくても正しく動作するようになっています。次の例で見るように、これは関数テーブルの仕組みによって実現しています。</p>
<p>次の例で見る <code>ABS</code> 関数は、引数の数値の絶対値を戻します。</p>

<source>
import java.io.IOException;
import java.util.List;
import java.util.ArrayList;
import org.apache.pig.EvalFunc;
import org.apache.pig.FuncSpec;
import org.apache.pig.data.Tuple;
import org.apache.pig.impl.logicalLayer.FrontendException;
import org.apache.pig.impl.logicalLayer.schema.Schema;
import org.apache.pig.data.DataType;

public class ABS extends EvalFunc&lt;Double&gt; {
    public Double exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        Double d;
        try{
            d = DataType.toDouble(input.get(0));
        } catch (NumberFormatException nfe){
            System.err.println("Failed to process input; error - " + nfe.getMessage());
            return null;
        } catch (Exception e){
            throw new IOException("Caught exception processing input row ", e);
        }
        return Math.abs(d);
    }
    public List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {
        List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt;();
        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.BYTEARRAY))));
        funcList.add(new FuncSpec(DoubleAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.DOUBLE))));
        funcList.add(new FuncSpec(FloatAbs.class.getName(),   new Schema(new Schema.FieldSchema(null, DataType.FLOAT))));
        funcList.add(new FuncSpec(IntAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.INTEGER))));
        funcList.add(new FuncSpec(LongAbs.class.getName(),  new Schema(new Schema.FieldSchema(null, DataType.LONG))));
        return funcList;
    }
}
</source>

<p>この例で重要なのは <code>getArgToFuncMapping()</code> メソッドです。このメソッドは、入力スキーマから関数実装クラスへのマッピングのリストを戻します。この例では、メインの <code>ABS</code> クラスは <code>bytearray</code> 型の入力値のみを処理し、その他の型の処理は同パッケージ内の別ファイル内で実装された他のクラスに委譲しています。他のクラスの一例として、 <code>int</code> 型の入力値を処理するクラスを見てみましょう。</p>

<source>
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;

public class IntAbs extends EvalFunc&lt;Integer&gt; {
    public Integer exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        Integer d;
        try{
            d = (Integer)input.get(0);
        } catch (Exception e){
            throw new IOException("Caught exception processing input row ", e);
        }
        return Math.abs(d);
    }
}
</source>


<p>エラー処理について: <code>ABS</code> クラスは、まだ実際の型に変換されていない <code>bytearray</code> 値を処理します。 <code>NumberFormatException</code> が起きた時に null を戻しているのはこのためです。しかしながら <code>IntAbs</code> は実際の型である <code>Integer</code> に変換された上で呼ばれるので、不正なデータは既に弾かれているはずです。入力データを <code>Integer</code> にキャストできない時に例外を投げているのはこのためです。</p>
<p>以上の例は、 UDF がただ一つの引数を取り、引数の型ごとに別々の実装があるという、そこそこ単純な例ですが、もっと複雑な場合もあります。 Pig は「正確合致」の実装を見つけられない場合は、「最良合致」の実装を見つけようとします。「最良合致」の規則は、安全に使える中で最も効率的な実装を見つける、というものです。つまり Pig は、それぞれの引数について、入力の型と同じかより大きい型の中で、最も小さい型の実装を見つけます。型の優先順位は次のとおりです: <code>int&gt;long&gt;float&gt;double</code> 。</p>
<p>例として、後述する <code>piggybank</code> 中の <code>MAX</code> 関数について見てみましょう。この関数は 2 つの値を引数に取って、大きい方の値を戻します。 <code>MAX</code> の関数テーブルは次のとおりです:</p>

<source>
public List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {
    List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt;();
    Util.addToFunctionList(funcList, IntMax.class.getName(), DataType.INTEGER);
    Util.addToFunctionList(funcList, DoubleMax.class.getName(), DataType.DOUBLE);
    Util.addToFunctionList(funcList, FloatMax.class.getName(), DataType.FLOAT);
    Util.addToFunctionList(funcList, LongMax.class.getName(), DataType.LONG);

    return funcList;
}
</source>

<p><code>Util.addToFunctionList</code> はヘルパメソッドです。これは、 2 番目の引数に指定したクラス名と、 3 番目の引数に指定した型のフィールドを 2 つ含むスキーマからなるマッピングを、最初の引数である関数テーブルに追加します。</p>

<p>この関数が Pig スクリプトの中でどのように使われるか見てみましょう:</p>

<source>
REGISTER piggybank.jar
A = LOAD 'student_data' AS (name: chararray, gpa1: float, gpa2: double);
B = FOREACH A GENERATE name, org.apache.pig.piggybank.evaluation.math.MAX(gpa1, gpa2);
DUMP B;
</source>

<p>この例では、関数は <code>float</code> 型の値と <code>double</code> 型の値を引数に取っています。この場合、 double 値を 2 つ取る実装が最良です。 Pig は暗黙キャストを挿入することで、ユーザに代わってこの選択を行います。上記のスクリプトは下記のスクリプトと同じように実行されます:</p>

<source>
A = LOAD 'student_data' AS (name: chararray, gpa1: float, gpa2: double);
B = FOREACH A GENERATE name, org.apache.pig.piggybank.evaluation.math.MAX((double)gpa1, gpa2);
DUMP B;
</source>

<p>「最良合致」の特殊な場合は、スキーマが指定されていないデータを処理する時です。この時、データの型は <code>bytearray</code> と解釈されます。データの型が分からないので、最良合致を見つける方法がありません。唯一、関数テーブルが 1 つの要素だけからなっている場合のみ、キャストが実行されます。この動作は、後方互換性を維持するために役に立ちます。</p>

<p>この章の最初で示した <code>UPPER</code> 関数の例をもう一度見てみましょう。この節で書いたとおり、 <code>UPPER</code> 関数がうまく働くのは <code>chararray</code> 型の値が渡された場合のみです。型が明示的に設定されていないデータが処理できるようにするためには、 1 要素からなる関数テーブルを追加します:</p>

<source>
package myudfs;
import java.io.IOException;
import org.apache.pig.EvalFunc;
import org.apache.pig.data.Tuple;

public class UPPER extends EvalFunc&lt;String&gt;
{
    public String exec(Tuple input) throws IOException {
        if (input == null || input.size() == 0)
            return null;
        try{
            String str = (String)input.get(0);
            return str.toUpperCase();
        }catch(Exception e){
            System.err.println("WARN: UPPER: failed to process input; error - " + e.getMessage());
            return null;
        }
    }
    public List&lt;FuncSpec&gt; getArgToFuncMapping() throws FrontendException {
        List&lt;FuncSpec&gt; funcList = new ArrayList&lt;FuncSpec&gt; ();
        funcList.add(new FuncSpec(this.getClass().getName(), new Schema(new Schema.FieldSchema(null, DataType.CHARARRAY))));
        return funcList;
    }
}
</source>

<p>こうすれば、次のスクリプトが動くようになります:</p>

<source>
-- myscript.pig
REGISTER myudfs.jar;
A = LOAD 'student_data' AS (name, age, gpa);
B = FOREACH A GENERATE myudfs.UPPER(name);
DUMP B;
</source>

</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="reporting-progress">
<title>進行状況の報告</title>

<p>大きな共有システムの勘所は、システムのリソースが効率的に使われるようにすることです。その一つの側面として、暴走して処理が進んでいないプロセスを検出することがあります。 Pig はこの目的のために、ハートビートの仕組みを採用しています。いずれかのタスクがハートビートを送出しなくなったら、システムはタスクが死んだものと判断してこれを殺します。</p>
<p>ほとんどの場合、 UDF による単一のタプルの処理の実行時間はとても短いため、 UDF がハートビートを送出する必要はありません。大きなバッグに対する集約関数でも同様です。なぜなら、バッグのイテレーション処理がハートビートを送出してくれるからです。しかしながら、分単位の時間が掛かる複雑な計算を実行する関数については、コードの中で進行状況を報告する必要があります。これを実施するのはとても簡単です。 <code>EvalFunc</code> クラスが提供している <code>progress</code> メソッドを、 <code>exec</code> メソッドから呼び出すだけです。</p>
<p>例えば、 <code>UPPER</code> 関数では次のようになります:</p>

<source>
public class UPPER extends EvalFunc&lt;String&gt;
{
        public String exec(Tuple input) throws IOException {
                if (input == null || input.size() == 0)
                return null;
                try{
                        progress();
                        String str = (String)input.get(0);
                        return str.toUpperCase();
                }catch(Exception e){
                    throw new IOException("Caught exception processing input row ", e);
                }
        }
}
</source>
</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="distributed-cache">
<title>分散キャッシュを使う</title>
<p>分散キャッシュに送り込む HDFS 上のファイルは、 EvalFunc クラスの getCacheFiles メソッドで指定します。 exec メソッドの中では、ファイルが既に分散キャッシュに入っているものとして処理できます。例を挙げます:</p>
<source>
public class Udfcachetest extends EvalFunc&lt;String&gt; { 

    public String exec(Tuple input) throws IOException { 
        FileReader fr = new FileReader("./smallfile"); 
        BufferedReader d = new BufferedReader(fr); 
        return d.readLine(); 
    } 

    public List&lt;String&gt; getCacheFiles() { 
        List&lt;String&gt; list = new ArrayList&lt;String&gt;(1); 
        list.add("/user/pig/tests/data/small#smallfile"); 
        return list; 
    } 
} 

a = load '1.txt'; 
b = foreach a generate Udfcachetest(*); 
dump b;
</source>
</section>

</section>

<!-- =============================================================== -->
<!-- BEGIN LOAD/STORE FUNCTIONS -->
<section id="load-store-functions">
<title>ロード・ストア関数</title>

<p>ロード・ストア関数は Pig にデータを入力する方法と、 Pig からデータを出力する方法を制御します。一つの関数で入力と出力の双方を行う場合もありますが、そうしなくても構いません。</p>
<p>Pig のロード・ストア関数の API は Hadoop の InputFormat と OutputFormat と適合するようにできています。このため、既存の Hadoop の InputFormat や OutputFormat を元にして、少ないコードで LoadFunc や StoreFunc が作れます。データをレコードに読み込む際に最も面倒なところは InputFormat の中で、データを書き込む際に最も面倒なところは OutputFormat の中で行います。このため、 Hadoop の InputFormat と OutputFormat さえ用意されていれば、 Pig が新しい形式のデータを読み書きできるようにすることは容易です。</p>
<p><strong>注意:</strong> LoadFunc と StoreFunc はいずれも Hadoop 0.20 の API を使うようにできています (InputFormat, OutputFormat その他のクラスについて) 。これらのクラスは org.apache.hadoop.mapred パッケージの中ではなく、<strong>新しい</strong> org.apache.hadoop.mapreduce パッケージの中にあります。</p>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="load-functions">
<title>ロード関数</title>
<p id="loadfunc"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadFunc.java?view=markup">LoadFunc</a> 抽象クラスには主に 3 つのメソッドがあります。多くの場合このクラスを拡張すれば充分間に合います。より多くの機能が必要な場合のために 3 つのオプショナルなインタフェースがあります:</p>

<ul>
<li id="LoadMetadata"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadMetadata.java?view=markup">LoadMetadata</a> には、メタデータを扱うためのメソッドがあります。多くの場合、何らかのメタデータの仕組みを扱わない限り、ロード関数の実装がこのインタフェースを実装する必要はありません。インタフェース中の getSchema() メソッドによって、データのスキーマを Pig に戻すことができます。ロード関数が DataByteArray 型のフィールドでなく実際の型のフィールドからなるデータを戻す場合、 getSchema() メソッドによってデータのスキーマを戻す必要があります。その他のメソッドは、パーティションキーや統計情報などのメタデータに関連するものです。該当する情報がない場合、これらのメソッドは null を戻すようにします。</li>

<li id="LoadPushDown"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadPushDown.java?view=markup">LoadPushDown</a> には、 Pig の処理をロード関数側にプッシュダウン最適化するためのメソッドがあります。現在のところ、 Pig が呼び出すのは pushProjection() メソッドだけです。このメソッドは Pig スクリプト中で実際に使われているフィールドをロード関数に伝えるために呼び出されます。ロード関数の実装は、要求を受け付けて必要なフィールドだけを戻すこともできますし、要求を受け付けずすべてのフィールドを戻すこともできます。ロード関数がフィールドを効率的に絞り込める場合、クエリの性能を上げるために LoadPushDown インタフェースを実装します。ロード関数がフィールドを絞り込むか絞り込まないかにかかわらず、 getSchema() メソッドを実装している場合には、データのタプル全体を表すスキーマを戻す必要があります。
<ul>
	<li id="pushprojection">pushProjection(): ロード関数の実装に対して、 Pig スクリプト中でどのフィールドが必要かを伝えます。これを受けてロード関数の実装は、必要なフィールドだけを読み込むことで性能を上げることができます。 pushProjection() メソッドは引数として requiredFieldList を取ります。 requiredFieldList は読み取り専用の引数で、 requiredField のリストからなります。それぞれの requiredField は Pig スクリプトで要求されたフィールドを表しており、 index, alias, type (将来の拡張用) , subFields からなります。必要な列のインデックスは requiredField.index に格納されます。必要なフィールドがマップだった場合、必要なキーのリストが requiredField.subFields に格納されます。たとえば、 Pig スクリプトがマップのキーとして "key1" と "key2" を使っていた場合、このマップに対する subFields は 2 つの requiredField を格納します。最初の requiredField の alias は "key1" で、二番目の requiredField の alias は "key2" です。 pushProjection() メソッドへのリクエストが受け付けられたかどうかを通知するには、戻り値中の requiredFieldResponse.requiredFieldRequestHonored を使います。</li>
</ul>
</li>

<li id="loadcaster"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/LoadCaster.java?view=markup">LoadCaster</a> には、 bytearray を特定の型に変換するメソッドがあります。 DataByteArray のフィールドから他の型へのキャスト (暗黙・明示的いずれも) をサポートしたい場合に、ロード関数はこのインタフェースを実装します。</li>
</ul>

 <p>LoadFunc 抽象クラスはロード関数を実装するためのメインのクラスです。オーバーライドする必要があるメソッドは次のとおりです:</p>
 <ul>
 <li id="getInputFormat">getInputFormat(): Pig はこのメソッドを呼び出してロード関数が使う InputFormat を知ります。 InputFormat と RecordReader のメソッドは Hadoop MapReduce と同じ方法と文脈で呼び出されます。 Hadoop に同梱されている InputFormat の実装を使う場合には、 org.apache.hadoop.mapreduce パッケージ下の新 API 版を使います。自作の InputFormat を使う場合には、 InputFormat は org.apache.hadoop.mapreduce 下の新 API を使って実装する必要があります。<br></br> <br></br> 
 
 テキストベースの InputFormat やファイルベースの InputFormat を使う自作のロード関数で、サブディレクトリ中のファイルを再帰的に読み込みたい場合には、 org.apache.pig.backend.hadoop.executionengine.mapReduceLayer パッケージ中の PigTextInputFormat か PigFileInputFormat クラスを使う必要があります。 Pig の InputFormat 実装を使うことで、指定されたディレクトリの一段下しか読まないという Hadoop 標準の TextInputFormat と FileInputFormat の制限が回避できます。たとえば、 LOAD 文で入力元として 'dir1' を指定し、このディレクトリのサブディレクトリとして 'dir2' と 'dir2/dir3' がある場合、 Hadoop 標準の TextInputFormat と FileInputFormat は 'dir1' 直下のファイルだけを読み込みます。 PigTextInputFormat か PigFileInputFormat を使うか、それらを拡張したクラスを使えば、全ディレクトリ中のファイルが読み込まれます。</li>
 
 <li id="setLocation">setLocation(): Pig はロード元の場所をロード関数に伝えるためにこのメソッドを呼び出します。ロード関数はこのメソッドの中で、同じ情報を InputFormat に伝える必要があります。このメソッドは複数回呼ばれる可能性があるため、不整合を起こす副作用がないように注意する必要があります。</li>
 
 <li id="prepareToRead">prepareToRead(): ロード関数が使う InputFormat に紐づいた RecordReader をロード関数に渡します。ロード関数実装は getNext() メソッドの中でこの RecordReader を使ってレコードを読み込みタプルを戻します。</li>
 
 <li id="getnext">getNext(): Pig はデータから新しいタプルを読み込むためにこのメソッドを呼び出します。このメソッドは RecordReader を使ってデータを読み込み、タプルを作って戻します。</li>
 </ul>

 <p id="loadfunc-default">次のメソッドは LoadFunc クラス内にデフォルトの実装がありますが、必要に応じてオーバーライドします:</p>
 <ul>
 <li id="setUdfContextSignature">setUdfContextSignature(): Pig はロード関数に対して固有のシグネチャを通知するために、フロントエンドとバックエンドの双方でこのメソッドを呼び出します。ロード関数はこのシグネチャを使って、フロントエンドとバックエンドでの多くのメソッド呼び出し間で共有する必要がある情報を UDFContext に格納します。一つの使い道として、 LoadPushDown.pushProjection(RequiredFieldList) が渡された RequiredFieldList を UDFContext に格納し、バックエンドの getNext() メソッドがタプルを構築する前にこれを参照する、ということが挙げられます。 LoadFunc クラス内のデフォルト実装は何もしていません。このメソッドは他のメソッドが呼び出される前に呼び出されます。</li>
 
 <li id="relativeToAbsolutePath">relativeToAbsolutePath(): Pig はロード元の相対位置を絶対位置に変換するためにこのメソッドを呼び出します。 LoadFunc クラス内のデフォルト実装は、 FileSystem 内の表現として相対位置を解決します。データのロード元がどこか他の場所の場合、ロード関数はこのメソッドをオーバーライドします。</li>
 </ul>

<p><strong>実装例</strong></p>
<p>次のロード関数は、 '\n' を行区切りとし、 '\t' をデフォルトのフィールド区切りとして、テキストデータを読み込みます。フィールド区切りはコンストラクタで指定することもできます。つまり、 PigStorage のロード関数と同じです。この実装は既存の Hadoop の TextInputFormat を InputFormat として使います。</p>
<source>
public class SimpleTextLoader extends LoadFunc {
    protected RecordReader in = null;
    private byte fieldDel = '\t';
    private ArrayList&lt;Object&gt; mProtoTuple = null;
    private TupleFactory mTupleFactory = TupleFactory.getInstance();
    private static final int BUFFER_SIZE = 1024;

    public SimpleTextLoader() {
    }

    /**
     * 指定された文字をフィールド区切り文字としてロード関数を構築します.
     *
     * @param delimiter
     *            フィールドを区切る 1 バイト文字.
     *            (デフォルトは "\t".)
     */
    public SimpleTextLoader(String delimiter) {
        this();
        if (delimiter.length() == 1) {
            this.fieldDel = (byte)delimiter.charAt(0);
        } else if (delimiter.length() &gt;  1 &amp; &amp; delimiter.charAt(0) == '\\') {
            switch (delimiter.charAt(1)) {
            case 't':
                this.fieldDel = (byte)'\t';
                break;

            case 'x':
               fieldDel =
                    Integer.valueOf(delimiter.substring(2), 16).byteValue();
               break;

            case 'u':
                this.fieldDel =
                    Integer.valueOf(delimiter.substring(2)).byteValue();
                break;

            default:
                throw new RuntimeException("Unknown delimiter " + delimiter);
            }
        } else {
            throw new RuntimeException("PigStorage delimeter must be a single character");
        }
    }

    @Override
    public Tuple getNext() throws IOException {
        try {
            boolean notDone = in.nextKeyValue();
            if (notDone) {
                return null;
            }
            Text value = (Text) in.getCurrentValue();
            byte[] buf = value.getBytes();
            int len = value.getLength();
            int start = 0;

            for (int i = 0; i &lt; len; i++) {
                if (buf[i] == fieldDel) {
                    readField(buf, start, i);
                    start = i + 1;
                }
            }
            // 最後のフィールドを読み込む
            readField(buf, start, len);

            Tuple t =  mTupleFactory.newTupleNoCopy(mProtoTuple);
            mProtoTuple = null;
            return t;
        } catch (InterruptedException e) {
            int errCode = 6018;
            String errMsg = "Error while reading input";
            throw new ExecException(errMsg, errCode,
                    PigException.REMOTE_ENVIRONMENT, e);
        }

    }

    private void readField(byte[] buf, int start, int end) {
        if (mProtoTuple == null) {
            mProtoTuple = new ArrayList&lt;Object&gt;();
        }

        if (start == end) {
            // NULL value
            mProtoTuple.add(null);
        } else {
            mProtoTuple.add(new DataByteArray(buf, start, end));
        }
    }

    @Override
    public InputFormat getInputFormat() {
        return new TextInputFormat();
    }

    @Override
    public void prepareToRead(RecordReader reader, PigSplit split) {
        in = reader;
    }

    @Override
    public void setLocation(String location, Job job)
            throws IOException {
        FileInputFormat.setInputPaths(job, location);
    }
}
</source>

</section>
<!-- END LOAD FUNCTION -->

<!-- =============================================================== -->
<section id="store-functions">
<title>ストア関数</title>

<p id="storefunc"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/StoreFunc.java?view=markup">StoreFunc</a> 抽象クラスにはデータを保存するためのメソッドがあります。多くの場合、このクラスを拡張すれば充分間に合います。より多くの機能が必要な場合のために、オプショナルなインタフェースが 1 つあります:</p>
<ul>
<li id="storemetadata"><a href="http://svn.apache.org/viewvc/pig/trunk/src/org/apache/pig/StoreMetadata.java?view=markup">StoreMetadata:</a> スキーマや統計情報を保存するメタデータの仕組みのためのメソッドがあります。このインタフェースはオプショナルです。メタデータを保存する必要がある場合だけ実装します。</li>
</ul>

<p id="storefunc-override">StoreFunc 中でオーバーライドする必要があるメソッドは次のとおりです:</p>
<ul>
<li id="getOutputFormat">getOutputFormat(): Pig はこのメソッドを呼び出してストア関数が使う OutputFormat を知ります。 OutputFormat, RecordWriter, OutputCommitter のメソッドは Hadoop MapReduce と同じ文脈で呼び出されます。 Hadoop に同梱された OutputFormat の実装を使う場合には、 org.apache.hadoop.mapreduce パッケージ下の新 API 版を使います。自作の OutputFormat を使う場合には、 org.apache.hadoop.mapreduce 下の新 API を使って実装する必要があります。 Pig は出力先の場所を知るために、前もって OutputFormat クラスの checkOutputSpecs() メソッドを呼びます。このメソッドはジョブが起動する際に Hadoop からも呼び出されます。このため、不整合を起こす副作用がないように注意する必要があります。</li>
<li id="setStoreLocation">setStoreLocation(): Pig は出力先の場所をストア関数に伝えるためにこのメソッドを呼びます。ストア関数はこのメソッドの中で、同じ情報を OutputFormat に伝える必要があります。このメソッドは複数回呼ばれる可能性があるため、不整合を起こす副作用がないように注意する必要があります。</li>
<li id="prepareToWrite">prepareToWrite(): データの書き込みは StoreFunc で指定された OutputFormat を通じて行われます。 prepareToWrite() メソッドは、 OutputFormat に紐づいた RecordWriter をストア関数に渡します。ストア関数実装は putNext() メソッドの中で、この RecordWriter を使ってデータのレコードを表すタプルを書き込みます。</li>
<li id="putNext">putNext(): Pig はタプルを書き込むためのこのメソッドを呼び出します。このメソッドは、 RecordWriter を使ってタプルを書き込みます。</li>
</ul>

<p id="storefunc-default">次のメソッドは StoreFunc クラス内にデフォルトの実装がありますが、必要に応じてオーバーライドします。</p>
<ul>
<li id="setStoreFuncUDFContextSignature">setStoreFuncUDFContextSignature(): Pig はストア関数に対して固有のシグネチャを通知するために、フロントエンドとバックエンドの双方でこのメソッドを呼び出します。ストア関数はこのシグネチャを使って、フロントエンドとバックエンドでの多くのメソッド呼び出し間で共有する必要がある情報を UDFContext に格納します。デフォルトの実装は何もしていません。このメソッドは他のメソッドが呼び出される前に呼び出されます。</li>
<li id="relToAbsPathForStoreLocation">relToAbsPathForStoreLocation(): Pig は保存先の相対位置を絶対位置に変換するためにこのメソッドを呼び出します。 StoreFunc クラス内のデフォルトの実装は FileSystem 内の表現として相対位置を解決します。</li>
<li id="checkschema">checkSchema(): データのスキーマが、そのストア関数で書き込めるものかどうかをチェックするために、このメソッドを実装します。 StoreFunc クラス内のデフォルトの実装は何もしていません。このメソッドは setStoreLocation() の前に呼び出されます。</li>
</ul>

<p><strong>実装例</strong></p>
<p>次のストア関数の実装例は、 '\n' を行区切りとし、 '\t' をデフォルトのフィールド区切りとしてテキストデータを書き込みます。フィールド区切りはコンストラクタで指定することもできます。つまり、 PigStorage のストア関数と同じです。この実装は 既存の Hadoop の TextOutputFormat を OutputFormat として使います。</p>

<source>
public class SimpleTextStorer extends StoreFunc {
    protected RecordWriter writer = null;

    private byte fieldDel = '\t';
    private static final int BUFFER_SIZE = 1024;
    private static final String UTF8 = "UTF-8";
    public PigStorage() {
    }

    public PigStorage(String delimiter) {
        this();
        if (delimiter.length() == 1) {
            this.fieldDel = (byte)delimiter.charAt(0);
        } else if (delimiter.length() > 1delimiter.charAt(0) == '\\') {
            switch (delimiter.charAt(1)) {
            case 't':
                this.fieldDel = (byte)'\t';
                break;

            case 'x':
               fieldDel =
                    Integer.valueOf(delimiter.substring(2), 16).byteValue();
               break;
            case 'u':
                this.fieldDel =
                    Integer.valueOf(delimiter.substring(2)).byteValue();
                break;

            default:
                throw new RuntimeException("Unknown delimiter " + delimiter);
            }
        } else {
            throw new RuntimeException("PigStorage delimeter must be a single character");
        }
    }

    ByteArrayOutputStream mOut = new ByteArrayOutputStream(BUFFER_SIZE);

    @Override
    public void putNext(Tuple f) throws IOException {
        int sz = f.size();
        for (int i = 0; i &lt; sz; i++) {
            Object field;
            try {
                field = f.get(i);
            } catch (ExecException ee) {
                throw ee;
            }

            putField(field);

            if (i != sz - 1) {
                mOut.write(fieldDel);
            }
        }
        Text text = new Text(mOut.toByteArray());
        try {
            writer.write(null, text);
            mOut.reset();
        } catch (InterruptedException e) {
            throw new IOException(e);
        }
    }

    @SuppressWarnings("unchecked")
    private void putField(Object field) throws IOException {
        //区切り文字の文字列定数
        String tupleBeginDelim = "(";
        String tupleEndDelim = ")";
        String bagBeginDelim = "{";
        String bagEndDelim = "}";
        String mapBeginDelim = "[";
        String mapEndDelim = "]";
        String fieldDelim = ",";
        String mapKeyValueDelim = "#";

        switch (DataType.findType(field)) {
        case DataType.NULL:
            break; // just leave it empty

        case DataType.BOOLEAN:
            mOut.write(((Boolean)field).toString().getBytes());
            break;

        case DataType.INTEGER:
            mOut.write(((Integer)field).toString().getBytes());
            break;

        case DataType.LONG:
            mOut.write(((Long)field).toString().getBytes());
            break;

        case DataType.FLOAT:
            mOut.write(((Float)field).toString().getBytes());
            break;

        case DataType.DOUBLE:
            mOut.write(((Double)field).toString().getBytes());
            break;

        case DataType.BYTEARRAY: {
            byte[] b = ((DataByteArray)field).get();
            mOut.write(b, 0, b.length);
            break;
                                 }

        case DataType.CHARARRAY:
            // oddly enough, writeBytes writes a string
            mOut.write(((String)field).getBytes(UTF8));
            break;

        case DataType.MAP:
            boolean mapHasNext = false;
            Map&lt;String, Object&gt; m = (Map&lt;String, Object&gt;)field;
            mOut.write(mapBeginDelim.getBytes(UTF8));
            for(Map.Entry&lt;String, Object&gt; e: m.entrySet()) {
                if(mapHasNext) {
                    mOut.write(fieldDelim.getBytes(UTF8));
                } else {
                    mapHasNext = true;
                }
                putField(e.getKey());
                mOut.write(mapKeyValueDelim.getBytes(UTF8));
                putField(e.getValue());
            }
            mOut.write(mapEndDelim.getBytes(UTF8));
            break;

        case DataType.TUPLE:
            boolean tupleHasNext = false;
            Tuple t = (Tuple)field;
            mOut.write(tupleBeginDelim.getBytes(UTF8));
            for(int i = 0; i &lt; t.size(); ++i) {
                if(tupleHasNext) {
                    mOut.write(fieldDelim.getBytes(UTF8));
                } else {
                    tupleHasNext = true;
                }
                try {
                    putField(t.get(i));
                } catch (ExecException ee) {
                    throw ee;
                }
            }
            mOut.write(tupleEndDelim.getBytes(UTF8));
            break;

        case DataType.BAG:
            boolean bagHasNext = false;
            mOut.write(bagBeginDelim.getBytes(UTF8));
            Iterator&lt;Tuple&gt; tupleIter = ((DataBag)field).iterator();
            while(tupleIter.hasNext()) {
                if(bagHasNext) {
                    mOut.write(fieldDelim.getBytes(UTF8));
                } else {
                    bagHasNext = true;
                }
                putField((Object)tupleIter.next());
            }
            mOut.write(bagEndDelim.getBytes(UTF8));
            break;

        default: {
            int errCode = 2108;
            String msg = "Could not determine data type of field: " + field;
            throw new ExecException(msg, errCode, PigException.BUG);
        }

        }
    }

    @Override
    public OutputFormat getOutputFormat() {
        return new TextOutputFormat&lt;WritableComparable, Text&gt;();
    }

    @Override
    public void prepareToWrite(RecordWriter writer) {
        this.writer = writer;
    }

    @Override
    public void setStoreLocation(String location, Job job) throws IOException {
        job.getConfiguration().set("mapred.textoutputformat.separator", "");
        FileOutputFormat.setOutputPath(job, new Path(location));
        if (location.endsWith(".bz2")) {
            FileOutputFormat.setCompressOutput(job, true);
            FileOutputFormat.setOutputCompressorClass(job,  BZip2Codec.class);
        }  else if (location.endsWith(".gz")) {
            FileOutputFormat.setCompressOutput(job, true);
            FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);
        }
    }
}
</source>

</section>
<!-- END STORE FUNCTION -->
</section>
<!-- END LOAD/STORE FUNCTIONS -->

<!-- =============================================================== -->
<section id="use-short-names">
<title>短い名前を使う</title>

<p>短い名前を使って Java UDF を呼び出す方法は 2 つあります。インポートリストにパッケージを指定する方法と、 DEFINE 文で別名を定義する方法です。</p>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="import-lists">
<title>インポートリスト</title>
<p>インポートリストに UDF が属するパッケージを指定することで、 UDF 呼び出しごとに完全修飾名を書く必要がなくなります。インポートリストは、 Pig 起動時にコマンドラインから Java プロパティ udf.import.list で指定します:</p>
<source>
pig -Dudf.import.list=com.yahoo.yst.sds.ULT
</source>
<p>複数の場所を指定することもできます:</p>
<source>
pig -Dudf.import.list=com.yahoo.yst.sds.ULT:org.apache.pig.piggybank.evaluation
</source>
<p>スクリプト中でインポートされた UDF を使うには、次のようにします:</p>
<source>
myscript.pig:
A = load '/data/SDS/data/searcg_US/20090820' using ULTLoader as (s, m, l);
....

command:
pig -cp sds.jar -Dudf.import.list=com.yahoo.yst.sds.ULT myscript.pig 
</source>
</section>

<!-- +++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="define-alias">
<title>別名を定義する</title>
<p><a href="basic.html#define-udfs">DEFINE 文</a>を使って、 関数の別名が定義できます:</p>

<source>
REGISTER piggybank.jar
DEFINE MAXNUM org.apache.pig.piggybank.evaluation.math.MAX;
A = LOAD 'student_data' AS (name: chararray, gpa1: float, gpa2: double);
B = FOREACH A GENERATE name, MAXNUM(gpa1, gpa2);
DUMP B;
</source>

<p>DEFINE 文の最初の引数は関数の別名、二番目の引数は関数の完全修飾名です。 DEFINE 文の実行以降は、別名を使って関数が呼び出せます。</p>

</section>

</section>


<!-- =============================================================== -->
<section>
<title>高度な話題</title>

<section id="udf-interfaces">
<title>各種 UDF インタフェース</title>
<p>Java の UDF はいくつかの方法で呼び出されます。最も単純な UDF は単に EvalFunc クラスを拡張し、 exec メソッドを実装するものです (see <a href="#eval-functions-write">評価関数の書き方</a>を参照してください) 。すべての評価関数はこのメソッドを実装します。加えて、関数が代数的である場合は、 <code>Algebraic</code> インタフェースを実装することで、 Combiner が使える場合には目に見えてクエリ性能が改善できます (<a href="#algebraic-interface">Algebraic インタフェース</a>を参照してください) 。最後に、関数が少しずつデータを処理できる場合には、 Accumulator インタフェースを実装することでメモリ使用量が改善できます (<a href="#accumulator-interface">Accumulator インタフェース</a>を参照してください) 。</p>

<p>オプティマイザは UDF の型とクエリの種類に応じて、 UDF を呼び出すのに適切なメソッドを選びます。一度に使われるのはひとつのインタフェースだけで、オプティマイザは関数を実行するのに最も効率的な方法を選びます。 Combiner が使われいて、関数が Algebraic インタフェースを実装している場合は、関数を呼び出すのに Algebraic インタフェースが使われます。 Combiner が使われていない場合、クエリ実行で Accumulator が使用可能で、関数が Accumulator を実装していれば、このインタフェースが使われます。どちらの条件も満たされなければ、 UDF を実行するのに exec メソッドが使われます。</p>
 </section>
 
 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="function-instantiation">
<title>関数のインスタンス化</title>
<p>多くのユーザが、 UDF のコンストラクタが呼ばれる回数をあらかじめ仮定するという間違いを犯しています。たとえば、ストア関数のコンストラクタで補助ファイルを作るのはいい考えだと思えるかも知れません。このやり方の問題は、多くの場合 Pig が、データのスキーマを検証するなどの目的のために、クライアント側でストア関数をインスタンス化するという事です。</p>
<p>ユーザは関数がインスタンス化される回数について仮定を置くべきではありません。その代わり、複数回呼び出されても大丈夫なようにコードを書くべきです。たとえば、ファイル作成前に存在チェックをする、などです。</p>
</section>

 
  <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="udf-configurations">
<title>UDF に設定を渡す</title>
<p>シングルトンクラスである UDFContext には UDF の作者向けの 2 つの機能があります。一つ目の機能として、バックエンドでは、 UDF 実装は getJobConf メソッドを呼ぶことで JobConf オブジェクトが得られます。このメソッドが呼べるのはバックエンド (実行時) だけです。フロントエンド (実行計画時) には、まだ JobConf が構築されていないからです。</p>

<p>二つ目の機能として、 UDFContext を使うことでフロントエンドからバックエンドに対して設定情報を渡すことができます。 UDF はフロントエンドのコンストラクタや describeSchema メソッド等で情報を設定オブジェクトに詰め込みます。これらの情報は、 EvalFunc の exec メソッドや LoadFunc の getNext メソッド等から読み出せます。 UDFContext を使っても、バックエンドの関数インスタンス間で情報をやり取りすることはできないということに注意してください。やり取りはフロントエンドからバックエンドに向けてのみ可能です。</p>

<p>情報を格納するには、 getUDFProperties をを呼び出します。このメソッドは情報を読み書きするための Properties オブジェクトを戻します。名前の衝突を避けるために、 Properties オブジェクトを取得するにはシグネチャを使う必要があります。これには二つの方法があります。一つは、 UDF のクラスオブジェクト (this.getClass()) をシグネチャとして使う方法です。この場合、 UDF は呼び出し間で Properties オブジェクトを共有することになります。二つ目は、クラスと文字列の配列の組み合わせをシグネチャとして使う方法です。コンストラクタ引数やその他識別のための文字列がシグネチャとして使えます。この場合、 UDF 呼び出しごとに違う Properties オブジェクトを使うことになるので、 UDF 呼び出しごとの名前空間の衝突が避けられます。</p>
</section>

 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="udf-monitoring">
<title>実行時間の長い UDF を監視する</title>
<p>UDF の実行時間がほとんどの場合に短いのに、一部だけあまりにも実行時間が長い、という例に時折行き当たるかも知れません。これはたとえば、自由形式の文字列に対して複雑な正規表現による解析を行ったり、 UDF が外部のサービスと通信したりする場合に起こり得ます。 Pig 0.8 以降、 UDF の実行時間を呼び出しごとに監視し、実行時間が長すぎる場合に実行を中断する機能が追加されました。この機能は簡単な Java アノテーションを付けることで有効にできます:</p>
	
<source>
	import org.apache.pig.builtin.MonitoredUDF;
	
	@MonitoredUDF
	public class MyUDF extends EvalFunc&lt;Integer&gt; {
	  /* 処理の実装 */
	}
</source>

<p>UDF に上記のようなアノテーションを付けることで、 exec() メソッドの実行時間が 10 秒を超える場合に、 exec() メソッドの実行が中断され、 null が戻るようになります。タイムアウト時間と、タイムアウト時のデフォルト値もアノテーションによって指定することができます:</p>

<source>
	import org.apache.pig.builtin.MonitoredUDF;
	
	@MonitoredUDF(timeUnit = TimeUnit.MILLISECONDS, duration = 100, intDefault = 10)
	public class MyUDF extends EvalFunc&lt;Integer&gt; {
	  /* 処理の実装 */
	}
</source>

<p>デフォルト値として指定できるのは intDefault, longDefault, doubleDefault, floatDefault, stringDefault です。 UDF の戻り値型に応じて、適切なデフォルト値が使われます。タプルやバッグのデフォルト値を設定することは、今のところできません。</p>

<p>必要であれば、 MonitoredUDFExecutor.ErrorCallback のサブクラス中の静的メソッド、 handleError メソッドか handleTimeout メソッドで、自作のエラー処理を実装することもできます。いずれのメソッドも、引数として例外を発生した EvalFunc オブジェクトと、例外オブジェクトを取ります。これにより、エラー処理のために必要な情報が得られるかも知れません。デフォルトの挙動は、エラーが発生するたびに Hadoop のカウンタを 1 ずつ増やすことです。独自処理のために ErrorCallback の実装を作ったら、アノテーションで次のように指定できます。</p>

<source>
	import org.apache.pig.builtin.MonitoredUDF;

	@MonitoredUDF(errorCallback=MySpecialErrorCallback.class)
	public class MyUDF extends EvalFunc&lt;Integer&gt; {
	  /* 処理の実装 */
	}
</source>

<p>現在のところ、 MonitoredUDF アノテーションは単純な UDF と Algebraic な UDF のみで動作し、 Accumulator モードでは動作しません。</p>

</section>
</section>
</section>


<!-- =============================================================== -->
<section id="python-udfs">
<title>Python UDF を書く</title>

<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="register-python">
<title>UDF を登録する</title>
<p>Python スクリプトは次のようにして登録できます。この例では Python スクリプトを実行するのに org.apache.pig.scripting.jython.JythonScriptEngine を使っています。複数言語をサポートし、実行するために、自作のスクリプトエンジンが開発できます。現在のところ、 Pig は「jython」キーワードを識別し、実行に必要なスクリプトエンジン (Jython) を配布します。</p>
<source>
Register 'test.py' using jython as myfuncs;
</source>

<p>次の書き方も可能です。 test.py 中の全関数が名前空間 myfuncs の中に作られます。</p>
<source>
register 'test.py' using org.apache.pig.scripting.jython.JythonScriptEngine as myfuncs;
</source>


<p>典型的な test.py は次のようなものです:</p>
<source>
@outputSchema("word:chararray")
def helloworld():  
  return 'Hello, World'

@outputSchema("word:chararray,num:long")
def complex(word):
  return str(word),len(word)

@outputSchemaFunction("squareSchema")
def square(num):
  return ((num)*(num))

@schemaFunction("squareSchema")
def squareSchema(input):
  return input

# No decorator - bytearray
def concat(str):
  return str+str
</source>

<p>上記の REGISTER 文は test.py 中の Python 関数を、 Pig の指定した名前空間 (ここでは myfuncs) の中に登録します。これにより、以降 Pig スクリプトの中で myfuncs.helloworld(), myfuncs.complex(), and myfuncs.square() のように関数が使えるようになります。使用例は次のとおりです:</p>

<source>
b = foreach a generate myfuncs.helloworld(), myfuncs.square(3);
</source>


<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
</section>
<section id="decorators">
<title>デコレータとスキーマ</title>
<p>Python スクリプト中の UDF の戻り値の型を Pig が判別できるように、 Python のデコレータを使って出力スキーマを定義します。</p>
<ul>
<li id="outputschema">outputSchema - Pig が解釈可能な書式でスクリプト UDF の出力スキーマを定義します。</li>
<li id="outputfunctionschema">outputFunctionSchema - 入力型に応じて出力スキーマを戻す関数を、委譲先として定義します。このデコレータは、入力型として複数の型を受け入れ、入力型に応じた処理を行う関数のために用意されています。複数の型を受け入れる関数の単純な例は square 関数です。この場合、出力スキーマを生成する関数は、単純に入力スキーマをそのまま戻します。</li>
<li id="schemafunction">schemaFunction - 出力スキーマを戻す委譲先の関数を定義します。この関数は Pig の UDF としては登録されません。</li>
</ul>

<p>デコレータが指定されていなければ、 Pig は出力型が bytearray であると仮定し、関数の戻り値を bytearray 値に変換します。この動作は Java UDF における振る舞いと整合しています。</p>

<p>スキーマ文字列の例はたとえば次のとおりです: y:{t:(word:chararray,num:long)} 。スキーマ文字列中の変数名は使われません。この変数名は単に、パーサが文字列を解釈できるようにすためだけのものです。</p>
</section>


<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section>
 <title>スクリプト例</title>
 <p>文字列操作、数値計算、データ型判別のような単純な処理なら、 Java で複雑な UDF を作らなくても、 Python で簡単にやってのけられます。スクリプト言語を使うことによるオーバーヘッドはかなり少なく、開発コストはほとんど無視できるほどです。次のコードは Pig で使える Python UDF の例です:</p>
 
 <source>
 mySampleLib.py
 ---------------------
 #/usr/bin/python
 
 ##################
 # Math functions #
 ##################
 #square - 任意の型の数値の二乗
 @outputSchemaFunction("squareSchema")
 def square(num):
   return ((num)*(num))
 @schemaFunction("squareSchema")
 def squareSchema(input):
   return input
 
 #Percent- Percentage
 @outputSchema("percent:double")
 def percent(num, total):
   return num * 100 / total
 
 ####################
 # String Functions #
 ####################
 #commaFormat- 数値をカンマで桁区切りする。例: 12345-> 12,345
 @outputSchema("numformat:chararray")
 def commaFormat(num):
   return '{:,}'.format(num)
 
 #concatMultiple- concat multiple words
 @outputSchema("onestring:chararray")
 def concatMult4(word1, word2, word3, word4):
   return word1 word2 word3 word4
 
 #######################
 # Data Type Functions #
 #######################
 #collectBag- バッグ中の要素を他のバッグに詰める
 #グループ化後の処理に有用
 @outputSchema("y:bag{t:tuple(len:int,word:chararray)}") 
 def collectBag(bag):
   outBag = []
   for word in bag:
     tup=(len(bag), word[1])
     outBag.append(tup)
   return outBag
 
 # いくつかのコメント- 
 # Pig のバッグはタプルのバッグです。 Python UDF もこのパターンをなぞっています。
 # Python のタプルは変更不可能であり、タプルに要素を追加することはできません。
 </source>
 </section>
 
 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="python-advanced">
 <title>高度な話題</title> 
  <section id="import-modules">
  
<!-- ++++++++++++++++++++ -->
<title>モジュールをインポートする</title>
<p>Python スクリプト中では Python のモジュールがインポートできます。 Pig は Python の依存性を再帰的に解決します。つまり、 Pig は必要な Python モジュールをすべてバックエンドに送り込みます。 Python のモジュールは Jython の探索パス上に置く必要があります: JYTHON_HOME, JYTHON_PATH, あるいはカレントディレクトリです。</p>
</section>

<!-- ++++++++++++++++++++ -->
<section id="combined-scripts">
<title>複合スクリプト</title>
  <p>UDF と Pig のスクリプトは一般的に別々のファイルに保存されます。しかし、テストの用途のため、コードを単一の「複合」スクリプトとして保存することもできます。ただし、「複合」スクリプトをある言語に埋め込んだら、 UDF はその言語で書く必要があります。</p>
 
 <p>下の例では Python と Pig を複合しています。この場合、「複合」スクリプトは Python だけに埋め込めます。</p>
 <p>Pig スクリプトの制御の流れを UDF 定義から分離するため、 Pig スクリプトは <code>if __name__ == '__main__': </code> の下で実行する必要があります。そうしないと、スクリプトはエラーを発生します。</p>
 <source>
 #!/usr/bin/jython
from org.apache.pig.scripting import *

@outputSchema("word:chararray")
def helloworld():  
   return 'Hello, World'
  
if __name__ == '__main__':
       P = Pig.compile("""a = load '1.txt' as (a0, a1);
                          b = foreach a generate helloworld();
                          store b into 'myoutput'; """)

result = P.bind().runSingle();
 </source>
   </section>

  </section>
</section>


 <!-- =============================================================== -->
 <section id="js-udfs">
 <title>JavaScript UDF を書く</title>
 
 <p><strong>注記:</strong> <em>JavaScript UDF は実験的機能です。</em></p>

<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="register-js">
 <title>UDF を登録する</title>
 <p>JavaScript の UDF は次のようにして登録できます。この例では JavaScript を実行するのに org.apache.pig.scripting.js.JsScriptEngine を使っています。複数言語をサポートし、実行するために、自作のスクリプトエンジンが開発できます。現在のところ、 Pig は「js」キーワードを識別し、実行に必要なスクリプトエンジン (Rhino) を配布します。</p>
 <source>
 register 'test.js' using javascript as myfuncs;
 </source>
 
 <p>次の書き方も可能です。 test.js 中の全関数が名前空間 myfuncs の中に作られます。</p>
 <source>
 register 'test.js' using org.apache.pig.scripting.js.JsScriptEngine as myfuncs;
 </source>

 <p>上記の REGISTER 文は test.js 中の JavaScript 関数を、 Pig の指定した名前空間 (ここでは myfuncs) の中に登録します。これにより、以降 Pig スクリプトの中で myfuncs.helloworld(), myfuncs.complex(), and myfuncs.square() のように関数が使えるようになります。使用例は次のとおりです:</p>
 
 <source>
 b = foreach a generate myfuncs.helloworld(), myfuncs.complex($0);
 </source>
 </section>
 
 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="schema-return-types">
 <title>戻り値の型とスキーマ</title>
 <p>JavaScript の関数は第一級のオブジェクトであるため、属性を追加することで注釈が付けられます。 UDF の戻り値の型を Pig が判別できるようにするため、関数に outputSchema 属性を追加します。</p>
 <ul>
 <li>outputSchema - Pig が解釈可能な書式で UDF の出力のスキーマを定義します。</li>
 <li>スキーマ文字列の例 - y:{t:(word:chararray,num:long)} <br></br>スキーマ文字列中の変数名は Pig と JavaScript の間で型変換を行う際に用いられます。つまり、 Pig タプルと JavaScript オブジェクト間の変換は変数名を使って行われます。</li>
 </ul>
 </section>
 
 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="js-example">
 <title>スクリプト例</title> 
  <p>単純な JavaScript UDF (udf.js) の例は次のとおりです。</p>
 <source>
 helloworld.outputSchema = "word:chararray";
function helloworld() {
    return 'Hello, World';
}
    
complex.outputSchema = "word:chararray,num:long";
function complex(word){
    return {word:word, num:word.length};
}
</source>
 
<p>Pig スクリプトは次のようにして JavaScript UDF (udf.js) を登録します。</p>
<source>
 register ‘udf.js’ using javascript as myfuncs; 
A = load ‘data’ as (a0:chararray, a1:int);
B = foreach A generate myfuncs.helloworld(), myfuncs.complex(a0);
... ... 
</source>
  </section>
  
   <!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="js-advanced">
 <title>高度な話題</title> 

 <p>UDF と Pig のスクリプトは一般的に別々のファイルに保存されます。しかし、テストの用途のため、コードを単一の「複合」スクリプトとして保存することもできます。ただし、「複合」スクリプトをある言語に埋め込んだら、 UDF はその言語で書く必要があります。</p>
 
 <p>下の例では JavaScript と Pig を複合しています。この場合、「複合」スクリプトは JavaScript だけに埋め込めます。</p>
<p>JavaScript の複合スクリプトでは、 Pig スクリプトの制御の流れは main 関数の中に書く必要があります。そうしないと、スクリプトはエラーを発生します。</p>
 <source>
importPackage(Packages.org.apache.pig.scripting.js)
pig = org.apache.pig.scripting.js.JSPig;

helloworld.outputSchema = "word:chararray" 
function helloworld() { 
    return 'Hello, World'; 
}

function main() {
  var P = pig.compile(" a = load '1.txt' as (a0, a1);”+
       “b = foreach a generate helloworld();”+
           “store b into 'myoutput';");

  var result = P.bind().runSingle();
}
 </source>
 
 </section>
</section>

 <!-- =============================================================== -->
 <section id="jruby-udfs">
 <title>Ruby UDF を書く</title>

 <p><strong>注記:</strong> <em>Ruby UDF は実験的機能です。</em></p>

<!-- ++++++++++++++++++++++++++++++++++++++++++++++++ -->
 <section id="write-jruby">
 <title>Ruby UDF を書く</title>
 <p>PigUdf クラスを継承して Ruby UDF のクラスを定義します。</p>
 <source>
 require 'pigudf'
 class Myudfs &lt; PigUdf
     def square num
         return nil if num.nil?
         num**2
     end
 end
 </source>
 </section>
 <section id="jruby-schema-return-types">
 <title>戻り値の型とスキーマ</title>
 <p>戻り値のスキーマを定義するには二つの方法があります:</p>
 <p>outputSchema - Pig が解釈可能な書式で UDF の出力のスキーマを定義します。</p>
 <source>
 outputSchema "word:chararray"
 </source>
 <source>
 outputSchema "t:(m:[], t:(name:chararray, age:int, gpa:double), b:{t:(name:chararray, age:int, gpa:double)})"
 </source>
 <p>スキーマ関数</p>
 <source>
 outputSchemaFunction :squareSchema
 def squareSchema input
     input
 end
 </source>
 <p>outputSchema 文と outputSchemaFunction 文は UDF 実装の直前に置きます。スキーマ関数自体はクラスの中のどこに置いても構いません。</p>
 </section>
 <section id="register-jruby">
 <title>UDF を登録する</title>
 <p>Ruby UDF は次のようにして登録します。</p>
 <source>
 register 'test.rb' using jruby as myfuncs;
 </source>
 <p>次の例はショートカット版の構文です:</p>
 <source>
 register 'test.rb' using org.apache.pig.scripting.jruby.JrubyScriptEngine as myfuncs;
 </source>
 <p>上記の <code>REGISTER</code> 文は test.rb 内の Ruby 関数を、 Pig の指定した名前空間 (ここでは myfuncs) の中に登録します。これにより、以降 Pig Latin スクリプトの中で <code>myfuncs.square()</code> のように関数が使えるようになります。使用例は次のとおりです:</p>
 <source>
 b = foreach a generate myfuncs.concat($0, $1);
 </source>
 </section>
 <section id="jruby-example">
 <title>スクリプト例</title>
 <p>Ruby UDF の完全なサンプルを 2 つお見せします。</p>
 <source>
 require 'pigudf'
 class Myudfs &lt; PigUdf
 outputSchema "word:chararray"
     def concat *input
         input.inject(:+)
     end
 end
 </source>
 <source>
 require 'pigudf'
 class Myudfs &lt; PigUdf
 outputSchemaFunction :squareSchema
     def square num
         return nil if num.nil?
         num**2
     end
     def squareSchema input
         input
     end
 end
 </source>
 </section>

 <section id="jruby-advanced">
 <title>高度な話題</title>
 <p>Ruby では Algebraic インタフェースや Accumulator インタフェースの UDF も書けます。それぞれ、 <code>AlgebraicPigUdf</code> か <code>AccumulatorPigUdf</code> を継承したクラスを作ります。 Algebraic な UDF では <code>initial</code>, <code>intermed</code>, <code>final</code> メソッドを定義します。 Accumulator な UDF では <code>exec</code> メソッドと <code>get</code> メソッドを定義します。それぞれの種類の UDF の例を下記に挙げます:</p>
 <source>
 class Count &lt; AlgebraicPigUdf
     output_schema Schema.long
     def initial t
          t.nil? ? 0 : 1
     end
     def intermed t
          return 0 if t.nil?
          t.flatten.inject(:+)
     end
     def final t
         intermed(t)
     end
 end
 </source>
 <source>
 class Sum &lt; AccumulatorPigUdf
     output_schema { |i| i.in.in[0] }
     def exec items
         @sum ||= 0
         @sum += items.flatten.inject(:+)
     end
     def get
         @sum
     end
 end
 </source>
 </section>

</section> 

<!-- ================================================================== -->
<!-- PIGGYBANK -->
<section id="piggybank">
<title>Piggy Bank</title>
<p>Piggy Bank は Pig ユーザが自作の Java UDF を共有するための場所です。関数は「ありのまま」で提供されます。関数のバグを見つけたら、バグを潰して修正を Piggy Bank に寄贈してください。欲しい UDF がなかったら、作って Piggy Bank に寄贈してください。</p>

<p><strong>注記:</strong> Piggy Bank は今のところ Java UDF だけを対象にしています。 Python と JavaScript の UDF は、将来対象になります。</p>

 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="piggbank-access">
<title>関数を入手する</title>

<p>Piggy Bank の関数は今のところソースの形で配布されます。ユーザはコードを自分でチェックアウトしてビルドする必要があります。現時点では、バイナリ配布もナイトリービルドもありません。</p>

<p>入手可能な全 UDF をビルドするには、次のようにします。</p>
<ul>
<li>UDF コードをチェックアウト: <code>svn co http://svn.apache.org/repos/asf/pig/trunk/contrib/piggybank</code> </li>
<li>pig.jar をクラスパスに追加: <code>export CLASSPATH=$CLASSPATH:/path/to/pig.jar</code> </li>
<li>JAR ファイルをビルド: <code>trunk/contrib/piggybank/java</code> ディレクトリ下で <code>ant</code> を実行します。<code>piggybank.jar</code> が同じディレクトリ内にできます。</li>
</ul>
<p></p>

<p>関数の <code>Javadoc</code> が必要なら、 <code>trunk/contrib/piggybank/java</code> ディレクトリで <code>ant javadoc</code> を実行します。ドキュメンテーションは <code>trunk/contrib/piggybank/java/build/javadoc</code> ディレクトリに生成されます。</p>

<p>関数を使うには、関数が属するパッケージを見つける必要があります。トップレベルのパッケージが関数の分類に対応しています。現在存在するのは次のパッケージです:</p>
<ul>
<li>org.apache.pig.piggybank.comparison - ORDER 演算子で使う比較関数</li>
<li>org.apache.pig.piggybank.evaluation - 集約関数や列変換関数のような評価関数</li>
<li>org.apache.pig.piggybank.filtering - FILTER 演算子で使う関数</li>
<li>org.apache.pig.piggybank.grouping - グループ化関数</li>
<li>org.apache.pig.piggybank.storage - ロード・ストア関数</li>
</ul>

<p>(関数が属するパッケージの正確な位置については Javadoc か、ソースツリーを見てください)</p>

<p>たとえば、 UPPER 関数を使うには次のようにします:</p>

<source>
REGISTER /public/share/pig/contrib/piggybank/java/piggybank.jar ;
TweetsInaug = FILTER Tweets BY org.apache.pig.piggybank.evaluation.string.UPPER(text) 
    MATCHES '.*(INAUG|OBAMA|BIDEN|CHENEY|BUSH).*' ;
STORE TweetsInaug INTO 'meta/inaug/tweets_inaug' ;
</source>
</section>

 <!-- ++++++++++++++++++++++++++++++++++++++++++++++++++++++ -->
<section id="piggybank-contribute">
<title>関数を寄贈する</title>

<p>自作の Java 関数を寄贈するには、次のようにします:</p>
<ol>
<li><a href="#piggbank-access">関数を入手する</a>で示した手順で既存の Javadoc を見て、同様の関数が既に存在していないことを確認します。</li>
<li><a href="#piggbank-access">関数を入手する</a>で示した手順で UDF のコードをチェックアウトします。</li>
<li>関数の分類に応じた場所に Java コードを配置します。ディレクトリ構造は 2 つのレベルにわかれています: (1) <a href="#piggbank-access">関数を入手する</a>で示した関数の分類 (2) 関数の小分類 (評価関数の中の数学関数 (math) 、文字列関数 (string) のように) 。自作の関数に新しい小分類が必要だと思ったら、追加して構いません。</li>
<li>関数を <a href="http://docs.oracle.com/javase/6/docs/technotes/tools/windows/javadoc.html">Javadoc</a> 形式で充分にドキュメント化します。</li>
<li>コードが <a href="https://cwiki.apache.org/confluence/display/PIG/HowToContribute">How to Contribute to Pig</a> に示した Pig のコーディング規約を遵守しているようにします。</li>
<li>ソースツリー中の test ディレクトリ下に、関数に対応するテストクラスを追加します。</li>
<li><a href="https://cwiki.apache.org/confluence/display/PIG/HowToContribute">How to Contribute to Pig</a> に示した手順でパッチを投稿します。</li>
</ol>
</section>
</section> 


</body>
</document>

